{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Todos\n\n- Save model\n- check for other necessray callbacks\n- run version 8 with 16 shift\n- Embedding\n    - evaluate embedding\n    - run new embedding version on full asset data\n    - search an try alternatives\n        - add sin / cos signals\n- Check for other LR scheduler\n- Read for decoder necessety ","metadata":{}},{"cell_type":"markdown","source":"## Kernel","metadata":{}},{"cell_type":"code","source":"from IPython.core.display import display, HTML, Javascript\nimport os\nif not os.path.exists(\"../input/g-research-crypto-forecasting/\"): os.chdir('/t/Datasets/kaggle_crypto/internal')\n","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"papermill":{"duration":0.072966,"end_time":"2021-11-29T18:05:23.845682","exception":false,"start_time":"2021-11-29T18:05:23.772716","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-17T00:37:26.676103Z","iopub.execute_input":"2022-02-17T00:37:26.676697Z","iopub.status.idle":"2022-02-17T00:37:26.700579Z","shell.execute_reply.started":"2022-02-17T00:37:26.676605Z","shell.execute_reply":"2022-02-17T00:37:26.69995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport gc\nimport traceback\nimport pdb\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport gresearch_crypto\nimport tensorflow as tf\nfrom tensorflow import keras\nimport matplotlib.pyplot as plt\nimport pandas as pd, numpy as np\nfrom scipy.stats import pearsonr\nfrom tensorflow.keras import layers\nfrom sklearn.metrics import r2_score\nimport tensorflow_probability as tfp\nimport tensorflow.keras.backend as K\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import RobustScaler\nfrom tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\nfrom sklearn.metrics import mean_absolute_error as mae\nfrom sklearn.preprocessing import MinMaxScaler\n\npd.set_option('display.max_columns', None)","metadata":{"papermill":{"duration":9.261664,"end_time":"2021-11-29T18:05:34.914352","exception":false,"start_time":"2021-11-29T18:05:25.652688","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-17T00:37:27.449865Z","iopub.execute_input":"2022-02-17T00:37:27.450127Z","iopub.status.idle":"2022-02-17T00:37:34.411247Z","shell.execute_reply.started":"2022-02-17T00:37:27.450097Z","shell.execute_reply":"2022-02-17T00:37:34.410533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"DEVICE = \"GPU\" #or \"TPU\"\n\nSEED = 42\n\n# LOAD STRICT? YES=1 NO=0 | see: https://www.kaggle.com/julian3833/proposal-for-a-meaningful-lb-strict-lgbm\nLOAD_STRICT = True\n\n# WHICH YEARS TO INCLUDE? YES=1 NO=0\nINC2021 = 0\nINC2020 = 0\nINC2019 = 0\nINC2018 = 0\nINC2017 = 0\nINCCOMP = 1\nINCSUPP = 0\n\n# TRAINING PARAMETERS\nDEBUG = True\nSINGLE_ASSET = True\nasset_id=3\nN_ASSETS = 14\nnum_shift = 15\nTIME2VEC_DIM=3\nWINDOW_SIZE = 64\nprediction_length = 1\nBATCH_SIZE = 64\nPCT_VALIDATION = 10 # last 10% of the data are used as validation set\nPCT_TEST = 10 # last 10% of the data are used as validation set","metadata":{"papermill":{"duration":0.050546,"end_time":"2021-11-29T18:05:35.179716","exception":false,"start_time":"2021-11-29T18:05:35.12917","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-17T00:37:34.412864Z","iopub.execute_input":"2022-02-17T00:37:34.413166Z","iopub.status.idle":"2022-02-17T00:37:34.422577Z","shell.execute_reply.started":"2022-02-17T00:37:34.413132Z","shell.execute_reply":"2022-02-17T00:37:34.421333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if DEVICE == \"TPU\":\n    print(\"connecting to TPU...\")\n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        print('Running on TPU ', tpu.master())\n    except ValueError:\n        tpu = None\n    if tpu:\n        try:\n            print(\"initializing  TPU ...\")\n            tf.config.experimental_connect_to_cluster(tpu)\n            tf.tpu.experimental.initialize_tpu_system(tpu)\n            strategy = tf.distribute.experimental.TPUStrategy(tpu)\n            print(\"TPU initialized\")\n        except: print(\"failed to initialize TPU\")\n    else: DEVICE = \"GPU\"\n\nif DEVICE != \"TPU\": strategy = tf.distribute.get_strategy()\nif DEVICE == \"GPU\": print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\nAUTO     = tf.data.experimental.AUTOTUNE\nREPLICAS = strategy.num_replicas_in_sync","metadata":{"_kg_hide-input":true,"papermill":{"duration":0.069363,"end_time":"2021-11-29T18:05:35.290484","exception":false,"start_time":"2021-11-29T18:05:35.221121","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-17T00:37:34.424968Z","iopub.execute_input":"2022-02-17T00:37:34.425933Z","iopub.status.idle":"2022-02-17T00:37:34.593804Z","shell.execute_reply.started":"2022-02-17T00:37:34.425866Z","shell.execute_reply":"2022-02-17T00:37:34.593104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compute_target(df):\n    \"\"\"\n    \n    \"\"\"\n    R=list()\n    c=list(df['Close'])\n    for i in range(df.shape[0]):\n        future=c[min([i+16,df.shape[0]-1])]\n        past=c[min([i+1,df.shape[0]-1])]\n        R.append(future/past)\n    df['R']=R\n    df['R']=np.log(df['R'])\n    df['pred']=np.exp(df['R'])-1\n    return df\n\n\n# we will use weighted correlation as function to evaluate our model performance\n# https://stackoverflow.com/questions/38641691/weighted-correlation-coefficient-with-pandas\ndef wmean(x, w):\n    return np.sum(x * w) / np.sum(w)\n\ndef wcov(x, y, w):\n    return np.sum(w * (x - wmean(x, w)) * (y - wmean(y, w))) / np.sum(w)\n\ndef wcorr(x, y, w):\n    return wcov(x, y, w) / np.sqrt(wcov(x, x, w) * wcov(y, y, w))\n\ndef wcorr(x, y, w=1):\n    return wcov(x, y, w) / np.sqrt(wcov(x, x, w) * wcov(y, y, w))\n\n# ----------------------------------------------------------------\n# ----------------------------------------------------------------\n# def tf_wmean(x, w):\n#     w_sum = tf.math.reduce_sum(w)\n#     xw_sum = tf.math.reduce_sum(tf.math.multiply(x, w))\n#     return  tf.math.divide(xw_sum, w_sum)\n# \n# def tf_wcov(x, y, w):\n#     w_sum = tf.math.reduce_sum(w)\n#     xyw = tf.math.multiply(x - tf_wmean(x, w), y - tf_wmean(y, w))\n#     w_xy_sum= tf.math.reduce_sum(tf.math.multiply(w, xyw))\n#     return  tf.math.divide(w_xy_sum, w_sum)\n# \n# def tf_wcorr(x, y, w=1):\n#     if not w:\n#         w=1.\n#     x = tf.cast(x, tf.float32)\n#     y = tf.cast(y, tf.float32)\n#     w = tf.cast(w, tf.float32)\n#     \n#     mul_xwcov_y_wcov = tf.math.multiply(tf_wcov(x, x, w), tf_wcov(y, y, w))\n#     return tf.math.divide(tf_wcov(x, y, w), tf.math.sqrt(mul_xwcov_y_wcov))\n\n\nfrom functools import partial\n\ndef corr_loss(y_true, y_pred):\n    x = tf.cast(y_true, tf.float32)\n    y = tf.cast(y_pred, tf.float32)\n    mx = K.mean(x)\n    my = K.mean(y)\n    xm, ym = x-mx, y-my\n    r_num = K.sum(tf.multiply(xm,ym))\n    r_den = K.sqrt(tf.multiply(K.sum(K.square(xm)), K.sum(K.square(ym))))\n    r = r_num / r_den\n    r = K.maximum(K.minimum(r, 1.0), -1.0)\n    return - r\n\ndef combined_loss(y_true, y_pred, weight1=0.5, weight2=None):\n    if not weight2:\n        weight2 = 1 - weight1\n    loss1 = corr_loss(y_true, y_pred)\n    mae_loss_ = tf.keras.losses.MeanAbsoluteError()\n    loss2 = mae_loss_(y_true, y_pred)\n    return loss1*weight1 + loss2*weight2\n\n\ndef competition_weighted_correlation(a, b, weights=1):\n\n    #w = np.ravel(weights)\n    #a = np.ravel(a)\n    #b = np.ravel(b)\n    a = tf.cast(a, dtype=tf.float32)\n    b = tf.cast(b, dtype=tf.float32)\n    \n    len_a = tf.cast(tf.size(a), dtype=tf.float32)\n    len_b = tf.cast(tf.size(b), dtype=tf.float32)\n    #sum_w = tf.reduce_sum(w)\n    mean_a = tf.cast(tf.math.reduce_mean(a), dtype=tf.float32)\n    mean_b = tf.cast(tf.math.reduce_mean(b), dtype=tf.float32)\n    var_a = tf.cast(tf.math.reduce_sum(tf.math.square(a - mean_a)) / len_a, dtype=tf.float32)\n    var_b = tf.cast(tf.math.reduce_sum(tf.math.square(b - mean_b)) / len_b, dtype=tf.float32)\n\n    cov = tf.math.reduce_sum((a * b)) / len_a - (mean_a * mean_b)\n    corr = cov / tf.math.sqrt(var_a * var_b)\n\n    return corr","metadata":{"execution":{"iopub.status.busy":"2022-02-17T00:37:34.597079Z","iopub.execute_input":"2022-02-17T00:37:34.597276Z","iopub.status.idle":"2022-02-17T00:37:34.61722Z","shell.execute_reply.started":"2022-02-17T00:37:34.59725Z","shell.execute_reply":"2022-02-17T00:37:34.616206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import datatable as dt\nextra_data_files = {0: '../input/cryptocurrency-extra-data-binance-coin', 2: '../input/cryptocurrency-extra-data-bitcoin-cash', 1: '../input/cryptocurrency-extra-data-bitcoin', 3: '../input/cryptocurrency-extra-data-cardano', 4: '../input/cryptocurrency-extra-data-dogecoin', 5: '../input/cryptocurrency-extra-data-eos-io', 6: '../input/cryptocurrency-extra-data-ethereum', 7: '../input/cryptocurrency-extra-data-ethereum-classic', 8: '../input/cryptocurrency-extra-data-iota', 9: '../input/cryptocurrency-extra-data-litecoin', 11: '../input/cryptocurrency-extra-data-monero', 10: '../input/cryptocurrency-extra-data-maker', 12: '../input/cryptocurrency-extra-data-stellar', 13: '../input/cryptocurrency-extra-data-tron'}\n\n# Uncomment to load the original csv [slower]\n# orig_df_train = pd.read_csv(data_path + 'train.csv') \n# supp_df_train = pd.read_csv(data_path + 'supplemental_train.csv')\n# df_asset_details = pd.read_csv(data_path  + 'asset_details.csv').sort_values(\"Asset_ID\")\n\norig_df_train = dt.fread('../input/cryptocurrency-extra-data-binance-coin/orig_train.jay').to_pandas()\ndf_asset_details = dt.fread('../input/cryptocurrency-extra-data-binance-coin/orig_asset_details.jay').to_pandas()\nsupp_df_train = dt.fread('../input/cryptocurrency-extra-data-binance-coin/orig_supplemental_train.jay').to_pandas()\nassets_details = dt.fread('../input/cryptocurrency-extra-data-binance-coin/orig_asset_details.jay').to_pandas()\nasset_weight_dict = {assets_details['Asset_ID'].tolist()[idx]: assets_details['Weight'].tolist()[idx] for idx in range(len(assets_details))}\nasset_name_dict = {assets_details['Asset_ID'].tolist()[idx]: assets_details['Asset_Name'].tolist()[idx] for idx in range(len(assets_details))}\n\ndef load_training_data_for_asset(asset_id, load_jay = True):\n    dfs = []\n    if INCCOMP: dfs.append(orig_df_train[orig_df_train[\"Asset_ID\"] == asset_id].copy())\n    if INCSUPP: dfs.append(supp_df_train[supp_df_train[\"Asset_ID\"] == asset_id].copy())\n    \n    if load_jay:\n        if INC2017 and os.path.exists(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2017) + '.csv'): dfs.append(dt.fread(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2017) + '.jay').to_pandas())\n        if INC2018 and os.path.exists(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2018) + '.csv'): dfs.append(dt.fread(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2018) + '.jay').to_pandas())\n        if INC2019 and os.path.exists(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2019) + '.csv'): dfs.append(dt.fread(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2019) + '.jay').to_pandas())\n        if INC2020 and os.path.exists(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2020) + '.csv'): dfs.append(dt.fread(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2020) + '.jay').to_pandas())\n        if INC2021 and os.path.exists(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2021) + '.csv'): dfs.append(dt.fread(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2021) + '.jay').to_pandas())\n    else: \n        if INC2017 and os.path.exists(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2017) + '.csv'): dfs.append(pd.read_csv(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2017) + '.csv'))\n        if INC2018 and os.path.exists(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2018) + '.csv'): dfs.append(pd.read_csv(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2018) + '.csv'))\n        if INC2019 and os.path.exists(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2019) + '.csv'): dfs.append(pd.read_csv(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2019) + '.csv'))\n        if INC2020 and os.path.exists(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2020) + '.csv'): dfs.append(pd.read_csv(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2020) + '.csv'))\n        if INC2021 and os.path.exists(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2021) + '.csv'): dfs.append(pd.read_csv(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2021) + '.csv'))\n    df = pd.concat(dfs, axis = 0) if len(dfs) > 1 else dfs[0]\n    df['date'] = pd.to_datetime(df['timestamp'], unit = 's')\n    if LOAD_STRICT: df = df.loc[df['date'] < \"2021-06-13 00:00:00\"]    \n    df = df.sort_values('date')\n    return df\n\ndef load_data_for_all_assets():\n    dfs = []\n    for asset_id in list(extra_data_files.keys()): dfs.append(load_training_data_for_asset(asset_id))\n    return pd.concat(dfs)","metadata":{"_kg_hide-input":true,"papermill":{"duration":24.69094,"end_time":"2021-11-29T18:06:00.13746","exception":false,"start_time":"2021-11-29T18:05:35.44652","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-17T00:37:34.619083Z","iopub.execute_input":"2022-02-17T00:37:34.61937Z","iopub.status.idle":"2022-02-17T00:37:57.756015Z","shell.execute_reply.started":"2022-02-17T00:37:34.619333Z","shell.execute_reply":"2022-02-17T00:37:57.755291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = dt.fread('../input/cryptocurrency-extra-data-binance-coin/orig_example_test.jay').to_pandas()\nsample_prediction_df = dt.fread('../input/cryptocurrency-extra-data-binance-coin/orig_example_sample_submission.jay').to_pandas()\nassets = dt.fread('../input/cryptocurrency-extra-data-binance-coin/orig_asset_details.jay').to_pandas()\nassets_order = dt.fread('../input/cryptocurrency-extra-data-binance-coin/orig_supplemental_train.jay').to_pandas().Asset_ID[:N_ASSETS]\n\nassets_order = dict((t,i) for i,t in enumerate(assets_order))\nprint(\"Loaded all data!\")","metadata":{"execution":{"iopub.status.busy":"2022-02-17T00:37:57.758407Z","iopub.execute_input":"2022-02-17T00:37:57.758916Z","iopub.status.idle":"2022-02-17T00:37:57.865418Z","shell.execute_reply.started":"2022-02-17T00:37:57.758876Z","shell.execute_reply":"2022-02-17T00:37:57.864602Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = load_data_for_all_assets().sort_values('timestamp').set_index(\"timestamp\")\nif DEBUG: \n    train = train[-500000:]  # [-2221694:]  #\nelse:\n    train = train[1000000:]\nprint(train.shape)","metadata":{"papermill":{"duration":17.031431,"end_time":"2021-11-29T18:06:17.210444","exception":false,"start_time":"2021-11-29T18:06:00.179013","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-17T00:37:57.866835Z","iopub.execute_input":"2022-02-17T00:37:57.867087Z","iopub.status.idle":"2022-02-17T00:38:08.067318Z","shell.execute_reply.started":"2022-02-17T00:37:57.867053Z","shell.execute_reply":"2022-02-17T00:38:08.066523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Memory saving function credit to https://www.kaggle.com/gemartin/load-data-reduce-memory-usage\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.\n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n\n    for col in df.columns:\n        col_type = df[col].dtype.name\n        \n        if col_type not in ['object', 'category', 'datetime64[ns, UTC]', 'datetime64[ns]']:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n\n    return df\n","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"papermill":{"duration":0.054816,"end_time":"2021-11-29T18:06:17.597492","exception":false,"start_time":"2021-11-29T18:06:17.542676","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-17T00:38:08.068529Z","iopub.execute_input":"2022-02-17T00:38:08.069014Z","iopub.status.idle":"2022-02-17T00:38:08.083878Z","shell.execute_reply.started":"2022-02-17T00:38:08.068972Z","shell.execute_reply":"2022-02-17T00:38:08.08305Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def upper_shadow(df): return df['High'] - np.maximum(df['Close'], df['Open'])\ndef lower_shadow(df): return np.minimum(df['Close'], df['Open']) - df['Low']\n\ndef get_features(df, row = False):\n    df_feat = df\n    df_feat['spread'] = df_feat['High'] - df_feat['Low']\n    df_feat['mean_trade'] = df_feat['Volume']/df_feat['Count']\n    df_feat['log_price_change'] = np.log(df_feat['Close']/df_feat['Open'])\n    df_feat['upper_Shadow'] = upper_shadow(df_feat)\n    df_feat['lower_Shadow'] = lower_shadow(df_feat)\n    df_feat[\"high_div_low\"] = df_feat[\"High\"] / df_feat[\"Low\"]\n    df_feat['UPS'] = (df_feat['High'] - np.maximum(df_feat['Close'], df_feat['Open']))\n    df_feat['LOS'] = (np.minimum(df_feat['Close'], df_feat['Open']) - df_feat['Low'])\n    df_feat['LOGVOL'] = np.log(1. + df_feat['Volume'])\n    df_feat['LOGCNT'] = np.log(1. + df_feat['Count'])\n    return df_feat\n\n\n# A utility function to build features around lags.\ndef get_features_hist(df_feat, row=False):\n    \n    ### features to consider. See potential features...\n    ### note that we predicting returns 15 minutes ahead. This minutes price data is therefore not sufficient. We must roll the variables, 15, 30, 90, 250, 1250\n       \n    # df_feat = df[['Open', 'High', 'Low', 'Close', 'Volume', 'Count', 'VWAP', 'lr15', 'm', 'lr16', 'Asset_ID']].copy()\n\n    if df_feat.shape[0]<3750:\n        df_feat['beta_num'] = np.nan\n        df_feat['m2'] = np.nan\n    else:\n        df_feat['beta_num'] = (df_feat['lr15']*df_feat['m']).rolling(3750).mean().values\n        df_feat['m2'] = (df_feat['m']*df_feat['m']).rolling(3750).mean().values\n        \n    if row:\n        # first .iloc as far back as we need, compute feature, then downsize until .iloc[-1]\n        df_feat = df_feat.iloc[-1]\n#         mean_price = df_feat[['Open', 'High', 'Low', 'Close']].mean()\n#         med_price = df_feat[['Open', 'High', 'Low', 'Close']].median()\n        df_feat['upper_shadow'] = df_feat['High'] / df_feat[['Close', 'Open']].max()\n        df_feat['lower_shadow'] = df_feat[['Close', 'Open']].min() / df_feat['Low']\n    else:\n#         mean_price = df_feat[['Open', 'High', 'Low', 'Close']].mean(axis=1)\n#         med_price = df_feat[['Open', 'High', 'Low', 'Close']].median(axis=1)\n        df_feat['upper_shadow'] = df_feat['High'] / df_feat[['Close', 'Open']].max(axis=1)\n        df_feat['lower_shadow'] = df_feat[['Close', 'Open']].min(axis=1) / df_feat['Low']\n        # df_feat = df_feat.drop('Asset_ID', axis=1)\n        \n    df_feat['beta'] = np.nan_to_num(df_feat['beta_num'] / df_feat['m2'], nan=0., posinf=0., neginf=0.)\n    df_feat['target_lag'] = df_feat['lr15'] - df_feat['beta']*df_feat['m']  # first 15 entries of target_lagg are NaN\n\n        ### Sense checks\n#         print((df_feat['Target'] - df_feat.groupby('Asset_ID')['target_lag'].shift(-16)).abs().mean())\n#         print(df_feat.loc[(df_feat.Target.isnull())&(df_feat.target_lag.notnull())].shape)\n#         print(df_feat.loc[(df_feat.Target.notnull())&(df_feat.target_lag.isnull())].shape)        \n        \n    df_feat['open2close'] = df_feat['Close'] / df_feat['Open']\n    df_feat['high2low'] = df_feat['High'] / df_feat['Low']\n           \n#     df_feat['high2mean'] = df_feat['High'] / mean_price\n#     df_feat['low2mean'] = df_feat['Low'] / mean_price\n#     df_feat['high2median'] = df_feat['High'] / med_price\n#     df_feat['low2median'] = df_feat['Low'] / med_price\n    df_feat['volume2count'] = df_feat['Volume'] / (df_feat['Count'] + 1)\n    df_feat['close2vwap'] = df_feat['Close'] / df_feat['VWAP']\n    \n    return df_feat.replace([np.inf, -np.inf], np.nan)\n\ndef log_return(series, periods=1):\n    return np.log(series).diff(periods=periods)\n\ndef timestamp_to_date(timestamp):\n    return(datetime.fromtimestamp(timestamp))\n\ndef get_lead_features():\n    # get_features from ethereum\n    pass\n\n","metadata":{"papermill":{"duration":37.064504,"end_time":"2021-11-29T18:06:54.699951","exception":false,"start_time":"2021-11-29T18:06:17.635447","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-17T00:38:08.085233Z","iopub.execute_input":"2022-02-17T00:38:08.085625Z","iopub.status.idle":"2022-02-17T00:38:08.113078Z","shell.execute_reply.started":"2022-02-17T00:38:08.085585Z","shell.execute_reply":"2022-02-17T00:38:08.112226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## FILTER DATAFRAME IF ONLY WORKING WITH ONE ASSET\nif SINGLE_ASSET:\n    train = train[train[\"Asset_ID\"]==asset_id]\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-02-17T00:38:08.116698Z","iopub.execute_input":"2022-02-17T00:38:08.117242Z","iopub.status.idle":"2022-02-17T00:38:08.295694Z","shell.execute_reply.started":"2022-02-17T00:38:08.117207Z","shell.execute_reply":"2022-02-17T00:38:08.29475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train[['Count', 'Open', 'High', 'Low', 'Close', 'Volume', 'VWAP', 'Target']] = train[['Count', 'Open', 'High', 'Low', 'Close', 'Volume', 'VWAP', 'Target']].astype(np.float32)\nprint(train.shape)\ntrain['Target'] = train['Target'].fillna(0)\nVWAP_max = np.max(train[np.isfinite(train.VWAP)].VWAP)\nVWAP_min = np.min(train[np.isfinite(train.VWAP)].VWAP)\ntrain['VWAP'] = np.nan_to_num(train.VWAP, posinf=VWAP_max, neginf=VWAP_min)\ndf = train[['Asset_ID', 'Target']].copy()\ntimes = dict((t,i) for i,t in enumerate(df.index.unique()))\ndf['id'] = df.index.map(times)\ndf['id'] = df['id'].astype(str) + '_' + df['Asset_ID'].astype(str)\nids = df.id.copy()\ndel df\n\ntrain = train.sort_index()\nind = train.index.unique()\ndef reindex(df):\n    df = df.reindex(range(ind[0],ind[-1]+60,60),method='nearest')\n    df = df.fillna(method=\"ffill\").fillna(method=\"bfill\")\n    return df\ntrain = train.groupby('Asset_ID').apply(reindex).reset_index(0, drop=True).sort_index()\ngc.collect()\nprint(train.shape)\n\n# Matching records and marking generated rows as 'non-real'\nprint(train.shape)\ntrain['group_num'] = train.index.map(times)\ntrain = train.dropna(subset=['group_num'])\ntrain['group_num'] = train['group_num'].astype('int')\ntrain['id'] = train['group_num'].astype(str) + '_' + train['Asset_ID'].astype(str)\ntrain['is_real'] = train.id.isin(ids) * 1\ntrain = train.drop('id', axis=1)\nprint(train.shape)","metadata":{"execution":{"iopub.status.busy":"2022-02-17T00:38:08.296841Z","iopub.execute_input":"2022-02-17T00:38:08.297078Z","iopub.status.idle":"2022-02-17T00:38:08.760404Z","shell.execute_reply.started":"2022-02-17T00:38:08.297033Z","shell.execute_reply":"2022-02-17T00:38:08.759366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['asset_order'] = train.Asset_ID.map(assets_order)\ntrain = train.sort_values(by=['group_num', 'asset_order'])\n# train = reduce_mem_usage(train)\ngc.collect()","metadata":{"papermill":{"duration":38.374972,"end_time":"2021-11-29T18:09:38.027192","exception":false,"start_time":"2021-11-29T18:08:59.65222","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-17T00:38:08.762072Z","iopub.execute_input":"2022-02-17T00:38:08.762339Z","iopub.status.idle":"2022-02-17T00:38:08.938219Z","shell.execute_reply.started":"2022-02-17T00:38:08.762303Z","shell.execute_reply":"2022-02-17T00:38:08.93743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if train.index.name == \"timestamp\":\n    train = train.reset_index()\ntrain['lr15'] = train.groupby('Asset_ID')['Close'].apply(lambda x: log_return(x, 15))\ntrain['lr16'] = train.groupby('Asset_ID')['Close'].apply(lambda x: log_return(x, 16))\ntrain = train.merge(df_asset_details[['Asset_ID','Weight']], how='left', on = 'Asset_ID')\ntrain['m'] = train['lr15']*train['Weight']\ntrain['m'] = train.groupby('timestamp')['m'].transform('sum') / np.sum(df_asset_details['Weight'])\n\ntrain.head(20)","metadata":{"execution":{"iopub.status.busy":"2022-02-17T00:38:08.939735Z","iopub.execute_input":"2022-02-17T00:38:08.940042Z","iopub.status.idle":"2022-02-17T00:38:09.00107Z","shell.execute_reply.started":"2022-02-17T00:38:08.939984Z","shell.execute_reply":"2022-02-17T00:38:09.000329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_split_indices(indices):\n    train_end_idx = int(len(indices) * (1 - (PCT_VALIDATION+PCT_TEST)/100))\n    val_end_idx = int(len(indices) * (1 - (PCT_TEST)/100))\n    return indices[train_end_idx], indices[val_end_idx]\n\ntrain_end_idx = {}\nval_end_idx = {}\nif not SINGLE_ASSET:\n    for i in range(N_ASSETS):\n        X_asset_indices = train[train[\"Asset_ID\"]==i].index\n        train_end_idx[i], val_end_idx[i] = get_split_indices(X_asset_indices)\nelse:\n    X_asset_indices = train[train[\"Asset_ID\"]==asset_id].index\n    train_end_idx[asset_id], val_end_idx[asset_id] = get_split_indices(X_asset_indices)","metadata":{"execution":{"iopub.status.busy":"2022-02-17T00:38:09.002308Z","iopub.execute_input":"2022-02-17T00:38:09.00274Z","iopub.status.idle":"2022-02-17T00:38:09.013502Z","shell.execute_reply.started":"2022-02-17T00:38:09.002701Z","shell.execute_reply":"2022-02-17T00:38:09.012775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create train data of shape: Batch Size X Window Size X Num Assets --> bBATCH_SIZEon mean over all features\n# create train lables of shape: batch size X Num Assets  -->  based on target of t(win_len)\n# given data and index, train batches + labels are created\n\n\nclass sample_generator(keras.utils.Sequence):\n    def __init__(self, x_set, y_set, batch_size, length, prediction_length):\n        self.x, self.y = x_set, y_set\n        self.batch_size = batch_size\n        self.length = length\n        self.prediction_length = prediction_length\n        self.size = len(x_set)\n    def __len__(self): return int((len(self.x)-self.length) / float(self.batch_size))\n    def __getitem__(self, idx):\n        batch_x=[]\n        batch_y=[]\n        for i in range(self.batch_size):\n            start_ind = self.batch_size*idx + i\n            end_ind = start_ind + self.length \n            if (end_ind+self.prediction_length-1) <= self.size:\n                batch_x.append(self.x[start_ind : end_ind])\n                batch_y.append(self.y[end_ind -1: end_ind+self.prediction_length-1]) \n        x,y = np.array(batch_x), np.array(batch_y)\n        assert x.shape == (BATCH_SIZE, WINDOW_SIZE,len(features)), f\"Shape Missmatch of train data generator X at idx {str(idx)} and i {str(i)}\"\n        assert x.shape == (BATCH_SIZE, WINDOW_SIZE,len(features)), f\"Shape Missmatch of train data generator Y at idx {str(idx)} and i {str(i)}\"\n        return x,y\n\nclass multivariate_sample_generator(keras.utils.Sequence):\n    # generator which supports multivariate ex-features input into nbeats\n    # -> Single asset per sequence\n    # --> x_sequence.shape = batch_size x Window_size x 1\n    # --> y_sequence.shape = batch_size x 1\n    # --> e_sequence.shape = batch_size x Window_size x num_ex_var    \n    def __init__(self, x_set, y_set, e_set, batch_size, length, prediction_length):\n        self.x, self.y, self.e = x_set, y_set, e_set\n        self.batch_size = batch_size\n        self.length = length\n        self.prediction_length = prediction_length\n        self.size = len(x_set)\n        self.num_assets = self.x.shape[1]\n    def __len__(self): return int(np.ceil(len(self.x) / float(self.batch_size)))\n    def __getitem__(self, idx):\n        # idx: how many times the generator is already called\n        batch_x=[]\n        batch_y=[]\n        batch_e=[]\n        num_sample=0\n        while num_sample < self.batch_size:\n            start_ind = self.batch_size//self.num_assets*idx + num_sample//self.num_assets\n            end_ind = start_ind + self.length \n            if (end_ind+self.prediction_length-1) <= self.size:\n                for a in range(self.num_assets):\n                    batch_x.append(self.x[start_ind : end_ind, a])\n                    batch_e.append(self.e[start_ind : end_ind, a, :])\n                    batch_y.append(self.y[end_ind -1: end_ind+self.prediction_length-1, a]) \n                    num_sample += 1\n                    if num_sample >= self.batch_size: break;  ## TODO: FIND BETTER SOLUTION (here we throw aray samples if we break out of loop)\n        return [np.array(batch_x).reshape(-1,self.length,1), np.array(batch_e)], np.array(batch_y)\n    \n\ndef assert_data_validity_generator(gen, batch_size, test_shift_size=1, num_tests=200):\n    for i in range(num_tests):\n        num_batch = np.random.randint(0, len(gen), dtype=int)\n        num_sample = np.random.randint(0, batch_size-1, dtype=int)\n        assert gen[num_batch][0][num_sample+test_shift_size,-1,0] == gen[num_batch][1][num_sample,0,0], \"Data Missmatch between Series and Target in generator data\"\n    print(\"All tests passed.\")\n","metadata":{"execution":{"iopub.status.busy":"2022-02-17T00:38:09.014888Z","iopub.execute_input":"2022-02-17T00:38:09.015352Z","iopub.status.idle":"2022-02-17T00:38:09.035233Z","shell.execute_reply.started":"2022-02-17T00:38:09.015315Z","shell.execute_reply":"2022-02-17T00:38:09.034448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_asset = train.copy()\nX_asset = get_features_hist(X_asset)\nX_asset[\"Target_shifted\"] = X_asset[\"Target\"].shift(num_shift)\nX_asset[\"Target\"].plot()\n\ntrain_end_idx[asset_id], val_end_idx[asset_id]\nfeatures = [\"Target_shifted\"]\n# features = [\"Target_shifted\", \"m\", \"upper_shadow\", \"lower_shadow\", \"open2close\", \"high2low\", \"volume2count\", \"close2vwap\"] \ntarget = [\"Target\"]\nX_asset.loc[X_asset.is_real==0, features] = 0 \nX_asset[features] = X_asset[features].fillna(0)\nscaler = MinMaxScaler()\nX_asset[features] = scaler.fit_transform(X_asset[features])  # TODO: NEEDS TO BE SEPARATED FOR TRAIN TEST\n \ntrain_generator = sample_generator(X_asset.loc[:train_end_idx[asset_id], features], \n                                   X_asset.loc[:train_end_idx[asset_id], target], \n                                   length = WINDOW_SIZE, batch_size = BATCH_SIZE, prediction_length=prediction_length)\nval_generator = sample_generator(X_asset.loc[train_end_idx[asset_id]:val_end_idx[asset_id], features], \n                                 X_asset.loc[train_end_idx[asset_id]:val_end_idx[asset_id], target], \n                                 length = WINDOW_SIZE, batch_size = BATCH_SIZE, prediction_length=prediction_length)\ntest_generator = sample_generator(X_asset.loc[val_end_idx[asset_id]:, features], \n                                  X_asset.loc[val_end_idx[asset_id]:, target], \n                                  length = WINDOW_SIZE, batch_size = BATCH_SIZE, prediction_length=prediction_length)\n\nprint(\"BATCH_SIZE: \", BATCH_SIZE)\nprint(f'Train batch shape: {train_generator[0][0].shape}')\nprint(f'Target batch shape: {train_generator[0][1].shape}')\n\nprint(\"Num batches of train, validation and test generator: \", len(train_generator),\", \", len(val_generator),\", \", len(test_generator))\nprint(\"Num Training Samples: \", len(train_generator)*BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2022-02-17T00:50:00.197725Z","iopub.execute_input":"2022-02-17T00:50:00.198607Z","iopub.status.idle":"2022-02-17T00:50:00.483976Z","shell.execute_reply.started":"2022-02-17T00:50:00.198534Z","shell.execute_reply":"2022-02-17T00:50:00.482539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span class=\"title-section w3-xxlarge\" id=\"training\">Training üèãÔ∏è</span>\n<hr>\n\nOur model will be trained for the number of FOLDS and EPOCHS you chose in the configuration above. Each fold the model with lowest validation loss will be saved and used to predict OOF and test. Adjust the variable `VERBOSE`. The variable `VERBOSE=1 or 2` will display the training and validation loss for each epoch as text. ","metadata":{"papermill":{"duration":0.038491,"end_time":"2021-11-29T18:09:46.955605","exception":false,"start_time":"2021-11-29T18:09:46.917114","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class Time2Vec(keras.layers.Layer):\n    def __init__(self, kernel_size=1):\n        super(Time2Vec, self).__init__(trainable=True, name='Time2VecLayer')\n        self.k = kernel_size\n    \n    def build(self, input_shape):  # build automatically executed before layer is called for the first time --  mostly used to instantiate weights\n        # trend\n        self.wb = self.add_weight(name='wb',shape=(input_shape[1],),initializer='uniform',trainable=True)\n        self.bb = self.add_weight(name='bb',shape=(input_shape[1],),initializer='uniform',trainable=True)\n        # periodic\n        self.wa = self.add_weight(name='wa',shape=(1, input_shape[1], self.k),initializer='uniform',trainable=True)\n        self.ba = self.add_weight(name='ba',shape=(1, input_shape[1], self.k),initializer='uniform',trainable=True)\n        super(Time2Vec, self).build(input_shape)\n    \n    def call(self, inputs, **kwargs):  # where the layer logic lives\n        bias = self.wb * inputs + self.bb\n        dp = K.dot(inputs, self.wa) + self.ba\n        wgts = K.sin(dp) # or K.cos(.)\n\n        ret = K.concatenate([K.expand_dims(bias, -1), wgts], -1)\n        ret = K.reshape(ret, (-1, inputs.shape[1]*(self.k+1)))\n        return ret\n    \n    def compute_output_shape(self, input_shape):\n        return (input_shape[0], input_shape[1]*(self.k + 1))\n    \n\n# https://towardsdatascience.com/time2vec-for-time-series-features-encoding-a03a4f3f937e\nclass T2V(keras.layers.Layer):\n    \n    def __init__(self, output_dim=None, **kwargs):\n        self.output_dim = output_dim\n        super(T2V, self).__init__(**kwargs)\n        \n    def build(self, input_shape):        \n        self.W = self.add_weight(name='W',\n                      shape=(input_shape[-1], self.output_dim),\n                      initializer='uniform',\n                      trainable=True)        \n        self.P = self.add_weight(name='P',\n                      shape=(input_shape[1], self.output_dim),\n                      initializer='uniform',\n                      trainable=True)        \n        self.w = self.add_weight(name='w',\n                      shape=(input_shape[1], 1),\n                      initializer='uniform',\n                      trainable=True)        \n        self.p = self.add_weight(name='p',\n                      shape=(input_shape[1], 1),\n                      initializer='uniform',\n                      trainable=True)        \n        super(T2V, self).build(input_shape)\n        \n    def call(self, x):\n        \n        original = self.w * x + self.p\n        sin_trans = K.sin(K.dot(x, self.W) + self.P)\n        \n        return K.concatenate([sin_trans, original], -1)","metadata":{"execution":{"iopub.status.busy":"2022-02-17T00:50:00.525214Z","iopub.execute_input":"2022-02-17T00:50:00.52544Z","iopub.status.idle":"2022-02-17T00:50:00.540886Z","shell.execute_reply.started":"2022-02-17T00:50:00.525406Z","shell.execute_reply":"2022-02-17T00:50:00.54001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://towardsdatascience.com/the-time-series-transformer-2a521a0efad3\nfrom tensorflow_addons.layers import MultiHeadAttention\n\nclass AttentionBlock(keras.Model):\n    def __init__(self, name='AttentionBlock', num_heads=2, head_size=128, ff_dim=None, dropout=0, **kwargs):\n        super().__init__(name=name, **kwargs)\n\n        if ff_dim is None:\n            ff_dim = head_size\n\n        self.attention = MultiHeadAttention(num_heads=num_heads, head_size=head_size, dropout=dropout)\n        self.attention_dropout = keras.layers.Dropout(dropout)\n        self.attention_norm = keras.layers.BatchNormalization(epsilon=1e-6)\n\n        self.ff_conv1 = keras.layers.Conv1D(filters=ff_dim, kernel_size=1, activation='relu')\n        # self.ff_conv2 at build()\n        self.ff_dropout = keras.layers.Dropout(dropout)\n        self.ff_norm = keras.layers.BatchNormalization(epsilon=1e-6)\n\n    def build(self, input_shape):\n        self.ff_conv2 = keras.layers.Conv1D(filters=input_shape[-1], kernel_size=1) \n\n    def call(self, inputs):\n        x = self.attention([inputs, inputs])\n        x = self.attention_dropout(x)\n        x = self.attention_norm(inputs + x)\n\n        x = self.ff_conv1(x)\n        x = self.ff_conv2(x)\n        x = self.ff_dropout(x)\n\n        x = self.ff_norm(inputs + x)\n        return x\n    \n\nclass ModelTrunk(keras.Model):\n    def __init__(self, name='ModelTrunk', time2vec_dim=1, num_heads=2, head_size=128, ff_dim=None, num_layers=1, dropout=0, **kwargs):\n        super().__init__(name=name, **kwargs)\n        self.time2vec = Time2Vec(kernel_size=time2vec_dim)\n        if ff_dim is None:\n            ff_dim = head_size\n        self.dropout = dropout\n        self.attention_layers = [AttentionBlock(num_heads=num_heads, head_size=head_size, ff_dim=ff_dim, dropout=dropout) for _ in range(num_layers)]\n\n        \n    def call(self, inputs):\n        time_embedding = keras.layers.TimeDistributed(self.time2vec)(inputs)\n        x = K.concatenate([inputs, time_embedding], -1)\n        for attention_layer in self.attention_layers:\n            x = attention_layer(x)\n\n        return K.reshape(x, (-1, x.shape[1] * x.shape[2])) # flat vector of features out\n    \n\ndef build_model_new(\n    input_shape,  # shape of time series sample\n    head_size, # size of multi head attention\n    num_heads,  # number of multi-head attention \n    ff_dim,  # \n    num_transformer_blocks, # \n    mlp_units, # list of N dense layers with i neurons\n    dropout=0,  # transformer block dropout rate\n    mlp_dropout=0,  # \n    time2vec_dim=3\n):\n    inputs = keras.Input(shape=input_shape)\n    \n    x = ModelTrunk(num_heads=num_heads, head_size=head_size, ff_dim=ff_dim, num_layers=num_transformer_blocks, time2vec_dim=time2vec_dim, dropout=dropout)(inputs)\n\n    # x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n    for dim in mlp_units:\n        x = layers.Dense(dim, activation=\"relu\")(x)\n        x = layers.Dropout(mlp_dropout)(x)\n    outputs = layers.Dense(1, kernel_initializer=\"normal\")(x)\n    return keras.Model(inputs, outputs)\n\n\n# https://github.com/tatp22/multidim-positional-encoding/blob/master/positional_encodings/tf_positional_encodings.py\nclass TFPositionalEncoding1D(tf.keras.layers.Layer):\n    def __init__(self, channels: int, dtype=tf.float32):\n        \"\"\"\n        Args:\n            channels int: The last dimension of the tensor you want to apply pos emb to.\n        Keyword Args:\n            dtype: output type of the encodings. Default is \"tf.float32\".\n        \"\"\"\n        super(TFPositionalEncoding1D, self).__init__()\n\n        self.channels = int(np.ceil(channels / 2) * 2)\n        self.inv_freq = np.float32(\n            1\n            / np.power(\n                10000, np.arange(0, self.channels, 2) / np.float32(self.channels)\n            )\n        )\n\n    @tf.function\n    def call(self, inputs):\n        \"\"\"\n        :param tensor: A 3d tensor of size (batch_size, x, ch)\n        :return: Positional Encoding Matrix of size (batch_size, x, ch)\n        \"\"\"\n        if len(inputs.shape) != 3:\n            raise RuntimeError(\"The input tensor has to be 3d!\")\n        _, x, org_channels = inputs.shape\n\n        dtype = self.inv_freq.dtype\n        pos_x = tf.range(x, dtype=dtype)\n        sin_inp_x = tf.einsum(\"i,j->ij\", pos_x, self.inv_freq)\n        emb = tf.expand_dims(tf.concat((tf.sin(sin_inp_x), tf.cos(sin_inp_x)), -1), 0)\n        emb = emb[0]  # A bit of a hack\n        return tf.repeat(emb[None, :, :org_channels], tf.shape(inputs)[0], axis=0)        \n        ","metadata":{"execution":{"iopub.status.busy":"2022-02-17T00:50:00.705448Z","iopub.execute_input":"2022-02-17T00:50:00.705827Z","iopub.status.idle":"2022-02-17T00:50:00.731486Z","shell.execute_reply.started":"2022-02-17T00:50:00.705792Z","shell.execute_reply":"2022-02-17T00:50:00.730033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://keras.io/examples/timeseries/timeseries_transformer_classification/\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\n\ndef transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n    # Normalization and Attention\n    x = layers.BatchNormalization(epsilon=1e-6)(inputs)\n    x = layers.MultiHeadAttention(\n        key_dim=head_size, num_heads=num_heads, dropout=dropout\n    )(x, x)\n    x = layers.Dropout(dropout)(x)\n    res = x + inputs\n\n    # Feed Forward Part\n    x = layers.BatchNormalization(epsilon=1e-6)(res)\n    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n    x = layers.Dropout(dropout)(x)\n    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n    return x + res\n\n# self.time2vec = Time2Vec(kernel_size=time2vec_dim)\n\ndef build_model(\n    input_shape,  # shape of time series sample\n    head_size, # size of multi head attention\n    num_heads,  # number of multi-head attention \n    ff_dim,  # \n    num_transformer_blocks, # \n    mlp_units, # list of N dense layers with i neurons\n    dropout=0,  # transformer block dropout rate\n    mlp_dropout=0,  # \n    time2vec_dim=1\n):\n    inputs = keras.Input(shape=input_shape)\n    \n    time_embedding = TFPositionalEncoding1D(channels=input_shape[-1])(inputs)\n    x = tf.keras.layers.Add()([inputs, time_embedding])\n    \n    # x = inputs\n    for _ in range(num_transformer_blocks):\n        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n\n    x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n    for dim in mlp_units:\n        x = layers.Dense(dim, activation=\"relu\")(x)\n        x = layers.Dropout(mlp_dropout)(x)\n    outputs = layers.Dense(1, kernel_initializer=\"normal\")(x)\n    return keras.Model(inputs, outputs)\n","metadata":{"execution":{"iopub.status.busy":"2022-02-17T00:50:00.893052Z","iopub.execute_input":"2022-02-17T00:50:00.893667Z","iopub.status.idle":"2022-02-17T00:50:00.904069Z","shell.execute_reply.started":"2022-02-17T00:50:00.893633Z","shell.execute_reply":"2022-02-17T00:50:00.903093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_shape = train_generator[0][0].shape[1:]\n\nmodel = build_model(\n    input_shape,\n    head_size=256,\n    num_heads=4,\n    ff_dim=4,\n    num_transformer_blocks=8,\n    mlp_units=[128, 128],\n    mlp_dropout=0.1,\n    dropout=0.1,\n    time2vec_dim=TIME2VEC_DIM\n)\n\n# tfp.stats.correlation\n#tf_corr_new = tfp.stats.correlation(x, y=None, sample_axis=0, event_axis=-1, keepdims=False, name=None)\n# tf.contrib.metrics.streaming_pearson_correlation(logits,labels)\n\n# model.compile(\n#     optimizer=\"Adam\",\n#     loss={\"head1\": \"mse\", \"head2\": \"mse\"},\n#     loss_weights={\"head1\": HEAD1_WEIGHT, \"head2\": HEAD2_WEIGHT},\n#     metrics={\"head1\": [\"mae\"], \"head2\": [\"mae\"]}\n# )\n# \nloss_func = combined_loss\nmodel.compile(\n    loss=loss_func,\n    optimizer=keras.optimizers.Adam(learning_rate=1e-2),\n    metrics=[\"mae\", tfp.stats.correlation],\n)\nmodel.summary()\n","metadata":{"execution":{"iopub.status.busy":"2022-02-17T00:50:01.053911Z","iopub.execute_input":"2022-02-17T00:50:01.054523Z","iopub.status.idle":"2022-02-17T00:50:01.738473Z","shell.execute_reply.started":"2022-02-17T00:50:01.054489Z","shell.execute_reply":"2022-02-17T00:50:01.737796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# input_shape = train_generator[0][0].shape[1:]\n# \n# model = build_model_new(\n#     input_shape,\n#     head_size=256,\n#     num_heads=4,\n#     ff_dim=4,\n#     num_transformer_blocks=8,\n#     mlp_units=[128, 128],\n#     mlp_dropout=0.1,\n#     dropout=0.1,\n#     time2vec_dim=TIME2VEC_DIM\n# )\n# \n# model.compile(\n#     loss=\"mae\",\n#     optimizer=keras.optimizers.Adam(learning_rate=1e-2),\n#     metrics=[\"mae\", tf_wcorr],\n# )\n# model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-02-17T00:50:01.741911Z","iopub.execute_input":"2022-02-17T00:50:01.742275Z","iopub.status.idle":"2022-02-17T00:50:01.747214Z","shell.execute_reply.started":"2022-02-17T00:50:01.742232Z","shell.execute_reply":"2022-02-17T00:50:01.746441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def lr_scheduler(epoch, lr, warmup_epochs=15, decay_epochs=100, initial_lr=1e-6, base_lr=1e-3, min_lr=5e-5):\n    if epoch <= warmup_epochs:\n        pct = epoch / warmup_epochs\n        return ((base_lr - initial_lr) * pct) + initial_lr\n\n    if epoch > warmup_epochs and epoch < warmup_epochs+decay_epochs:\n        pct = 1 - ((epoch - warmup_epochs) / decay_epochs)\n        return ((base_lr - min_lr) * pct) + min_lr\n\n    return min_lr\n\n","metadata":{"execution":{"iopub.status.busy":"2022-02-17T00:50:01.748404Z","iopub.execute_input":"2022-02-17T00:50:01.748891Z","iopub.status.idle":"2022-02-17T00:50:01.757265Z","shell.execute_reply.started":"2022-02-17T00:50:01.748849Z","shell.execute_reply":"2022-02-17T00:50:01.756451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"callbacks = [keras.callbacks.EarlyStopping(monitor='val_correlation', mode='max',patience=5, restore_best_weights=True)]\ncallbacks += [keras.callbacks.LearningRateScheduler(lr_scheduler, verbose=1)]\nfilepath = \"./transformer_model\"\ncallbacks += [keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False)]\n\nEPOCHS = 20\nhistory = model.fit(\n            train_generator, \n            validation_data = (val_generator),\n            epochs=EPOCHS,\n            batch_size=BATCH_SIZE,\n            callbacks=callbacks)\n\nmodel.evaluate(test_generator, verbose=1)","metadata":{"execution":{"iopub.status.busy":"2022-02-17T00:50:01.76068Z","iopub.execute_input":"2022-02-17T00:50:01.760949Z","iopub.status.idle":"2022-02-17T00:59:13.604094Z","shell.execute_reply.started":"2022-02-17T00:50:01.76092Z","shell.execute_reply":"2022-02-17T00:59:13.603416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tf.keras.utils.plot_model(get_model(), show_shapes=True)","metadata":{"papermill":{"duration":6.240302,"end_time":"2021-11-29T18:09:55.791092","exception":false,"start_time":"2021-11-29T18:09:49.55079","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-17T00:59:13.605398Z","iopub.execute_input":"2022-02-17T00:59:13.605663Z","iopub.status.idle":"2022-02-17T00:59:13.610915Z","shell.execute_reply.started":"2022-02-17T00:59:13.605628Z","shell.execute_reply":"2022-02-17T00:59:13.608967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# EPOCHS = 100\n# seed = 1 \n# \n# tf.random.set_seed(seed)\n# estop = keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 5, verbose = 0, mode = 'min',restore_best_weights = True)\n# # scheduler = keras.optimizers.schedules.ExponentialDecay(1e-3, (0.5 * len(X_train) / BATCH_SIZE), 1e-3)\n# # lr = keras.callbacks.LearningRateScheduler(scheduler, verbose = 1)\n# mcp_save = keras.callbacks.ModelCheckpoint('model_multivariate_min_val_loss', save_best_only=True, monitor='val_loss', mode='min')\n# plateau = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=1, mode='min')\n# \n# history = model.fit(train_generator, validation_data = (val_generator), epochs = EPOCHS, callbacks = [estop, mcp_save, plateau], verbose=1)\n# model.save(\"model_multivariate\")\n\n","metadata":{"papermill":{"duration":791.992821,"end_time":"2021-11-29T18:23:07.875583","exception":false,"start_time":"2021-11-29T18:09:55.882762","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-02-17T00:59:13.613208Z","iopub.execute_input":"2022-02-17T00:59:13.613664Z","iopub.status.idle":"2022-02-17T00:59:13.619716Z","shell.execute_reply.started":"2022-02-17T00:59:13.613625Z","shell.execute_reply":"2022-02-17T00:59:13.618913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history['val_loss'], label= \"validation_loss\")\nplt.plot(history.history['loss'], label= \"train_loss\")\nplt.legend(loc=\"upper left\", bbox_to_anchor=(1, 0.5))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-17T00:59:13.620719Z","iopub.execute_input":"2022-02-17T00:59:13.620977Z","iopub.status.idle":"2022-02-17T00:59:13.834846Z","shell.execute_reply.started":"2022-02-17T00:59:13.620926Z","shell.execute_reply":"2022-02-17T00:59:13.834207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history['val_mae'], label= \"validation metric (mae)\")\nplt.plot(history.history['mae'], label= \"train metric (mae)\")\nplt.legend(loc=\"upper left\", bbox_to_anchor=(1, 0.5))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-17T00:59:13.836128Z","iopub.execute_input":"2022-02-17T00:59:13.836366Z","iopub.status.idle":"2022-02-17T00:59:14.056202Z","shell.execute_reply.started":"2022-02-17T00:59:13.836333Z","shell.execute_reply":"2022-02-17T00:59:14.055496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history.history.keys()","metadata":{"execution":{"iopub.status.busy":"2022-02-17T00:59:14.057348Z","iopub.execute_input":"2022-02-17T00:59:14.058255Z","iopub.status.idle":"2022-02-17T00:59:14.063936Z","shell.execute_reply.started":"2022-02-17T00:59:14.058214Z","shell.execute_reply":"2022-02-17T00:59:14.063131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history['val_correlation'], label= \"validation metric W-Corr\")\nplt.plot(history.history['correlation'], label= \"train metric W-Corr\")\nplt.legend(loc=\"upper left\", bbox_to_anchor=(1, 0.5))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-17T00:59:14.065698Z","iopub.execute_input":"2022-02-17T00:59:14.066047Z","iopub.status.idle":"2022-02-17T00:59:14.281173Z","shell.execute_reply.started":"2022-02-17T00:59:14.066013Z","shell.execute_reply":"2022-02-17T00:59:14.280453Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create predictions on training set\npredictions = model.predict(train_generator)\ny_true = []\nfor x,y in train_generator: y_true.append(y)\ny_true = np.squeeze(np.concatenate(y_true), axis=-1)\n\n\nprint(predictions.shape, y_true.shape)\nassert predictions.shape == y_true.shape, f\"{predictions.shape}, {y_true.shape}\"\n\n# Evaluate predictions on validation set\nprint(\"Window Size: \", WINDOW_SIZE)\nprint(\"Prediction length: \", prediction_length)\nprint(\"Epochs: \", EPOCHS)\n\nprint('---------------------')\nprint('Asset:    Corr. coef.')\nprint('---------------------')\nasset_w_corr = []\nasset_corrs = []\nasset_mae = []\ny_true = np.squeeze(y_true)\ny_pred = np.squeeze(predictions)\nreal_target_ind = np.argwhere(y_true!=0)\n# asset_id = list(assets_order.keys())[i]\n# asset_name = assets[assets.Asset_ID == asset_id]['Asset_Name'].item()\nmae_asset = mae(y_true, y_pred)\nasset_corr = np.corrcoef(np.nan_to_num(y_pred.flatten()), np.nan_to_num(y_true.flatten()))[0,1]\n\nprint(f\"corr: {asset_corr:.4f}\")\nprint(f\"mae: {mae_asset:.4f}\")\nprint(\"\\n\")\nprint(f\"Predictions Min: {predictions.min()}, Predictions Max: {predictions.max()}, Predictions mean:{predictions.mean()}, Predictions var:{predictions.var()}\") \nprint(f\"y_true Min: {y_true.min()}, y_true Max: {y_true.max()}, y_true mean:{y_true.mean()}, y_true var:{y_true.var()}\") ","metadata":{"execution":{"iopub.status.busy":"2022-02-17T00:59:14.283396Z","iopub.execute_input":"2022-02-17T00:59:14.284233Z","iopub.status.idle":"2022-02-17T00:59:25.446139Z","shell.execute_reply.started":"2022-02-17T00:59:14.284191Z","shell.execute_reply":"2022-02-17T00:59:25.44524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create predictions on validation set\npredictions = model.predict(val_generator)\ny_true = []\nfor x,y in val_generator: y_true.append(y)\ny_true = np.squeeze(np.concatenate(y_true), axis=-1)\n\n\nprint(predictions.shape, y_true.shape)\nassert predictions.shape == y_true.shape, f\"{predictions.shape}, {y_true.shape}\"\n\n# Evaluate predictions on validation set\nprint(\"Window Size: \", WINDOW_SIZE)\nprint(\"Prediction length: \", prediction_length)\nprint(\"Epochs: \", EPOCHS)\n\nprint('---------------------')\nprint('Asset:    Corr. coef.')\nprint('---------------------')\nasset_w_corr = []\nasset_corrs = []\nasset_mae = []\ny_true = np.squeeze(y_true)\ny_pred = np.squeeze(predictions)\nreal_target_ind = np.argwhere(y_true!=0)\n# asset_id = list(assets_order.keys())[i]\n# asset_name = assets[assets.Asset_ID == asset_id]['Asset_Name'].item()\nmae_asset = mae(y_true, y_pred)\nasset_corr = np.corrcoef(np.nan_to_num(y_pred.flatten()), np.nan_to_num(y_true.flatten()))[0,1]\n\nprint(f\"corr: {asset_corr:.4f}\")\nprint(f\"mae: {mae_asset:.4f}\")\nprint(\"\\n\")\nprint(f\"Predictions Min: {predictions.min()}, Predictions Max: {predictions.max()}, Predictions mean:{predictions.mean()}, Predictions var:{predictions.var()}\") \nprint(f\"y_true Min: {y_true.min()}, y_true Max: {y_true.max()}, y_true mean:{y_true.mean()}, y_true var:{y_true.var()}\") ","metadata":{"execution":{"iopub.status.busy":"2022-02-17T00:59:25.448301Z","iopub.execute_input":"2022-02-17T00:59:25.448786Z","iopub.status.idle":"2022-02-17T00:59:26.72903Z","shell.execute_reply.started":"2022-02-17T00:59:25.448742Z","shell.execute_reply":"2022-02-17T00:59:26.727478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create predictions on validation set\npredictions = model.predict(test_generator)\ny_true = []\nfor x,y in test_generator: y_true.append(y)\ny_true = np.squeeze(np.concatenate(y_true), axis=-1)\n\n\nprint(predictions.shape, y_true.shape)\nassert predictions.shape == y_true.shape, f\"{predictions.shape}, {y_true.shape}\"\n\n# Evaluate predictions on validation set\nprint(\"Window Size: \", WINDOW_SIZE)\nprint(\"Prediction length: \", prediction_length)\nprint(\"Epochs: \", EPOCHS)\n\nasset_w_corr = []\nasset_corrs = []\nasset_mae = []\ny_true = np.squeeze(y_true)\npredictions = np.squeeze(predictions)\nreal_target_ind = np.argwhere(y_true!=0)\n# asset_id = list(assets_order.keys())[i]\n# asset_name = assets[assets.Asset_ID == asset_id]['Asset_Name'].item()\nmae_asset = mae(y_true, predictions)\nasset_corr = np.corrcoef(np.nan_to_num(y_pred.flatten()), np.nan_to_num(y_true.flatten()))[0,1]\n\nprint('---------------------')\nprint('Asset:    Corr. coef.')\nprint('---------------------')\nprint(f\"corr: {asset_corr:.4f}\")\nprint(f\"mae: {mae_asset:.4f}\")\nprint(\"\\n\")\nprint(f\"Predictions Min: {predictions.min()}, Predictions Max: {predictions.max()}, Predictions mean:{predictions.mean()}, Predictions var:{predictions.var()}\") \nprint(f\"y_true Min: {y_true.min()}, y_true Max: {y_true.max()}, y_true mean:{y_true.mean()}, y_true var:{y_true.var()}\") \n\n","metadata":{"execution":{"iopub.status.busy":"2022-02-17T00:59:26.731599Z","iopub.execute_input":"2022-02-17T00:59:26.731937Z","iopub.status.idle":"2022-02-17T00:59:27.980154Z","shell.execute_reply.started":"2022-02-17T00:59:26.731852Z","shell.execute_reply":"2022-02-17T00:59:27.979435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}