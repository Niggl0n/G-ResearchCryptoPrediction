{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "transformer-model_colab.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# VERSION \n",
        "\n",
        "**Transfromer Colab v2**"
      ],
      "metadata": {
        "id": "u04TwdXA9xSW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Todos\n",
        "\n",
        "\n",
        "- Embedding\n",
        "    - evaluate embedding\n",
        "    - run new embedding version on full asset data\n",
        "    - search an try alternatives\n",
        "        - add sin / cos signals\n",
        "- Check for other LR scheduler\n",
        "\n",
        "- How can constant features be fed efficiently to transformer networks? \n",
        "  - Expl: Weights, Coin Index\n",
        "\n",
        "- What are GARCH models? Can they be used to this problem\n",
        "\n",
        "\n",
        "Paper:\n",
        "  - Multi-Transformer: A New Neural Network-Based Architecture\n",
        "for Forecasting S&P Volatility\n",
        "  - file:///tmp/mozilla_niggl0/mathematics-09-01794.pdf\n",
        "    - the input variables of the models proposed are the daily logarithmic returns (rt−i)\n",
        "  and the standard deviation of the last five daily logarithmic returns:\n",
        "    - In accordance with other studies such as [38] or [50], the realized volatility is used as\n",
        "  response variable for the models based on ANNs\n",
        "    - Positional encoding adapted compared to original papaer in order to make it independent of feature/timeseries used "
      ],
      "metadata": {
        "id": "IeHbanPQgdZ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Kernel"
      ],
      "metadata": {
        "id": "bqI4QtfZgdaC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datatable\n",
        "!pip install tensorflow-addons\n",
        "\n",
        "# memory footprint support libraries/code\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isn’t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        " process = psutil.Process(os.getpid())\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm() \n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5r1geHXmBGX",
        "outputId": "435b88b8-cd46-493c-947e-656f43cf2bb5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datatable\n",
            "  Downloading datatable-1.0.0-cp37-cp37m-manylinux_2_12_x86_64.whl (96.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 96.9 MB 113 kB/s \n",
            "\u001b[?25hInstalling collected packages: datatable\n",
            "Successfully installed datatable-1.0.0\n",
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.16.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 6.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.16.1\n",
            "Collecting gputil\n",
            "  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n",
            "Building wheels for collected packages: gputil\n",
            "  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gputil: filename=GPUtil-1.4.0-py3-none-any.whl size=7411 sha256=cfd9d7b5813f8724e3ef73bc45a496f6f21b356d83b8745cb8edfbe6e3cf0fb2\n",
            "  Stored in directory: /root/.cache/pip/wheels/6e/f8/83/534c52482d6da64622ddbf72cd93c35d2ef2881b78fd08ff0c\n",
            "Successfully built gputil\n",
            "Installing collected packages: gputil\n",
            "Successfully installed gputil-1.4.0\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.7/dist-packages (0.5.1)\n",
            "Gen RAM Free: 12.5 GB  | Proc size: 147.1 MB\n",
            "GPU RAM Free: 15109MB | Used: 0MB | Util   0% | Total 15109MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Only if necessary to kill the session again\n",
        "\n",
        "# !kill -9 -1\n"
      ],
      "metadata": {
        "id": "Rgjhemutvhtc"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %load_ext google.colab.data_table#To diable the display\n",
        "# %unload_ext google.colab.data_table"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "_kg_hide-output": true,
        "papermill": {
          "duration": 0.072966,
          "end_time": "2021-11-29T18:05:23.845682",
          "exception": false,
          "start_time": "2021-11-29T18:05:23.772716",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2022-02-17T00:37:26.676103Z",
          "iopub.execute_input": "2022-02-17T00:37:26.676697Z",
          "iopub.status.idle": "2022-02-17T00:37:26.700579Z",
          "shell.execute_reply.started": "2022-02-17T00:37:26.676605Z",
          "shell.execute_reply": "2022-02-17T00:37:26.69995Z"
        },
        "trusted": true,
        "id": "V7tOVJGxgdaF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gc\n",
        "import datatable as dt\n",
        "import traceback\n",
        "import pdb\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from IPython.core.display import display, HTML, Javascript\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
        "from tensorflow_addons.layers import MultiHeadAttention\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd, numpy as np\n",
        "from functools import partial\n",
        "from scipy.stats import pearsonr\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import RobustScaler, MinMaxScaler\n",
        "from sklearn.metrics import mean_absolute_error as mae\n",
        "\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "%cd /content/gdrive/MyDrive/Github/GResearch/\n",
        "# if not os.path.exists(\"../input/g-research-crypto-forecasting/\"): os.chdir('/t/Datasets/kaggle_crypto/internal')"
      ],
      "metadata": {
        "papermill": {
          "duration": 9.261664,
          "end_time": "2021-11-29T18:05:34.914352",
          "exception": false,
          "start_time": "2021-11-29T18:05:25.652688",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2022-02-17T00:37:27.449865Z",
          "iopub.execute_input": "2022-02-17T00:37:27.450127Z",
          "iopub.status.idle": "2022-02-17T00:37:34.411247Z",
          "shell.execute_reply.started": "2022-02-17T00:37:27.450097Z",
          "shell.execute_reply": "2022-02-17T00:37:34.410533Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "fUeURtRcgdaH",
        "outputId": "424bc2f7-41e4-45ea-ac7d-dc31812e98f1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style type='text/css'>\n",
              ".datatable table.frame { margin-bottom: 0; }\n",
              ".datatable table.frame thead { border-bottom: none; }\n",
              ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
              ".datatable .bool    { background: #DDDD99; }\n",
              ".datatable .object  { background: #565656; }\n",
              ".datatable .int     { background: #5D9E5D; }\n",
              ".datatable .float   { background: #4040CC; }\n",
              ".datatable .str     { background: #CC4040; }\n",
              ".datatable .time    { background: #40CC40; }\n",
              ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
              ".datatable .frame tbody td { text-align: left; }\n",
              ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
              ".datatable th:nth-child(2) { padding-left: 12px; }\n",
              ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
              ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
              ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
              ".datatable .sp {  opacity: 0.25;}\n",
              ".datatable .footer { font-size: 9px; }\n",
              ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "/content/gdrive/MyDrive/Github/GResearch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = \"GPU\" #or \"TPU\"\n",
        "\n",
        "SEED = 42\n",
        "\n",
        "# LOAD STRICT? YES=1 NO=0 | see: https://www.kaggle.com/julian3833/proposal-for-a-meaningful-lb-strict-lgbm\n",
        "LOAD_STRICT = True\n",
        "\n",
        "# WHICH YEARS TO INCLUDE? YES=1 NO=0\n",
        "INC2021 = 0\n",
        "INC2020 = 0\n",
        "INC2019 = 0\n",
        "INC2018 = 0\n",
        "INC2017 = 0\n",
        "INCCOMP = 1\n",
        "INCSUPP = 0\n",
        "\n",
        "# TRAINING PARAMETERS\n",
        "DEBUG = True\n",
        "SINGLE_ASSET = True\n",
        "asset_id=3\n",
        "N_ASSETS = 14\n",
        "num_shift = 15\n",
        "num_debug_samples = 2000000\n",
        "TEMP_EMBEDDING = False\n",
        "TIME2VEC_DIM=3\n",
        "WINDOW_SIZE = 128\n",
        "prediction_length = 1\n",
        "BATCH_SIZE = 128\n",
        "PCT_VALIDATION = 10 # last 10% of the data are used as validation set\n",
        "PCT_TEST = 10 # last 10% of the data are used as validation set"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.050546,
          "end_time": "2021-11-29T18:05:35.179716",
          "exception": false,
          "start_time": "2021-11-29T18:05:35.12917",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2022-02-17T00:37:34.412864Z",
          "iopub.execute_input": "2022-02-17T00:37:34.413166Z",
          "iopub.status.idle": "2022-02-17T00:37:34.422577Z",
          "shell.execute_reply.started": "2022-02-17T00:37:34.413132Z",
          "shell.execute_reply": "2022-02-17T00:37:34.421333Z"
        },
        "trusted": true,
        "id": "qQD2DffPgdaK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if DEVICE == \"TPU\":\n",
        "    print(\"connecting to TPU...\")\n",
        "    try:\n",
        "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "        print('Running on TPU ', tpu.master())\n",
        "    except ValueError:\n",
        "        tpu = None\n",
        "    if tpu:\n",
        "        try:\n",
        "            print(\"initializing  TPU ...\")\n",
        "            tf.config.experimental_connect_to_cluster(tpu)\n",
        "            tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "            strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "            print(\"TPU initialized\")\n",
        "        except: print(\"failed to initialize TPU\")\n",
        "    else: DEVICE = \"GPU\"\n",
        "\n",
        "if DEVICE != \"TPU\": strategy = tf.distribute.get_strategy()\n",
        "if DEVICE == \"GPU\": print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
        "AUTO     = tf.data.experimental.AUTOTUNE\n",
        "REPLICAS = strategy.num_replicas_in_sync"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "papermill": {
          "duration": 0.069363,
          "end_time": "2021-11-29T18:05:35.290484",
          "exception": false,
          "start_time": "2021-11-29T18:05:35.221121",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2022-02-17T00:37:34.424968Z",
          "iopub.execute_input": "2022-02-17T00:37:34.425933Z",
          "iopub.status.idle": "2022-02-17T00:37:34.593804Z",
          "shell.execute_reply.started": "2022-02-17T00:37:34.425866Z",
          "shell.execute_reply": "2022-02-17T00:37:34.593104Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgeI6XZUgdaM",
        "outputId": "fddaabff-f42b-4457-f83b-a8dcc340a15e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_target(df):\n",
        "    \"\"\"\n",
        "    \n",
        "    \"\"\"\n",
        "    R=list()\n",
        "    c=list(df['Close'])\n",
        "    for i in range(df.shape[0]):\n",
        "        future=c[min([i+16,df.shape[0]-1])]\n",
        "        past=c[min([i+1,df.shape[0]-1])]\n",
        "        R.append(future/past)\n",
        "    df['R']=R\n",
        "    df['R']=np.log(df['R'])\n",
        "    df['pred']=np.exp(df['R'])-1\n",
        "    return df\n",
        "\n",
        "\n",
        "# we will use weighted correlation as function to evaluate our model performance\n",
        "# https://stackoverflow.com/questions/38641691/weighted-correlation-coefficient-with-pandas\n",
        "def wmean(x, w):\n",
        "    return np.sum(x * w) / np.sum(w)\n",
        "\n",
        "def wcov(x, y, w):\n",
        "    return np.sum(w * (x - wmean(x, w)) * (y - wmean(y, w))) / np.sum(w)\n",
        "\n",
        "def wcorr(x, y, w):\n",
        "    return wcov(x, y, w) / np.sqrt(wcov(x, x, w) * wcov(y, y, w))\n",
        "\n",
        "def wcorr(x, y, w=1):\n",
        "    return wcov(x, y, w) / np.sqrt(wcov(x, x, w) * wcov(y, y, w))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def corr_loss(y_true, y_pred):\n",
        "    x = tf.cast(y_true, tf.float32)\n",
        "    y = tf.cast(y_pred, tf.float32)\n",
        "    mx = K.mean(x)\n",
        "    my = K.mean(y)\n",
        "    xm, ym = x-mx, y-my\n",
        "    r_num = K.sum(tf.multiply(xm,ym))\n",
        "    r_den = K.sqrt(tf.multiply(K.sum(K.square(xm)), K.sum(K.square(ym))))\n",
        "    r = r_num / r_den\n",
        "    r = K.maximum(K.minimum(r, 1.0), -1.0)\n",
        "    return - r\n",
        "\n",
        "def combined_loss(y_true, y_pred, weight1=0.5, weight2=None):\n",
        "    if not weight2:\n",
        "        weight2 = 1 - weight1\n",
        "    loss1 = corr_loss(y_true, y_pred)\n",
        "    mae_loss_ = tf.keras.losses.MeanAbsoluteError()\n",
        "    loss2 = mae_loss_(y_true, y_pred)\n",
        "    return loss1*weight1 + loss2*weight2\n",
        "\n",
        "\n",
        "def competition_weighted_correlation(a, b, weights=1):\n",
        "\n",
        "    #w = np.ravel(weights)\n",
        "    #a = np.ravel(a)\n",
        "    #b = np.ravel(b)\n",
        "    a = tf.cast(a, dtype=tf.float32)\n",
        "    b = tf.cast(b, dtype=tf.float32)\n",
        "    \n",
        "    len_a = tf.cast(tf.size(a), dtype=tf.float32)\n",
        "    len_b = tf.cast(tf.size(b), dtype=tf.float32)\n",
        "    #sum_w = tf.reduce_sum(w)\n",
        "    mean_a = tf.cast(tf.math.reduce_mean(a), dtype=tf.float32)\n",
        "    mean_b = tf.cast(tf.math.reduce_mean(b), dtype=tf.float32)\n",
        "    var_a = tf.cast(tf.math.reduce_sum(tf.math.square(a - mean_a)) / len_a, dtype=tf.float32)\n",
        "    var_b = tf.cast(tf.math.reduce_sum(tf.math.square(b - mean_b)) / len_b, dtype=tf.float32)\n",
        "\n",
        "    cov = tf.math.reduce_sum((a * b)) / len_a - (mean_a * mean_b)\n",
        "    corr = cov / tf.math.sqrt(var_a * var_b)\n",
        "\n",
        "    return corr"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T00:37:34.597079Z",
          "iopub.execute_input": "2022-02-17T00:37:34.597276Z",
          "iopub.status.idle": "2022-02-17T00:37:34.61722Z",
          "shell.execute_reply.started": "2022-02-17T00:37:34.59725Z",
          "shell.execute_reply": "2022-02-17T00:37:34.616206Z"
        },
        "trusted": true,
        "id": "LqMReNYPgdaO"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extra_data_files = {0: '', \n",
        "                    2: '', \n",
        "                    1: '', \n",
        "                    3: '', \n",
        "                    4: '', \n",
        "                    5: '', \n",
        "                    6: '', \n",
        "                    7: '', \n",
        "                    8: '', \n",
        "                    9: '', \n",
        "                    11:'', \n",
        "                    10:'', \n",
        "                    12:'', \n",
        "                    13:''}\n",
        "\n",
        "# Uncomment to load the original csv [slower]\n",
        "# orig_df_train = pd.read_csv(data_path + 'train.csv') \n",
        "# supp_df_train = pd.read_csv(data_path + 'supplemental_train.csv')\n",
        "# df_asset_details = pd.read_csv(data_path  + 'asset_details.csv').sort_values(\"Asset_ID\")\n",
        "\n",
        "orig_df_train = dt.fread('orig_train.jay').to_pandas()\n",
        "df_asset_details = dt.fread('orig_asset_details.jay').to_pandas()\n",
        "supp_df_train = dt.fread('orig_supplemental_train.jay').to_pandas()\n",
        "assets_details = dt.fread('orig_asset_details.jay').to_pandas()\n",
        "asset_weight_dict = {assets_details['Asset_ID'].tolist()[idx]: assets_details['Weight'].tolist()[idx] for idx in range(len(assets_details))}\n",
        "asset_name_dict = {assets_details['Asset_ID'].tolist()[idx]: assets_details['Asset_Name'].tolist()[idx] for idx in range(len(assets_details))}\n",
        "\n",
        "def load_training_data_for_asset(asset_id, load_jay = True):\n",
        "    dfs = []\n",
        "    if INCCOMP: dfs.append(orig_df_train[orig_df_train[\"Asset_ID\"] == asset_id].copy())\n",
        "    if INCSUPP: dfs.append(supp_df_train[supp_df_train[\"Asset_ID\"] == asset_id].copy())\n",
        "    \n",
        "    if load_jay:\n",
        "        if INC2017 and os.path.exists(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2017) + '.csv'): dfs.append(dt.fread(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2017) + '.jay').to_pandas())\n",
        "        if INC2018 and os.path.exists(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2018) + '.csv'): dfs.append(dt.fread(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2018) + '.jay').to_pandas())\n",
        "        if INC2019 and os.path.exists(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2019) + '.csv'): dfs.append(dt.fread(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2019) + '.jay').to_pandas())\n",
        "        if INC2020 and os.path.exists(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2020) + '.csv'): dfs.append(dt.fread(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2020) + '.jay').to_pandas())\n",
        "        if INC2021 and os.path.exists(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2021) + '.csv'): dfs.append(dt.fread(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2021) + '.jay').to_pandas())\n",
        "    else: \n",
        "        if INC2017 and os.path.exists(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2017) + '.csv'): dfs.append(pd.read_csv(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2017) + '.csv'))\n",
        "        if INC2018 and os.path.exists(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2018) + '.csv'): dfs.append(pd.read_csv(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2018) + '.csv'))\n",
        "        if INC2019 and os.path.exists(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2019) + '.csv'): dfs.append(pd.read_csv(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2019) + '.csv'))\n",
        "        if INC2020 and os.path.exists(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2020) + '.csv'): dfs.append(pd.read_csv(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2020) + '.csv'))\n",
        "        if INC2021 and os.path.exists(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2021) + '.csv'): dfs.append(pd.read_csv(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2021) + '.csv'))\n",
        "    df = pd.concat(dfs, axis = 0) if len(dfs) > 1 else dfs[0]\n",
        "    df['date'] = pd.to_datetime(df['timestamp'], unit = 's')\n",
        "    if LOAD_STRICT: df = df.loc[df['date'] < \"2021-06-13 00:00:00\"]    \n",
        "    df = df.sort_values('date')\n",
        "    return df\n",
        "\n",
        "def load_data_for_all_assets():\n",
        "    dfs = []\n",
        "    for asset_id in list(extra_data_files.keys()): dfs.append(load_training_data_for_asset(asset_id))\n",
        "    return pd.concat(dfs)"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "papermill": {
          "duration": 24.69094,
          "end_time": "2021-11-29T18:06:00.13746",
          "exception": false,
          "start_time": "2021-11-29T18:05:35.44652",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2022-02-17T00:37:34.619083Z",
          "iopub.execute_input": "2022-02-17T00:37:34.61937Z",
          "iopub.status.idle": "2022-02-17T00:37:57.756015Z",
          "shell.execute_reply.started": "2022-02-17T00:37:34.619333Z",
          "shell.execute_reply": "2022-02-17T00:37:57.755291Z"
        },
        "trusted": true,
        "id": "n8CHJ_6MgdaQ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = dt.fread('orig_example_test.jay').to_pandas()\n",
        "sample_prediction_df = dt.fread('orig_example_sample_submission.jay').to_pandas()\n",
        "assets = dt.fread('orig_asset_details.jay').to_pandas()\n",
        "assets_order = dt.fread('orig_supplemental_train.jay').to_pandas().Asset_ID[:N_ASSETS]\n",
        "\n",
        "assets_order = dict((t,i) for i,t in enumerate(assets_order))\n",
        "print(\"Loaded all data!\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T00:37:57.758407Z",
          "iopub.execute_input": "2022-02-17T00:37:57.758916Z",
          "iopub.status.idle": "2022-02-17T00:37:57.865418Z",
          "shell.execute_reply.started": "2022-02-17T00:37:57.758876Z",
          "shell.execute_reply": "2022-02-17T00:37:57.864602Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFd-WzivgdaT",
        "outputId": "edd830bc-57bc-42ab-e7fd-fa763d84dea3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded all data!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Memory saving function credit to https://www.kaggle.com/gemartin/load-data-reduce-memory-usage\n",
        "def reduce_mem_usage(df):\n",
        "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
        "        to reduce memory usage.\n",
        "    \"\"\"\n",
        "    start_mem = df.memory_usage().sum() / 1024**2\n",
        "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
        "\n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtype.name\n",
        "        \n",
        "        if col_type not in ['object', 'category', 'datetime64[ns, UTC]', 'datetime64[ns]']:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)  \n",
        "            else:\n",
        "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)\n",
        "\n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
        "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "_kg_hide-output": true,
        "papermill": {
          "duration": 0.054816,
          "end_time": "2021-11-29T18:06:17.597492",
          "exception": false,
          "start_time": "2021-11-29T18:06:17.542676",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2022-02-17T00:38:08.068529Z",
          "iopub.execute_input": "2022-02-17T00:38:08.069014Z",
          "iopub.status.idle": "2022-02-17T00:38:08.083878Z",
          "shell.execute_reply.started": "2022-02-17T00:38:08.068972Z",
          "shell.execute_reply": "2022-02-17T00:38:08.08305Z"
        },
        "trusted": true,
        "id": "n1itrPqGgdaW"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "lmMrk6vqGrmW"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def get_time_features(df, row=False):  # TODO: can this be used as an alternative to temporal embeddings ?\n",
        "    if not row:\n",
        "        df = df.reset_index()\n",
        "        timestamp = df[\"timestamp\"].map(timestamp_to_date)\n",
        "        df[\"sin_month\"] = (np.sin(2 * np.pi * timestamp.dt.month/12))\n",
        "        df[\"cos_month\"] = (np.cos(2 * np.pi * timestamp.dt.month/12))\n",
        "        df[\"sin_day\"] = (np.sin(2 * np.pi * timestamp.dt.day/31))\n",
        "        df[\"cos_day\"] = (np.cos(2 * np.pi * timestamp.dt.day/31))\n",
        "        df[\"sin_hour\"] = (np.sin(2 * np.pi * timestamp.dt.hour/24))\n",
        "        df[\"cos_hour\"] = (np.cos(2 * np.pi * timestamp.dt.hour/24))\n",
        "        df[\"sin_minute\"] = (np.sin(2 * np.pi * timestamp.dt.minute/60))\n",
        "        df[\"cos_minute\"] = (np.cos(2 * np.pi * timestamp.dt.minute/60))\n",
        "        df = df.set_index(\"timestamp\")\n",
        "    return df\n",
        "\n",
        "\n",
        "def upper_shadow(df): return df['High'] - np.maximum(df['Close'], df['Open'])\n",
        "def lower_shadow(df): return np.minimum(df['Close'], df['Open']) - df['Low']\n",
        "\n",
        "# calculate z-score\n",
        "def zscore(x, window):\n",
        "    r = x.rolling(window=window, min_periods = 1)\n",
        "    m = r.mean()\n",
        "    s = r.std(ddof=0)\n",
        "    z = (x-m)/s\n",
        "    return z\n",
        "def upper_shadow_15(df): return df['High'].rolling(window=15, min_periods=1).max() - np.maximum(df['Close'], df['Open']).shift(15)\n",
        "def lower_shadow_15(df): return np.minimum(df['Close'], df['Open']).shift(15) - df['Low'].rolling(window=15, min_periods=1).min()\n",
        "\n",
        "\n",
        "def get_features(df, row = False):\n",
        "    df_feat = df\n",
        "    df_feat['spread'] = df_feat['High'] - df_feat['Low']\n",
        "    df_feat['mean_trade'] = df_feat['Volume']/df_feat['Count']\n",
        "    df_feat['log_price_change'] = np.log(df_feat['Close']/df_feat['Open'])\n",
        "    df_feat['upper_Shadow'] = upper_shadow(df_feat)\n",
        "    df_feat['lower_Shadow'] = lower_shadow(df_feat)\n",
        "    df_feat[\"high_div_low\"] = df_feat[\"High\"] / df_feat[\"Low\"]\n",
        "    df_feat['UPS'] = (df_feat['High'] - np.maximum(df_feat['Close'], df_feat['Open']))\n",
        "    df_feat['LOS'] = (np.minimum(df_feat['Close'], df_feat['Open']) - df_feat['Low'])\n",
        "    df_feat['LOGVOL'] = np.log(1. + df_feat['Volume'])\n",
        "    df_feat['LOGCNT'] = np.log(1. + df_feat['Count'])\n",
        "    return df_feat\n",
        "\n",
        "\n",
        "# A utility function to build features around lags.\n",
        "def get_features_hist(df_feat, row=False):\n",
        "    \n",
        "    ### features to consider. See potential features...\n",
        "    ### note that we predicting returns 15 minutes ahead. This minutes price data is therefore not sufficient. We must roll the variables, 15, 30, 90, 250, 1250\n",
        "       \n",
        "    # df_feat = df[['Open', 'High', 'Low', 'Close', 'Volume', 'Count', 'VWAP', 'lr15', 'm', 'lr16', 'Asset_ID']].copy()\n",
        "\n",
        "    if df_feat.shape[0]<3750:\n",
        "        df_feat['beta_num'] = np.nan\n",
        "        df_feat['m2'] = np.nan\n",
        "    else:\n",
        "        df_feat['beta_num'] = (df_feat['lr15']*df_feat['m']).rolling(3750).mean().values\n",
        "        df_feat['m2'] = (df_feat['m']*df_feat['m']).rolling(3750).mean().values\n",
        "        \n",
        "    if row:\n",
        "        # first .iloc as far back as we need, compute feature, then downsize until .iloc[-1]\n",
        "        df_feat = df_feat.iloc[-1]\n",
        "#         mean_price = df_feat[['Open', 'High', 'Low', 'Close']].mean()\n",
        "#         med_price = df_feat[['Open', 'High', 'Low', 'Close']].median()\n",
        "        df_feat['upper_shadow'] = df_feat['High'] / df_feat[['Close', 'Open']].max()\n",
        "        df_feat['lower_shadow'] = df_feat[['Close', 'Open']].min() / df_feat['Low']\n",
        "    else:\n",
        "#         mean_price = df_feat[['Open', 'High', 'Low', 'Close']].mean(axis=1)\n",
        "#         med_price = df_feat[['Open', 'High', 'Low', 'Close']].median(axis=1)\n",
        "        df_feat['upper_shadow'] = df_feat['High'] / df_feat[['Close', 'Open']].max(axis=1)\n",
        "        df_feat['lower_shadow'] = df_feat[['Close', 'Open']].min(axis=1) / df_feat['Low']\n",
        "        # df_feat = df_feat.drop('Asset_ID', axis=1)\n",
        "        \n",
        "    df_feat['beta'] = np.nan_to_num(df_feat['beta_num'] / df_feat['m2'], nan=0., posinf=0., neginf=0.)\n",
        "    df_feat['target_lag'] = df_feat['lr15'] - df_feat['beta']*df_feat['m']  # first 15 entries of target_lagg are NaN\n",
        "\n",
        "        ### Sense checks\n",
        "#         print((df_feat['Target'] - df_feat.groupby('Asset_ID')['target_lag'].shift(-16)).abs().mean())\n",
        "#         print(df_feat.loc[(df_feat.Target.isnull())&(df_feat.target_lag.notnull())].shape)\n",
        "#         print(df_feat.loc[(df_feat.Target.notnull())&(df_feat.target_lag.isnull())].shape)        \n",
        "        \n",
        "    df_feat['open2close'] = df_feat['Close'] / df_feat['Open']\n",
        "    df_feat['high2low'] = df_feat['High'] / df_feat['Low']\n",
        "           \n",
        "#     df_feat['high2mean'] = df_feat['High'] / mean_price\n",
        "#     df_feat['low2mean'] = df_feat['Low'] / mean_price\n",
        "#     df_feat['high2median'] = df_feat['High'] / med_price\n",
        "#     df_feat['low2median'] = df_feat['Low'] / med_price\n",
        "    df_feat['volume2count'] = df_feat['Volume'] / (df_feat['Count'] + 1)\n",
        "    df_feat['close2vwap'] = df_feat['Close'] / df_feat['VWAP']\n",
        "    \n",
        "\n",
        "    # https://www.kaggle.com/rafalradwanski/g-research-features-to-increase-your-score\n",
        "    df_feat['zscore_15_Close'] = zscore(df_feat['Close'], 15)\n",
        "    df_feat['zscore_lower_shadow_15'] = zscore(lower_shadow_15(df_feat), 1440*30*12)\n",
        "    df_feat['zscore_upper_shadow_15'] = zscore(upper_shadow_15(df_feat), 1440*30*12)\n",
        "    df_feat['Volume_change_15'] = (df_feat['Volume'] / df_feat['Volume'].shift(15)) - 1\n",
        "    df_feat['Volume_change_60'] = (df_feat['Volume'] / df_feat['Volume'].shift(60)) - 1\n",
        "    df_feat['Return_15'] = (df_feat['Close'] / df_feat['Close'].shift(15)) - 1\n",
        "    df_feat['Return_1h'] = (df_feat['Close'] / df_feat['Close'].shift(60)) - 1\n",
        "    df_feat['Return_1d'] = (df_feat['Close'] / df_feat['Close'].shift(60*24)) - 1\n",
        "    df_feat['Return_1w'] = (df_feat['Close'] / df_feat['Close'].shift(60*24*7)) - 1\n",
        "    #df_feat['Candle_body_%'] = (df_feat['Close'] / df_feat['Open']) - 1\n",
        "    df_feat['ATR_15'] = ((df_feat['High'].rolling(window=15, min_periods=1).max())/ (df_feat['Low'].rolling(window=15, min_periods=1).min()) - 1)\n",
        "    df_feat['ATR_1h'] = ((df_feat['High'].rolling(window=60, min_periods=1).max())/ (df_feat['Low'].rolling(window=60, min_periods=1).min()) - 1)\n",
        "    df_feat['ATR_1d'] = ((df_feat['High'].rolling(window=60*24, min_periods=1).max())/ (df_feat['Low'].rolling(window=60*24, min_periods=1).min()) - 1)\n",
        "    df_feat['Z-score_return15_60'] = zscore(df_feat['Return_15'], 60)\n",
        "\n",
        "    df_feat = df_feat.fillna(0)\n",
        "    df_feat = df_feat.replace([np.inf, -np.inf], value=0)\n",
        "\n",
        "    return df_feat\n",
        "\n",
        "def log_return(series, periods=1):\n",
        "    return np.log(series).diff(periods=periods)\n",
        "\n",
        "def timestamp_to_date(timestamp):\n",
        "    return(datetime.fromtimestamp(timestamp))\n",
        "\n",
        "def get_lead_features():\n",
        "    # get_features from ethereum\n",
        "    pass\n",
        "\n"
      ],
      "metadata": {
        "papermill": {
          "duration": 37.064504,
          "end_time": "2021-11-29T18:06:54.699951",
          "exception": false,
          "start_time": "2021-11-29T18:06:17.635447",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2022-02-17T00:38:08.085233Z",
          "iopub.execute_input": "2022-02-17T00:38:08.085625Z",
          "iopub.status.idle": "2022-02-17T00:38:08.113078Z",
          "shell.execute_reply.started": "2022-02-17T00:38:08.085585Z",
          "shell.execute_reply": "2022-02-17T00:38:08.112226Z"
        },
        "trusted": true,
        "id": "nRgybcglgdaX"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = load_data_for_all_assets().sort_values('timestamp').set_index(\"timestamp\")\n",
        "if DEBUG: \n",
        "    train = train[int(-1*num_debug_samples):]  # [-2221694:]  #\n",
        "else:\n",
        "    train = train[1000000:]\n",
        "print(train.shape)\n",
        "\n",
        "## FILTER DATAFRAME IF ONLY WORKING WITH ONE ASSET\n",
        "if SINGLE_ASSET:\n",
        "    train = train[train[\"Asset_ID\"]==asset_id]\n",
        "gc.collect()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T00:38:08.116698Z",
          "iopub.execute_input": "2022-02-17T00:38:08.117242Z",
          "iopub.status.idle": "2022-02-17T00:38:08.295694Z",
          "shell.execute_reply.started": "2022-02-17T00:38:08.117207Z",
          "shell.execute_reply": "2022-02-17T00:38:08.29475Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dg3jyBJJgdaa",
        "outputId": "931c4f19-ce69-4a48-d307-ae98d0cb9e41"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2000000, 10)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train[['Count', 'Open', 'High', 'Low', 'Close', 'Volume', 'VWAP', 'Target']] = train[['Count', 'Open', 'High', 'Low', 'Close', 'Volume', 'VWAP', 'Target']].astype(np.float32)\n",
        "print(train.shape)\n",
        "train['Target'] = train['Target'].fillna(0)\n",
        "VWAP_max = np.max(train[np.isfinite(train.VWAP)].VWAP)\n",
        "VWAP_min = np.min(train[np.isfinite(train.VWAP)].VWAP)\n",
        "train['VWAP'] = np.nan_to_num(train.VWAP, posinf=VWAP_max, neginf=VWAP_min)\n",
        "df = train[['Asset_ID', 'Target']].copy()\n",
        "times = dict((t,i) for i,t in enumerate(df.index.unique()))\n",
        "df['id'] = df.index.map(times)\n",
        "df['id'] = df['id'].astype(str) + '_' + df['Asset_ID'].astype(str)\n",
        "ids = df.id.copy()\n",
        "del df\n",
        "\n",
        "train = train.sort_index()\n",
        "ind = train.index.unique()\n",
        "def reindex(df):\n",
        "    df = df.reindex(range(ind[0],ind[-1]+60,60),method='nearest')\n",
        "    df = df.fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
        "    return df\n",
        "train = train.groupby('Asset_ID').apply(reindex).reset_index(0, drop=True).sort_index()\n",
        "gc.collect()\n",
        "print(train.shape)\n",
        "\n",
        "# Matching records and marking generated rows as 'non-real'\n",
        "print(train.shape)\n",
        "train['group_num'] = train.index.map(times)\n",
        "train = train.dropna(subset=['group_num'])\n",
        "train['group_num'] = train['group_num'].astype('int')\n",
        "train['id'] = train['group_num'].astype(str) + '_' + train['Asset_ID'].astype(str)\n",
        "train['is_real'] = train.id.isin(ids) * 1\n",
        "train = train.drop('id', axis=1)\n",
        "print(train.shape)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T00:38:08.296841Z",
          "iopub.execute_input": "2022-02-17T00:38:08.297078Z",
          "iopub.status.idle": "2022-02-17T00:38:08.760404Z",
          "shell.execute_reply.started": "2022-02-17T00:38:08.297033Z",
          "shell.execute_reply": "2022-02-17T00:38:08.759366Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qeyD6yKfgdab",
        "outputId": "9b012099-563f-4f12-cc7f-2b670f58c88b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(142862, 10)\n",
            "(142867, 10)\n",
            "(142867, 10)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:28: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(142862, 12)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train['asset_order'] = train.Asset_ID.map(assets_order)\n",
        "train = train.sort_values(by=['group_num', 'asset_order'])\n",
        "# train = reduce_mem_usage(train)\n",
        "gc.collect()"
      ],
      "metadata": {
        "papermill": {
          "duration": 38.374972,
          "end_time": "2021-11-29T18:09:38.027192",
          "exception": false,
          "start_time": "2021-11-29T18:08:59.65222",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2022-02-17T00:38:08.762072Z",
          "iopub.execute_input": "2022-02-17T00:38:08.762339Z",
          "iopub.status.idle": "2022-02-17T00:38:08.938219Z",
          "shell.execute_reply.started": "2022-02-17T00:38:08.762303Z",
          "shell.execute_reply": "2022-02-17T00:38:08.93743Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3PG8wEFtgdad",
        "outputId": "cb514c7c-08c4-481d-99b6-55c66b20ba21"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if train.index.name == \"timestamp\":\n",
        "    train = train.reset_index()\n",
        "train['lr15'] = train.groupby('Asset_ID')['Close'].apply(lambda x: log_return(x, 15))\n",
        "train['lr16'] = train.groupby('Asset_ID')['Close'].apply(lambda x: log_return(x, 16))\n",
        "train = train.merge(df_asset_details[['Asset_ID','Weight']], how='left', on = 'Asset_ID')\n",
        "train['m'] = train['lr15']*train['Weight']\n",
        "train['m'] = train.groupby('timestamp')['m'].transform('sum') / np.sum(df_asset_details['Weight'])\n",
        "\n",
        "train.head(20)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T00:38:08.939735Z",
          "iopub.execute_input": "2022-02-17T00:38:08.940042Z",
          "iopub.status.idle": "2022-02-17T00:38:09.00107Z",
          "shell.execute_reply.started": "2022-02-17T00:38:08.939984Z",
          "shell.execute_reply": "2022-02-17T00:38:09.000329Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 677
        },
        "id": "WbOjRhcpgdae",
        "outputId": "7001dd78-818e-471c-b80f-e94934de91e6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-292a3408-408b-44b1-a447-3eaebe7af510\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>timestamp</th>\n",
              "      <th>Asset_ID</th>\n",
              "      <th>Count</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Volume</th>\n",
              "      <th>VWAP</th>\n",
              "      <th>Target</th>\n",
              "      <th>date</th>\n",
              "      <th>group_num</th>\n",
              "      <th>is_real</th>\n",
              "      <th>asset_order</th>\n",
              "      <th>lr15</th>\n",
              "      <th>lr16</th>\n",
              "      <th>Weight</th>\n",
              "      <th>m</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1614970380</td>\n",
              "      <td>3</td>\n",
              "      <td>1073.0</td>\n",
              "      <td>1.166615</td>\n",
              "      <td>1.167175</td>\n",
              "      <td>1.161562</td>\n",
              "      <td>1.165562</td>\n",
              "      <td>959713.937500</td>\n",
              "      <td>1.165009</td>\n",
              "      <td>-0.004678</td>\n",
              "      <td>2021-03-05 18:53:00</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.406719</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1614970440</td>\n",
              "      <td>3</td>\n",
              "      <td>1187.0</td>\n",
              "      <td>1.165161</td>\n",
              "      <td>1.165695</td>\n",
              "      <td>1.161240</td>\n",
              "      <td>1.162910</td>\n",
              "      <td>582920.437500</td>\n",
              "      <td>1.163121</td>\n",
              "      <td>-0.004364</td>\n",
              "      <td>2021-03-05 18:54:00</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.406719</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1614970500</td>\n",
              "      <td>3</td>\n",
              "      <td>869.0</td>\n",
              "      <td>1.163272</td>\n",
              "      <td>1.168200</td>\n",
              "      <td>1.162616</td>\n",
              "      <td>1.165958</td>\n",
              "      <td>777516.625000</td>\n",
              "      <td>1.165240</td>\n",
              "      <td>-0.004653</td>\n",
              "      <td>2021-03-05 18:55:00</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.406719</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1614970560</td>\n",
              "      <td>3</td>\n",
              "      <td>607.0</td>\n",
              "      <td>1.166286</td>\n",
              "      <td>1.168378</td>\n",
              "      <td>1.165377</td>\n",
              "      <td>1.166998</td>\n",
              "      <td>384822.093750</td>\n",
              "      <td>1.166897</td>\n",
              "      <td>-0.004888</td>\n",
              "      <td>2021-03-05 18:56:00</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.406719</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1614970620</td>\n",
              "      <td>3</td>\n",
              "      <td>500.0</td>\n",
              "      <td>1.166474</td>\n",
              "      <td>1.167720</td>\n",
              "      <td>1.164744</td>\n",
              "      <td>1.165446</td>\n",
              "      <td>377173.375000</td>\n",
              "      <td>1.166171</td>\n",
              "      <td>-0.003190</td>\n",
              "      <td>2021-03-05 18:57:00</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.406719</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1614970680</td>\n",
              "      <td>3</td>\n",
              "      <td>488.0</td>\n",
              "      <td>1.165892</td>\n",
              "      <td>1.168800</td>\n",
              "      <td>1.163648</td>\n",
              "      <td>1.165696</td>\n",
              "      <td>368391.250000</td>\n",
              "      <td>1.165716</td>\n",
              "      <td>-0.002206</td>\n",
              "      <td>2021-03-05 18:58:00</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.406719</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1614970740</td>\n",
              "      <td>3</td>\n",
              "      <td>480.0</td>\n",
              "      <td>1.165877</td>\n",
              "      <td>1.170000</td>\n",
              "      <td>1.163446</td>\n",
              "      <td>1.165506</td>\n",
              "      <td>605561.000000</td>\n",
              "      <td>1.166067</td>\n",
              "      <td>-0.004436</td>\n",
              "      <td>2021-03-05 18:59:00</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.406719</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1614970800</td>\n",
              "      <td>3</td>\n",
              "      <td>648.0</td>\n",
              "      <td>1.166696</td>\n",
              "      <td>1.170200</td>\n",
              "      <td>1.165500</td>\n",
              "      <td>1.167425</td>\n",
              "      <td>608736.187500</td>\n",
              "      <td>1.167228</td>\n",
              "      <td>-0.002907</td>\n",
              "      <td>2021-03-05 19:00:00</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.406719</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1614970860</td>\n",
              "      <td>3</td>\n",
              "      <td>566.0</td>\n",
              "      <td>1.167069</td>\n",
              "      <td>1.170000</td>\n",
              "      <td>1.162845</td>\n",
              "      <td>1.164543</td>\n",
              "      <td>428201.468750</td>\n",
              "      <td>1.165682</td>\n",
              "      <td>-0.001878</td>\n",
              "      <td>2021-03-05 19:01:00</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.406719</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1614970920</td>\n",
              "      <td>3</td>\n",
              "      <td>590.0</td>\n",
              "      <td>1.164159</td>\n",
              "      <td>1.165900</td>\n",
              "      <td>1.161450</td>\n",
              "      <td>1.162816</td>\n",
              "      <td>722182.312500</td>\n",
              "      <td>1.163320</td>\n",
              "      <td>-0.001526</td>\n",
              "      <td>2021-03-05 19:02:00</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.406719</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>1614970980</td>\n",
              "      <td>3</td>\n",
              "      <td>1210.0</td>\n",
              "      <td>1.162722</td>\n",
              "      <td>1.165500</td>\n",
              "      <td>1.158280</td>\n",
              "      <td>1.160603</td>\n",
              "      <td>746743.125000</td>\n",
              "      <td>1.161398</td>\n",
              "      <td>-0.001409</td>\n",
              "      <td>2021-03-05 19:03:00</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.406719</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>1614971040</td>\n",
              "      <td>3</td>\n",
              "      <td>596.0</td>\n",
              "      <td>1.160416</td>\n",
              "      <td>1.162900</td>\n",
              "      <td>1.158420</td>\n",
              "      <td>1.161409</td>\n",
              "      <td>479921.968750</td>\n",
              "      <td>1.160744</td>\n",
              "      <td>-0.001600</td>\n",
              "      <td>2021-03-05 19:04:00</td>\n",
              "      <td>11</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.406719</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>1614971100</td>\n",
              "      <td>3</td>\n",
              "      <td>554.0</td>\n",
              "      <td>1.160860</td>\n",
              "      <td>1.166076</td>\n",
              "      <td>1.159849</td>\n",
              "      <td>1.162176</td>\n",
              "      <td>722702.937500</td>\n",
              "      <td>1.162317</td>\n",
              "      <td>0.002723</td>\n",
              "      <td>2021-03-05 19:05:00</td>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.406719</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>1614971160</td>\n",
              "      <td>3</td>\n",
              "      <td>451.0</td>\n",
              "      <td>1.161943</td>\n",
              "      <td>1.162800</td>\n",
              "      <td>1.158970</td>\n",
              "      <td>1.160954</td>\n",
              "      <td>914327.875000</td>\n",
              "      <td>1.160721</td>\n",
              "      <td>0.002237</td>\n",
              "      <td>2021-03-05 19:06:00</td>\n",
              "      <td>13</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.406719</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>1614971220</td>\n",
              "      <td>3</td>\n",
              "      <td>540.0</td>\n",
              "      <td>1.160818</td>\n",
              "      <td>1.162600</td>\n",
              "      <td>1.157760</td>\n",
              "      <td>1.160207</td>\n",
              "      <td>393290.312500</td>\n",
              "      <td>1.159973</td>\n",
              "      <td>0.001026</td>\n",
              "      <td>2021-03-05 19:07:00</td>\n",
              "      <td>14</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.406719</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>1614971280</td>\n",
              "      <td>3</td>\n",
              "      <td>404.0</td>\n",
              "      <td>1.159226</td>\n",
              "      <td>1.160558</td>\n",
              "      <td>1.157850</td>\n",
              "      <td>1.159554</td>\n",
              "      <td>227465.265625</td>\n",
              "      <td>1.158912</td>\n",
              "      <td>0.000414</td>\n",
              "      <td>2021-03-05 19:08:00</td>\n",
              "      <td>15</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.005168</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.406719</td>\n",
              "      <td>-0.000557</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>1614971340</td>\n",
              "      <td>3</td>\n",
              "      <td>307.0</td>\n",
              "      <td>1.159387</td>\n",
              "      <td>1.160000</td>\n",
              "      <td>1.158010</td>\n",
              "      <td>1.158738</td>\n",
              "      <td>128807.898438</td>\n",
              "      <td>1.158823</td>\n",
              "      <td>-0.000826</td>\n",
              "      <td>2021-03-05 19:09:00</td>\n",
              "      <td>16</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.003594</td>\n",
              "      <td>-0.005871</td>\n",
              "      <td>4.406719</td>\n",
              "      <td>-0.000387</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>1614971400</td>\n",
              "      <td>3</td>\n",
              "      <td>668.0</td>\n",
              "      <td>1.159262</td>\n",
              "      <td>1.161300</td>\n",
              "      <td>1.156050</td>\n",
              "      <td>1.158878</td>\n",
              "      <td>680162.625000</td>\n",
              "      <td>1.159220</td>\n",
              "      <td>0.001253</td>\n",
              "      <td>2021-03-05 19:10:00</td>\n",
              "      <td>17</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.006091</td>\n",
              "      <td>-0.003474</td>\n",
              "      <td>4.406719</td>\n",
              "      <td>-0.000657</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>1614971460</td>\n",
              "      <td>3</td>\n",
              "      <td>1109.0</td>\n",
              "      <td>1.158689</td>\n",
              "      <td>1.160000</td>\n",
              "      <td>1.154500</td>\n",
              "      <td>1.156387</td>\n",
              "      <td>529555.750000</td>\n",
              "      <td>1.157131</td>\n",
              "      <td>0.000142</td>\n",
              "      <td>2021-03-05 19:11:00</td>\n",
              "      <td>18</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.009134</td>\n",
              "      <td>-0.008242</td>\n",
              "      <td>4.406719</td>\n",
              "      <td>-0.000985</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>1614971520</td>\n",
              "      <td>3</td>\n",
              "      <td>598.0</td>\n",
              "      <td>1.156798</td>\n",
              "      <td>1.159300</td>\n",
              "      <td>1.155429</td>\n",
              "      <td>1.157631</td>\n",
              "      <td>719086.562500</td>\n",
              "      <td>1.157503</td>\n",
              "      <td>-0.000820</td>\n",
              "      <td>2021-03-05 19:12:00</td>\n",
              "      <td>19</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.006728</td>\n",
              "      <td>-0.008059</td>\n",
              "      <td>4.406719</td>\n",
              "      <td>-0.000725</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-292a3408-408b-44b1-a447-3eaebe7af510')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-292a3408-408b-44b1-a447-3eaebe7af510 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-292a3408-408b-44b1-a447-3eaebe7af510');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "     timestamp  Asset_ID   Count      Open      High       Low     Close  \\\n",
              "0   1614970380         3  1073.0  1.166615  1.167175  1.161562  1.165562   \n",
              "1   1614970440         3  1187.0  1.165161  1.165695  1.161240  1.162910   \n",
              "2   1614970500         3   869.0  1.163272  1.168200  1.162616  1.165958   \n",
              "3   1614970560         3   607.0  1.166286  1.168378  1.165377  1.166998   \n",
              "4   1614970620         3   500.0  1.166474  1.167720  1.164744  1.165446   \n",
              "5   1614970680         3   488.0  1.165892  1.168800  1.163648  1.165696   \n",
              "6   1614970740         3   480.0  1.165877  1.170000  1.163446  1.165506   \n",
              "7   1614970800         3   648.0  1.166696  1.170200  1.165500  1.167425   \n",
              "8   1614970860         3   566.0  1.167069  1.170000  1.162845  1.164543   \n",
              "9   1614970920         3   590.0  1.164159  1.165900  1.161450  1.162816   \n",
              "10  1614970980         3  1210.0  1.162722  1.165500  1.158280  1.160603   \n",
              "11  1614971040         3   596.0  1.160416  1.162900  1.158420  1.161409   \n",
              "12  1614971100         3   554.0  1.160860  1.166076  1.159849  1.162176   \n",
              "13  1614971160         3   451.0  1.161943  1.162800  1.158970  1.160954   \n",
              "14  1614971220         3   540.0  1.160818  1.162600  1.157760  1.160207   \n",
              "15  1614971280         3   404.0  1.159226  1.160558  1.157850  1.159554   \n",
              "16  1614971340         3   307.0  1.159387  1.160000  1.158010  1.158738   \n",
              "17  1614971400         3   668.0  1.159262  1.161300  1.156050  1.158878   \n",
              "18  1614971460         3  1109.0  1.158689  1.160000  1.154500  1.156387   \n",
              "19  1614971520         3   598.0  1.156798  1.159300  1.155429  1.157631   \n",
              "\n",
              "           Volume      VWAP    Target                date  group_num  is_real  \\\n",
              "0   959713.937500  1.165009 -0.004678 2021-03-05 18:53:00          0        1   \n",
              "1   582920.437500  1.163121 -0.004364 2021-03-05 18:54:00          1        1   \n",
              "2   777516.625000  1.165240 -0.004653 2021-03-05 18:55:00          2        1   \n",
              "3   384822.093750  1.166897 -0.004888 2021-03-05 18:56:00          3        1   \n",
              "4   377173.375000  1.166171 -0.003190 2021-03-05 18:57:00          4        1   \n",
              "5   368391.250000  1.165716 -0.002206 2021-03-05 18:58:00          5        1   \n",
              "6   605561.000000  1.166067 -0.004436 2021-03-05 18:59:00          6        1   \n",
              "7   608736.187500  1.167228 -0.002907 2021-03-05 19:00:00          7        1   \n",
              "8   428201.468750  1.165682 -0.001878 2021-03-05 19:01:00          8        1   \n",
              "9   722182.312500  1.163320 -0.001526 2021-03-05 19:02:00          9        1   \n",
              "10  746743.125000  1.161398 -0.001409 2021-03-05 19:03:00         10        1   \n",
              "11  479921.968750  1.160744 -0.001600 2021-03-05 19:04:00         11        1   \n",
              "12  722702.937500  1.162317  0.002723 2021-03-05 19:05:00         12        1   \n",
              "13  914327.875000  1.160721  0.002237 2021-03-05 19:06:00         13        1   \n",
              "14  393290.312500  1.159973  0.001026 2021-03-05 19:07:00         14        1   \n",
              "15  227465.265625  1.158912  0.000414 2021-03-05 19:08:00         15        1   \n",
              "16  128807.898438  1.158823 -0.000826 2021-03-05 19:09:00         16        1   \n",
              "17  680162.625000  1.159220  0.001253 2021-03-05 19:10:00         17        1   \n",
              "18  529555.750000  1.157131  0.000142 2021-03-05 19:11:00         18        1   \n",
              "19  719086.562500  1.157503 -0.000820 2021-03-05 19:12:00         19        1   \n",
              "\n",
              "    asset_order      lr15      lr16    Weight         m  \n",
              "0             0       NaN       NaN  4.406719  0.000000  \n",
              "1             0       NaN       NaN  4.406719  0.000000  \n",
              "2             0       NaN       NaN  4.406719  0.000000  \n",
              "3             0       NaN       NaN  4.406719  0.000000  \n",
              "4             0       NaN       NaN  4.406719  0.000000  \n",
              "5             0       NaN       NaN  4.406719  0.000000  \n",
              "6             0       NaN       NaN  4.406719  0.000000  \n",
              "7             0       NaN       NaN  4.406719  0.000000  \n",
              "8             0       NaN       NaN  4.406719  0.000000  \n",
              "9             0       NaN       NaN  4.406719  0.000000  \n",
              "10            0       NaN       NaN  4.406719  0.000000  \n",
              "11            0       NaN       NaN  4.406719  0.000000  \n",
              "12            0       NaN       NaN  4.406719  0.000000  \n",
              "13            0       NaN       NaN  4.406719  0.000000  \n",
              "14            0       NaN       NaN  4.406719  0.000000  \n",
              "15            0 -0.005168       NaN  4.406719 -0.000557  \n",
              "16            0 -0.003594 -0.005871  4.406719 -0.000387  \n",
              "17            0 -0.006091 -0.003474  4.406719 -0.000657  \n",
              "18            0 -0.009134 -0.008242  4.406719 -0.000985  \n",
              "19            0 -0.006728 -0.008059  4.406719 -0.000725  "
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_split_indices(indices):\n",
        "    train_end_idx = int(len(indices) * (1 - (PCT_VALIDATION+PCT_TEST)/100))\n",
        "    val_end_idx = int(len(indices) * (1 - (PCT_TEST)/100))\n",
        "    return indices[train_end_idx], indices[val_end_idx]\n",
        "\n",
        "train_end_idx = {}\n",
        "val_end_idx = {}\n",
        "if not SINGLE_ASSET:\n",
        "    for i in range(N_ASSETS):\n",
        "        X_asset_indices = train[train[\"Asset_ID\"]==i].index\n",
        "        train_end_idx[i], val_end_idx[i] = get_split_indices(X_asset_indices)\n",
        "else:\n",
        "    X_asset_indices = train[train[\"Asset_ID\"]==asset_id].index\n",
        "    train_end_idx[asset_id], val_end_idx[asset_id] = get_split_indices(X_asset_indices)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T00:38:09.002308Z",
          "iopub.execute_input": "2022-02-17T00:38:09.00274Z",
          "iopub.status.idle": "2022-02-17T00:38:09.013502Z",
          "shell.execute_reply.started": "2022-02-17T00:38:09.002701Z",
          "shell.execute_reply": "2022-02-17T00:38:09.012775Z"
        },
        "trusted": true,
        "id": "QBrweJbVgdaf"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create train data of shape: Batch Size X Window Size X Num Assets --> bBATCH_SIZEon mean over all features\n",
        "# create train lables of shape: batch size X Num Assets  -->  based on target of t(win_len)\n",
        "# given data and index, train batches + labels are created\n",
        "\n",
        "\n",
        "class sample_generator(keras.utils.Sequence):\n",
        "    def __init__(self, x_set, y_set, batch_size, length, prediction_length):\n",
        "        self.x, self.y = x_set, y_set\n",
        "        self.batch_size = batch_size\n",
        "        self.length = length\n",
        "        self.prediction_length = prediction_length\n",
        "        self.size = len(x_set)\n",
        "    def __len__(self): return int((len(self.x)-self.length) / float(self.batch_size))\n",
        "    def __getitem__(self, idx):\n",
        "        batch_x=[]\n",
        "        batch_y=[]\n",
        "        for i in range(self.batch_size):\n",
        "            start_ind = self.batch_size*idx + i\n",
        "            end_ind = start_ind + self.length \n",
        "            if (end_ind+self.prediction_length-1) <= self.size:\n",
        "                batch_x.append(self.x[start_ind : end_ind])\n",
        "                batch_y.append(self.y[end_ind -1: end_ind+self.prediction_length-1]) \n",
        "        x,y = np.array(batch_x), np.array(batch_y)\n",
        "        assert x.shape == (BATCH_SIZE, WINDOW_SIZE,len(features)), f\"Shape Missmatch of train data generator X at idx {str(idx)} and i {str(i)}\"\n",
        "        assert x.shape == (BATCH_SIZE, WINDOW_SIZE,len(features)), f\"Shape Missmatch of train data generator Y at idx {str(idx)} and i {str(i)}\"\n",
        "        return x,y\n",
        "\n",
        "class multivariate_sample_generator(keras.utils.Sequence):\n",
        "    # generator which supports multivariate ex-features input into nbeats\n",
        "    # -> Single asset per sequence\n",
        "    # --> x_sequence.shape = batch_size x Window_size x 1\n",
        "    # --> y_sequence.shape = batch_size x 1\n",
        "    # --> e_sequence.shape = batch_size x Window_size x num_ex_var    \n",
        "    def __init__(self, x_set, y_set, e_set, batch_size, length, prediction_length):\n",
        "        self.x, self.y, self.e = x_set, y_set, e_set\n",
        "        self.batch_size = batch_size\n",
        "        self.length = length\n",
        "        self.prediction_length = prediction_length\n",
        "        self.size = len(x_set)\n",
        "        self.num_assets = self.x.shape[1]\n",
        "    def __len__(self): return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
        "    def __getitem__(self, idx):\n",
        "        # idx: how many times the generator is already called\n",
        "        batch_x=[]\n",
        "        batch_y=[]\n",
        "        batch_e=[]\n",
        "        num_sample=0\n",
        "        while num_sample < self.batch_size:\n",
        "            start_ind = self.batch_size//self.num_assets*idx + num_sample//self.num_assets\n",
        "            end_ind = start_ind + self.length \n",
        "            if (end_ind+self.prediction_length-1) <= self.size:\n",
        "                for a in range(self.num_assets):\n",
        "                    batch_x.append(self.x[start_ind : end_ind, a])\n",
        "                    batch_e.append(self.e[start_ind : end_ind, a, :])\n",
        "                    batch_y.append(self.y[end_ind -1: end_ind+self.prediction_length-1, a]) \n",
        "                    num_sample += 1\n",
        "                    if num_sample >= self.batch_size: break;  ## TODO: FIND BETTER SOLUTION (here we throw aray samples if we break out of loop)\n",
        "        return [np.array(batch_x).reshape(-1,self.length,1), np.array(batch_e)], np.array(batch_y)\n",
        "    \n",
        "\n",
        "def assert_data_validity_generator(gen, batch_size, test_shift_size=1, num_tests=200):\n",
        "    for i in range(num_tests):\n",
        "        num_batch = np.random.randint(0, len(gen), dtype=int)\n",
        "        num_sample = np.random.randint(0, batch_size-1, dtype=int)\n",
        "        assert gen[num_batch][0][num_sample+test_shift_size,-1,0] == gen[num_batch][1][num_sample,0,0], \"Data Missmatch between Series and Target in generator data\"\n",
        "    print(\"All tests passed.\")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T00:38:09.014888Z",
          "iopub.execute_input": "2022-02-17T00:38:09.015352Z",
          "iopub.status.idle": "2022-02-17T00:38:09.035233Z",
          "shell.execute_reply.started": "2022-02-17T00:38:09.015315Z",
          "shell.execute_reply": "2022-02-17T00:38:09.034448Z"
        },
        "trusted": true,
        "id": "n8iZNJFggdag"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_asset = train.copy()\n",
        "X_asset = get_features_hist(X_asset)\n",
        "X_asset[\"Target_shifted\"] = X_asset[\"Target\"].shift(num_shift)\n",
        "X_asset[\"Target\"].plot()\n",
        "\n",
        "train_end_idx[asset_id], val_end_idx[asset_id]\n",
        "features = [\"Target_shifted\"]\n",
        "# features = [\"Target_shifted\", \"m\", \"upper_shadow\", \"lower_shadow\", \"open2close\", \"high2low\", \"volume2count\", \"close2vwap\"] \n",
        "target = [\"Target\"]\n",
        "X_asset.loc[X_asset.is_real==0, features] = 0 \n",
        "X_asset[features] = X_asset[features].fillna(0)\n",
        "scaler = MinMaxScaler()\n",
        "X_asset[features] = scaler.fit_transform(X_asset[features])  # TODO: NEEDS TO BE SEPARATED FOR TRAIN TEST\n",
        " \n",
        "train_generator = sample_generator(X_asset.loc[:train_end_idx[asset_id], features], \n",
        "                                   X_asset.loc[:train_end_idx[asset_id], target], \n",
        "                                   length = WINDOW_SIZE, batch_size = BATCH_SIZE, prediction_length=prediction_length)\n",
        "val_generator = sample_generator(X_asset.loc[train_end_idx[asset_id]:val_end_idx[asset_id], features], \n",
        "                                 X_asset.loc[train_end_idx[asset_id]:val_end_idx[asset_id], target], \n",
        "                                 length = WINDOW_SIZE, batch_size = BATCH_SIZE, prediction_length=prediction_length)\n",
        "test_generator = sample_generator(X_asset.loc[val_end_idx[asset_id]:, features], \n",
        "                                  X_asset.loc[val_end_idx[asset_id]:, target], \n",
        "                                  length = WINDOW_SIZE, batch_size = BATCH_SIZE, prediction_length=prediction_length)\n",
        "\n",
        "print(\"BATCH_SIZE: \", BATCH_SIZE)\n",
        "print(f'Train batch shape: {train_generator[0][0].shape}')\n",
        "print(f'Target batch shape: {train_generator[0][1].shape}')\n",
        "\n",
        "print(\"Num batches of train, validation and test generator: \", len(train_generator),\", \", len(val_generator),\", \", len(test_generator))\n",
        "print(\"Num Training Samples: \", len(train_generator)*BATCH_SIZE)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T00:50:00.197725Z",
          "iopub.execute_input": "2022-02-17T00:50:00.198607Z",
          "iopub.status.idle": "2022-02-17T00:50:00.483976Z",
          "shell.execute_reply.started": "2022-02-17T00:50:00.198534Z",
          "shell.execute_reply": "2022-02-17T00:50:00.482539Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "id": "tQRkhrE-gdah",
        "outputId": "236f375c-ddca-4bce-88ae-61bb25732dae"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BATCH_SIZE:  128\n",
            "Train batch shape: (128, 128, 1)\n",
            "Target batch shape: (128, 1, 1)\n",
            "Num batches of train, validation and test generator:  891 ,  110 ,  110\n",
            "Num Training Samples:  114048\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <span class=\"title-section w3-xxlarge\" id=\"training\">Training 🏋️</span>\n",
        "<hr>\n",
        "\n",
        "Our model will be trained for the number of FOLDS and EPOCHS you chose in the configuration above. Each fold the model with lowest validation loss will be saved and used to predict OOF and test. Adjust the variable `VERBOSE`. The variable `VERBOSE=1 or 2` will display the training and validation loss for each epoch as text. "
      ],
      "metadata": {
        "papermill": {
          "duration": 0.038491,
          "end_time": "2021-11-29T18:09:46.955605",
          "exception": false,
          "start_time": "2021-11-29T18:09:46.917114",
          "status": "completed"
        },
        "tags": [],
        "id": "7SIUAeqjgdai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Time2Vec(keras.layers.Layer):\n",
        "    def __init__(self, kernel_size=1):\n",
        "        super(Time2Vec, self).__init__(trainable=True, name='Time2VecLayer')\n",
        "        self.k = kernel_size\n",
        "    \n",
        "    def build(self, input_shape):  # build automatically executed before layer is called for the first time --  mostly used to instantiate weights\n",
        "        # trend\n",
        "        self.wb = self.add_weight(name='wb',shape=(input_shape[1],),initializer='uniform',trainable=True)\n",
        "        self.bb = self.add_weight(name='bb',shape=(input_shape[1],),initializer='uniform',trainable=True)\n",
        "        # periodic\n",
        "        self.wa = self.add_weight(name='wa',shape=(1, input_shape[1], self.k),initializer='uniform',trainable=True)\n",
        "        self.ba = self.add_weight(name='ba',shape=(1, input_shape[1], self.k),initializer='uniform',trainable=True)\n",
        "        super(Time2Vec, self).build(input_shape)\n",
        "    \n",
        "    def call(self, inputs, **kwargs):  # where the layer logic lives\n",
        "        bias = self.wb * inputs + self.bb\n",
        "        dp = K.dot(inputs, self.wa) + self.ba\n",
        "        wgts = K.sin(dp) # or K.cos(.)\n",
        "\n",
        "        ret = K.concatenate([K.expand_dims(bias, -1), wgts], -1)\n",
        "        ret = K.reshape(ret, (-1, inputs.shape[1]*(self.k+1)))\n",
        "        return ret\n",
        "    \n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], input_shape[1]*(self.k + 1))\n",
        "    \n",
        "\n",
        "# https://towardsdatascience.com/time2vec-for-time-series-features-encoding-a03a4f3f937e\n",
        "class T2V(keras.layers.Layer):\n",
        "    \n",
        "    def __init__(self, output_dim=None, **kwargs):\n",
        "        self.output_dim = output_dim\n",
        "        super(T2V, self).__init__(**kwargs)\n",
        "        \n",
        "    def build(self, input_shape):        \n",
        "        self.W = self.add_weight(name='W',\n",
        "                      shape=(input_shape[-1], self.output_dim),\n",
        "                      initializer='uniform',\n",
        "                      trainable=True)        \n",
        "        self.P = self.add_weight(name='P',\n",
        "                      shape=(input_shape[1], self.output_dim),\n",
        "                      initializer='uniform',\n",
        "                      trainable=True)        \n",
        "        self.w = self.add_weight(name='w',\n",
        "                      shape=(input_shape[1], 1),\n",
        "                      initializer='uniform',\n",
        "                      trainable=True)        \n",
        "        self.p = self.add_weight(name='p',\n",
        "                      shape=(input_shape[1], 1),\n",
        "                      initializer='uniform',\n",
        "                      trainable=True)        \n",
        "        super(T2V, self).build(input_shape)\n",
        "        \n",
        "    def call(self, x):\n",
        "        \n",
        "        original = self.w * x + self.p\n",
        "        sin_trans = K.sin(K.dot(x, self.W) + self.P)\n",
        "        \n",
        "        return K.concatenate([sin_trans, original], -1)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T00:50:00.525214Z",
          "iopub.execute_input": "2022-02-17T00:50:00.52544Z",
          "iopub.status.idle": "2022-02-17T00:50:00.540886Z",
          "shell.execute_reply.started": "2022-02-17T00:50:00.525406Z",
          "shell.execute_reply": "2022-02-17T00:50:00.54001Z"
        },
        "trusted": true,
        "id": "xbUPXzfdgdaj"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://towardsdatascience.com/the-time-series-transformer-2a521a0efad3\n",
        "\n",
        "class AttentionBlock(keras.Model):\n",
        "    def __init__(self, name='AttentionBlock', num_heads=2, head_size=128, ff_dim=None, dropout=0, **kwargs):\n",
        "        super().__init__(name=name, **kwargs)\n",
        "\n",
        "        if ff_dim is None:\n",
        "            ff_dim = head_size\n",
        "\n",
        "        self.attention = MultiHeadAttention(num_heads=num_heads, head_size=head_size, dropout=dropout)\n",
        "        self.attention_dropout = keras.layers.Dropout(dropout)\n",
        "        self.attention_norm = keras.layers.BatchNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.ff_conv1 = keras.layers.Conv1D(filters=ff_dim, kernel_size=1, activation='relu')\n",
        "        # self.ff_conv2 at build()\n",
        "        self.ff_dropout = keras.layers.Dropout(dropout)\n",
        "        self.ff_norm = keras.layers.BatchNormalization(epsilon=1e-6)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.ff_conv2 = keras.layers.Conv1D(filters=input_shape[-1], kernel_size=1) \n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.attention([inputs, inputs])\n",
        "        x = self.attention_dropout(x)\n",
        "        x = self.attention_norm(inputs + x)\n",
        "\n",
        "        x = self.ff_conv1(x)\n",
        "        x = self.ff_conv2(x)\n",
        "        x = self.ff_dropout(x)\n",
        "\n",
        "        x = self.ff_norm(inputs + x)\n",
        "        return x\n",
        "    \n",
        "\n",
        "class ModelTrunk(keras.Model):\n",
        "    def __init__(self, name='ModelTrunk', time2vec_dim=1, num_heads=2, head_size=128, ff_dim=None, num_layers=1, dropout=0, **kwargs):\n",
        "        super().__init__(name=name, **kwargs)\n",
        "        self.time2vec = Time2Vec(kernel_size=time2vec_dim)\n",
        "        if ff_dim is None:\n",
        "            ff_dim = head_size\n",
        "        self.dropout = dropout\n",
        "        self.attention_layers = [AttentionBlock(num_heads=num_heads, head_size=head_size, ff_dim=ff_dim, dropout=dropout) for _ in range(num_layers)]\n",
        "\n",
        "        \n",
        "    def call(self, inputs):\n",
        "        time_embedding = keras.layers.TimeDistributed(self.time2vec)(inputs)\n",
        "        x = K.concatenate([inputs, time_embedding], -1)\n",
        "        for attention_layer in self.attention_layers:\n",
        "            x = attention_layer(x)\n",
        "\n",
        "        return K.reshape(x, (-1, x.shape[1] * x.shape[2])) # flat vector of features out\n",
        "    \n",
        "\n",
        "def build_model_new(\n",
        "    input_shape,  # shape of time series sample\n",
        "    head_size, # size of multi head attention\n",
        "    num_heads,  # number of multi-head attention \n",
        "    ff_dim,  # \n",
        "    num_transformer_blocks, # \n",
        "    mlp_units, # list of N dense layers with i neurons\n",
        "    dropout=0,  # transformer block dropout rate\n",
        "    mlp_dropout=0,  # \n",
        "    time2vec_dim=3\n",
        "):\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    \n",
        "    x = ModelTrunk(num_heads=num_heads, head_size=head_size, ff_dim=ff_dim, num_layers=num_transformer_blocks, time2vec_dim=time2vec_dim, dropout=dropout)(inputs)\n",
        "\n",
        "    # x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
        "    for dim in mlp_units:\n",
        "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
        "        x = layers.Dropout(mlp_dropout)(x)\n",
        "    outputs = layers.Dense(1, kernel_initializer=\"normal\")(x)\n",
        "    return keras.Model(inputs, outputs)\n",
        "\n",
        "\n",
        "# https://github.com/tatp22/multidim-positional-encoding/blob/master/positional_encodings/tf_positional_encodings.py\n",
        "class TFPositionalEncoding1D(tf.keras.layers.Layer):\n",
        "    def __init__(self, channels: int, dtype=tf.float32):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            channels int: The last dimension of the tensor you want to apply pos emb to.\n",
        "        Keyword Args:\n",
        "            dtype: output type of the encodings. Default is \"tf.float32\".\n",
        "        \"\"\"\n",
        "        super(TFPositionalEncoding1D, self).__init__()\n",
        "\n",
        "        self.channels = int(np.ceil(channels / 2) * 2)\n",
        "        self.inv_freq = np.float32(\n",
        "            1\n",
        "            / np.power(\n",
        "                10000, np.arange(0, self.channels, 2) / np.float32(self.channels)\n",
        "            )\n",
        "        )\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, inputs):\n",
        "        \"\"\"\n",
        "        :param tensor: A 3d tensor of size (batch_size, x, ch)\n",
        "        :return: Positional Encoding Matrix of size (batch_size, x, ch)\n",
        "        \"\"\"\n",
        "        if len(inputs.shape) != 3:\n",
        "            raise RuntimeError(\"The input tensor has to be 3d!\")\n",
        "        _, x, org_channels = inputs.shape\n",
        "\n",
        "        dtype = self.inv_freq.dtype\n",
        "        pos_x = tf.range(x, dtype=dtype)\n",
        "        sin_inp_x = tf.einsum(\"i,j->ij\", pos_x, self.inv_freq)\n",
        "        emb = tf.expand_dims(tf.concat((tf.sin(sin_inp_x), tf.cos(sin_inp_x)), -1), 0)\n",
        "        emb = emb[0]  # A bit of a hack\n",
        "        return tf.repeat(emb[None, :, :org_channels], tf.shape(inputs)[0], axis=0)        \n",
        "        "
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T00:50:00.705448Z",
          "iopub.execute_input": "2022-02-17T00:50:00.705827Z",
          "iopub.status.idle": "2022-02-17T00:50:00.731486Z",
          "shell.execute_reply.started": "2022-02-17T00:50:00.705792Z",
          "shell.execute_reply": "2022-02-17T00:50:00.730033Z"
        },
        "trusted": true,
        "id": "5MMg1hmCgdal"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://keras.io/examples/timeseries/timeseries_transformer_classification/\n",
        "\n",
        "\n",
        "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
        "    # Normalization and Attention\n",
        "    x = layers.BatchNormalization(epsilon=1e-6)(inputs)\n",
        "    x = layers.MultiHeadAttention(\n",
        "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
        "    )(x, x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    res = x + inputs\n",
        "\n",
        "    # Feed Forward Part\n",
        "    x = layers.BatchNormalization(epsilon=1e-6)(res)\n",
        "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
        "    return x + res\n",
        "\n",
        "# self.time2vec = Time2Vec(kernel_size=time2vec_dim)\n",
        "\n",
        "def build_model(\n",
        "    input_shape,  # shape of time series sample\n",
        "    head_size, # size of multi head attention\n",
        "    num_heads,  # number of multi-head attention \n",
        "    ff_dim,  # \n",
        "    num_transformer_blocks, # \n",
        "    mlp_units, # list of N dense layers with i neurons\n",
        "    dropout=0,  # transformer block dropout rate\n",
        "    mlp_dropout=0,  # \n",
        "    temp_embedding=False\n",
        "):\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    \n",
        "    if temp_embedding:\n",
        "      time_embedding = TFPositionalEncoding1D(channels=input_shape[-1])(inputs)\n",
        "      x = tf.keras.layers.Add()([inputs, time_embedding])\n",
        "    else:\n",
        "      x = inputs\n",
        "    \n",
        "    for _ in range(num_transformer_blocks):\n",
        "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
        "\n",
        "    x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
        "    for dim in mlp_units:\n",
        "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
        "        x = layers.Dropout(mlp_dropout)(x)\n",
        "        # x = layers.BatchNormalization(epsilon=1e-6)(x)\n",
        "    outputs = layers.Dense(1, kernel_initializer=\"normal\")(x)\n",
        "    return keras.Model(inputs, outputs)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T00:50:00.893052Z",
          "iopub.execute_input": "2022-02-17T00:50:00.893667Z",
          "iopub.status.idle": "2022-02-17T00:50:00.904069Z",
          "shell.execute_reply.started": "2022-02-17T00:50:00.893633Z",
          "shell.execute_reply": "2022-02-17T00:50:00.903093Z"
        },
        "trusted": true,
        "id": "gI2DksYwgdap"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lr_scheduler(epoch, lr, warmup_epochs=15, decay_epochs=100, initial_lr=1e-6, base_lr=1e-3, min_lr=5e-5):\n",
        "    if epoch <= warmup_epochs:\n",
        "        pct = epoch / warmup_epochs\n",
        "        return ((base_lr - initial_lr) * pct) + initial_lr\n",
        "\n",
        "    if epoch > warmup_epochs and epoch < warmup_epochs+decay_epochs:\n",
        "        pct = 1 - ((epoch - warmup_epochs) / decay_epochs)\n",
        "        return ((base_lr - min_lr) * pct) + min_lr\n",
        "\n",
        "    return min_lr"
      ],
      "metadata": {
        "id": "vxiThfy0B1ur"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = train_generator[0][0].shape[1:]\n",
        "\n",
        "model = build_model(\n",
        "    input_shape,\n",
        "    head_size=256,\n",
        "    num_heads=4,\n",
        "    ff_dim=4,\n",
        "    num_transformer_blocks=8,\n",
        "    mlp_units=[128, 128],\n",
        "    mlp_dropout=0.1,\n",
        "    dropout=0.1,\n",
        "    temp_embedding=TEMP_EMBEDDING\n",
        ")\n",
        "\n",
        "# tfp.stats.correlation\n",
        "#tf_corr_new = tfp.stats.correlation(x, y=None, sample_axis=0, event_axis=-1, keepdims=False, name=None)\n",
        "# tf.contrib.metrics.streaming_pearson_correlation(logits,labels)\n",
        "\n",
        "# model.compile(\n",
        "#     optimizer=\"Adam\",\n",
        "#     loss={\"head1\": \"mse\", \"head2\": \"mse\"},\n",
        "#     loss_weights={\"head1\": HEAD1_WEIGHT, \"head2\": HEAD2_WEIGHT},\n",
        "#     metrics={\"head1\": [\"mae\"], \"head2\": [\"mae\"]}\n",
        "# )\n",
        "# \n",
        "loss_func = combined_loss\n",
        "model.compile(\n",
        "    loss=loss_func,\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=1e-2),\n",
        "    metrics=[\"mae\", tfp.stats.correlation],\n",
        ")\n",
        "\n",
        "model.summary()\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T00:50:01.053911Z",
          "iopub.execute_input": "2022-02-17T00:50:01.054523Z",
          "iopub.status.idle": "2022-02-17T00:50:01.738473Z",
          "shell.execute_reply.started": "2022-02-17T00:50:01.054489Z",
          "shell.execute_reply": "2022-02-17T00:50:01.737796Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1sSkWOGIgdat",
        "outputId": "d38db476-335d-4d82-b25f-26802e8870ab"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 128, 1)]     0           []                               \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 128, 1)      4           ['input_1[0][0]']                \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " multi_head_attention (MultiHea  (None, 128, 1)      7169        ['batch_normalization[0][0]',    \n",
            " dAttention)                                                      'batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 128, 1)       0           ['multi_head_attention[0][0]']   \n",
            "                                                                                                  \n",
            " tf.__operators__.add (TFOpLamb  (None, 128, 1)      0           ['dropout[0][0]',                \n",
            " da)                                                              'input_1[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 128, 1)      4           ['tf.__operators__.add[0][0]']   \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv1d (Conv1D)                (None, 128, 4)       8           ['batch_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)            (None, 128, 4)       0           ['conv1d[0][0]']                 \n",
            "                                                                                                  \n",
            " conv1d_1 (Conv1D)              (None, 128, 1)       5           ['dropout_1[0][0]']              \n",
            "                                                                                                  \n",
            " tf.__operators__.add_1 (TFOpLa  (None, 128, 1)      0           ['conv1d_1[0][0]',               \n",
            " mbda)                                                            'tf.__operators__.add[0][0]']   \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 128, 1)      4           ['tf.__operators__.add_1[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " multi_head_attention_1 (MultiH  (None, 128, 1)      7169        ['batch_normalization_2[0][0]',  \n",
            " eadAttention)                                                    'batch_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)            (None, 128, 1)       0           ['multi_head_attention_1[0][0]'] \n",
            "                                                                                                  \n",
            " tf.__operators__.add_2 (TFOpLa  (None, 128, 1)      0           ['dropout_2[0][0]',              \n",
            " mbda)                                                            'tf.__operators__.add_1[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 128, 1)      4           ['tf.__operators__.add_2[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv1d_2 (Conv1D)              (None, 128, 4)       8           ['batch_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_3 (Dropout)            (None, 128, 4)       0           ['conv1d_2[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_3 (Conv1D)              (None, 128, 1)       5           ['dropout_3[0][0]']              \n",
            "                                                                                                  \n",
            " tf.__operators__.add_3 (TFOpLa  (None, 128, 1)      0           ['conv1d_3[0][0]',               \n",
            " mbda)                                                            'tf.__operators__.add_2[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_4 (BatchNo  (None, 128, 1)      4           ['tf.__operators__.add_3[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " multi_head_attention_2 (MultiH  (None, 128, 1)      7169        ['batch_normalization_4[0][0]',  \n",
            " eadAttention)                                                    'batch_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_4 (Dropout)            (None, 128, 1)       0           ['multi_head_attention_2[0][0]'] \n",
            "                                                                                                  \n",
            " tf.__operators__.add_4 (TFOpLa  (None, 128, 1)      0           ['dropout_4[0][0]',              \n",
            " mbda)                                                            'tf.__operators__.add_3[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_5 (BatchNo  (None, 128, 1)      4           ['tf.__operators__.add_4[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv1d_4 (Conv1D)              (None, 128, 4)       8           ['batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_5 (Dropout)            (None, 128, 4)       0           ['conv1d_4[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_5 (Conv1D)              (None, 128, 1)       5           ['dropout_5[0][0]']              \n",
            "                                                                                                  \n",
            " tf.__operators__.add_5 (TFOpLa  (None, 128, 1)      0           ['conv1d_5[0][0]',               \n",
            " mbda)                                                            'tf.__operators__.add_4[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_6 (BatchNo  (None, 128, 1)      4           ['tf.__operators__.add_5[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " multi_head_attention_3 (MultiH  (None, 128, 1)      7169        ['batch_normalization_6[0][0]',  \n",
            " eadAttention)                                                    'batch_normalization_6[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_6 (Dropout)            (None, 128, 1)       0           ['multi_head_attention_3[0][0]'] \n",
            "                                                                                                  \n",
            " tf.__operators__.add_6 (TFOpLa  (None, 128, 1)      0           ['dropout_6[0][0]',              \n",
            " mbda)                                                            'tf.__operators__.add_5[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_7 (BatchNo  (None, 128, 1)      4           ['tf.__operators__.add_6[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv1d_6 (Conv1D)              (None, 128, 4)       8           ['batch_normalization_7[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_7 (Dropout)            (None, 128, 4)       0           ['conv1d_6[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_7 (Conv1D)              (None, 128, 1)       5           ['dropout_7[0][0]']              \n",
            "                                                                                                  \n",
            " tf.__operators__.add_7 (TFOpLa  (None, 128, 1)      0           ['conv1d_7[0][0]',               \n",
            " mbda)                                                            'tf.__operators__.add_6[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_8 (BatchNo  (None, 128, 1)      4           ['tf.__operators__.add_7[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " multi_head_attention_4 (MultiH  (None, 128, 1)      7169        ['batch_normalization_8[0][0]',  \n",
            " eadAttention)                                                    'batch_normalization_8[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_8 (Dropout)            (None, 128, 1)       0           ['multi_head_attention_4[0][0]'] \n",
            "                                                                                                  \n",
            " tf.__operators__.add_8 (TFOpLa  (None, 128, 1)      0           ['dropout_8[0][0]',              \n",
            " mbda)                                                            'tf.__operators__.add_7[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_9 (BatchNo  (None, 128, 1)      4           ['tf.__operators__.add_8[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv1d_8 (Conv1D)              (None, 128, 4)       8           ['batch_normalization_9[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_9 (Dropout)            (None, 128, 4)       0           ['conv1d_8[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_9 (Conv1D)              (None, 128, 1)       5           ['dropout_9[0][0]']              \n",
            "                                                                                                  \n",
            " tf.__operators__.add_9 (TFOpLa  (None, 128, 1)      0           ['conv1d_9[0][0]',               \n",
            " mbda)                                                            'tf.__operators__.add_8[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_10 (BatchN  (None, 128, 1)      4           ['tf.__operators__.add_9[0][0]'] \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_5 (MultiH  (None, 128, 1)      7169        ['batch_normalization_10[0][0]', \n",
            " eadAttention)                                                    'batch_normalization_10[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_10 (Dropout)           (None, 128, 1)       0           ['multi_head_attention_5[0][0]'] \n",
            "                                                                                                  \n",
            " tf.__operators__.add_10 (TFOpL  (None, 128, 1)      0           ['dropout_10[0][0]',             \n",
            " ambda)                                                           'tf.__operators__.add_9[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_11 (BatchN  (None, 128, 1)      4           ['tf.__operators__.add_10[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv1d_10 (Conv1D)             (None, 128, 4)       8           ['batch_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_11 (Dropout)           (None, 128, 4)       0           ['conv1d_10[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_11 (Conv1D)             (None, 128, 1)       5           ['dropout_11[0][0]']             \n",
            "                                                                                                  \n",
            " tf.__operators__.add_11 (TFOpL  (None, 128, 1)      0           ['conv1d_11[0][0]',              \n",
            " ambda)                                                           'tf.__operators__.add_10[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_12 (BatchN  (None, 128, 1)      4           ['tf.__operators__.add_11[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_6 (MultiH  (None, 128, 1)      7169        ['batch_normalization_12[0][0]', \n",
            " eadAttention)                                                    'batch_normalization_12[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_12 (Dropout)           (None, 128, 1)       0           ['multi_head_attention_6[0][0]'] \n",
            "                                                                                                  \n",
            " tf.__operators__.add_12 (TFOpL  (None, 128, 1)      0           ['dropout_12[0][0]',             \n",
            " ambda)                                                           'tf.__operators__.add_11[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_13 (BatchN  (None, 128, 1)      4           ['tf.__operators__.add_12[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv1d_12 (Conv1D)             (None, 128, 4)       8           ['batch_normalization_13[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_13 (Dropout)           (None, 128, 4)       0           ['conv1d_12[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_13 (Conv1D)             (None, 128, 1)       5           ['dropout_13[0][0]']             \n",
            "                                                                                                  \n",
            " tf.__operators__.add_13 (TFOpL  (None, 128, 1)      0           ['conv1d_13[0][0]',              \n",
            " ambda)                                                           'tf.__operators__.add_12[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_14 (BatchN  (None, 128, 1)      4           ['tf.__operators__.add_13[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_7 (MultiH  (None, 128, 1)      7169        ['batch_normalization_14[0][0]', \n",
            " eadAttention)                                                    'batch_normalization_14[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_14 (Dropout)           (None, 128, 1)       0           ['multi_head_attention_7[0][0]'] \n",
            "                                                                                                  \n",
            " tf.__operators__.add_14 (TFOpL  (None, 128, 1)      0           ['dropout_14[0][0]',             \n",
            " ambda)                                                           'tf.__operators__.add_13[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_15 (BatchN  (None, 128, 1)      4           ['tf.__operators__.add_14[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv1d_14 (Conv1D)             (None, 128, 4)       8           ['batch_normalization_15[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_15 (Dropout)           (None, 128, 4)       0           ['conv1d_14[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_15 (Conv1D)             (None, 128, 1)       5           ['dropout_15[0][0]']             \n",
            "                                                                                                  \n",
            " tf.__operators__.add_15 (TFOpL  (None, 128, 1)      0           ['conv1d_15[0][0]',              \n",
            " ambda)                                                           'tf.__operators__.add_14[0][0]']\n",
            "                                                                                                  \n",
            " global_average_pooling1d (Glob  (None, 128)         0           ['tf.__operators__.add_15[0][0]']\n",
            " alAveragePooling1D)                                                                              \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 128)          16512       ['global_average_pooling1d[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_16 (Dropout)           (None, 128)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 128)          16512       ['dropout_16[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_17 (Dropout)           (None, 128)          0           ['dense_1[0][0]']                \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 1)            129         ['dropout_17[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 90,673\n",
            "Trainable params: 90,641\n",
            "Non-trainable params: 32\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 50\n",
        "\n",
        "model_filepath = f\"/content/gdrive/MyDrive/Github/GResearch/G-ResearchCryptoPrediction/models/checkpoints/\"+ \\\n",
        "  f\"tf_model_features_{len(features)}_seqlen_{WINDOW_SIZE}_tempembedding_{TEMP_EMBEDDING}_epochs_{EPOCHS}\"    # +\"cp_{epoch:02d}\"\n",
        "history_filepath = f\"/content/gdrive/MyDrive/Github/GResearch/G-ResearchCryptoPrediction/history/\"+ \\\n",
        "  f\"tf_model_features_{len(features)}_seqlen_{WINDOW_SIZE}_tempembedding_{TEMP_EMBEDDING}_epochs_{EPOCHS}.log\"\n",
        "  \n",
        "callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', mode='min',patience=5, restore_best_weights=True)]\n",
        "callbacks += [keras.callbacks.LearningRateScheduler(lr_scheduler, verbose=1)]\n",
        "callbacks += [keras.callbacks.ModelCheckpoint(model_filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False)]\n",
        "callbacks += [keras.callbacks.CSVLogger(history_filepath, separator=',', append=False)]\n",
        "\n",
        "history = model.fit(\n",
        "            train_generator, \n",
        "            validation_data = (val_generator),\n",
        "            epochs=EPOCHS,\n",
        "            batch_size=BATCH_SIZE,\n",
        "            callbacks=callbacks)\n",
        "\n",
        "model.evaluate(test_generator, verbose=1)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T00:50:01.76068Z",
          "iopub.execute_input": "2022-02-17T00:50:01.760949Z",
          "iopub.status.idle": "2022-02-17T00:59:13.604094Z",
          "shell.execute_reply.started": "2022-02-17T00:50:01.76092Z",
          "shell.execute_reply": "2022-02-17T00:59:13.603416Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxX1sJJKgdaw",
        "outputId": "d0e91a80-ca5a-4d74-ff9f-44131511422a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1: LearningRateScheduler setting learning rate to 1e-06.\n",
            "Epoch 1/50\n",
            "891/891 [==============================] - ETA: 0s - loss: 0.0742 - mae: 0.1891 - correlation: 0.0407\n",
            "Epoch 1: val_loss improved from inf to 0.00106, saving model to /content/gdrive/MyDrive/Github/GResearch/G-ResearchCryptoPrediction/models/checkpoints/tf_model_features_1_seqlen_128_tempembedding_False_epochs_50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as query_layer_call_fn, query_layer_call_and_return_conditional_losses, key_layer_call_fn, key_layer_call_and_return_conditional_losses, value_layer_call_fn while saving (showing 5 of 96). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/Github/GResearch/G-ResearchCryptoPrediction/models/checkpoints/tf_model_features_1_seqlen_128_tempembedding_False_epochs_50/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/Github/GResearch/G-ResearchCryptoPrediction/models/checkpoints/tf_model_features_1_seqlen_128_tempembedding_False_epochs_50/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r891/891 [==============================] - 251s 256ms/step - loss: 0.0742 - mae: 0.1891 - correlation: 0.0407 - val_loss: 0.0011 - val_mae: 0.1209 - val_correlation: 0.1188 - lr: 1.0000e-06\n",
            "\n",
            "Epoch 2: LearningRateScheduler setting learning rate to 6.76e-05.\n",
            "Epoch 2/50\n",
            "891/891 [==============================] - ETA: 0s - loss: -0.0644 - mae: 0.0594 - correlation: 0.1882\n",
            "Epoch 2: val_loss did not improve from 0.00106\n",
            "891/891 [==============================] - 218s 245ms/step - loss: -0.0644 - mae: 0.0594 - correlation: 0.1882 - val_loss: 0.0056 - val_mae: 0.0406 - val_correlation: 0.0293 - lr: 6.7600e-05\n",
            "\n",
            "Epoch 3: LearningRateScheduler setting learning rate to 0.0001342.\n",
            "Epoch 3/50\n",
            "891/891 [==============================] - ETA: 0s - loss: -0.0883 - mae: 0.0436 - correlation: 0.2202\n",
            "Epoch 3: val_loss improved from 0.00106 to -0.05711, saving model to /content/gdrive/MyDrive/Github/GResearch/G-ResearchCryptoPrediction/models/checkpoints/tf_model_features_1_seqlen_128_tempembedding_False_epochs_50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as query_layer_call_fn, query_layer_call_and_return_conditional_losses, key_layer_call_fn, key_layer_call_and_return_conditional_losses, value_layer_call_fn while saving (showing 5 of 96). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/Github/GResearch/G-ResearchCryptoPrediction/models/checkpoints/tf_model_features_1_seqlen_128_tempembedding_False_epochs_50/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/Github/GResearch/G-ResearchCryptoPrediction/models/checkpoints/tf_model_features_1_seqlen_128_tempembedding_False_epochs_50/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r891/891 [==============================] - 232s 260ms/step - loss: -0.0883 - mae: 0.0436 - correlation: 0.2202 - val_loss: -0.0571 - val_mae: 0.0379 - val_correlation: 0.1521 - lr: 1.3420e-04\n",
            "\n",
            "Epoch 4: LearningRateScheduler setting learning rate to 0.00020080000000000003.\n",
            "Epoch 4/50\n",
            "891/891 [==============================] - ETA: 0s - loss: -0.1079 - mae: 0.0237 - correlation: 0.2396\n",
            "Epoch 4: val_loss improved from -0.05711 to -0.07898, saving model to /content/gdrive/MyDrive/Github/GResearch/G-ResearchCryptoPrediction/models/checkpoints/tf_model_features_1_seqlen_128_tempembedding_False_epochs_50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as query_layer_call_fn, query_layer_call_and_return_conditional_losses, key_layer_call_fn, key_layer_call_and_return_conditional_losses, value_layer_call_fn while saving (showing 5 of 96). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/Github/GResearch/G-ResearchCryptoPrediction/models/checkpoints/tf_model_features_1_seqlen_128_tempembedding_False_epochs_50/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/Github/GResearch/G-ResearchCryptoPrediction/models/checkpoints/tf_model_features_1_seqlen_128_tempembedding_False_epochs_50/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r891/891 [==============================] - 232s 260ms/step - loss: -0.1079 - mae: 0.0237 - correlation: 0.2396 - val_loss: -0.0790 - val_mae: 0.0134 - val_correlation: 0.1714 - lr: 2.0080e-04\n",
            "\n",
            "Epoch 5: LearningRateScheduler setting learning rate to 0.00026740000000000005.\n",
            "Epoch 5/50\n",
            "891/891 [==============================] - ETA: 0s - loss: -0.1180 - mae: 0.0147 - correlation: 0.2507\n",
            "Epoch 5: val_loss did not improve from -0.07898\n",
            "891/891 [==============================] - 220s 247ms/step - loss: -0.1180 - mae: 0.0147 - correlation: 0.2507 - val_loss: -0.0695 - val_mae: 0.0117 - val_correlation: 0.1507 - lr: 2.6740e-04\n",
            "\n",
            "Epoch 6: LearningRateScheduler setting learning rate to 0.00033400000000000004.\n",
            "Epoch 6/50\n",
            "891/891 [==============================] - ETA: 0s - loss: -0.1279 - mae: 0.0158 - correlation: 0.2717\n",
            "Epoch 6: val_loss did not improve from -0.07898\n",
            "891/891 [==============================] - 220s 247ms/step - loss: -0.1279 - mae: 0.0158 - correlation: 0.2717 - val_loss: -0.0779 - val_mae: 0.0129 - val_correlation: 0.1687 - lr: 3.3400e-04\n",
            "\n",
            "Epoch 7: LearningRateScheduler setting learning rate to 0.0004006000000000001.\n",
            "Epoch 7/50\n",
            "891/891 [==============================] - ETA: 0s - loss: -0.1331 - mae: 0.0152 - correlation: 0.2813\n",
            "Epoch 7: val_loss improved from -0.07898 to -0.12048, saving model to /content/gdrive/MyDrive/Github/GResearch/G-ResearchCryptoPrediction/models/checkpoints/tf_model_features_1_seqlen_128_tempembedding_False_epochs_50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as query_layer_call_fn, query_layer_call_and_return_conditional_losses, key_layer_call_fn, key_layer_call_and_return_conditional_losses, value_layer_call_fn while saving (showing 5 of 96). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/Github/GResearch/G-ResearchCryptoPrediction/models/checkpoints/tf_model_features_1_seqlen_128_tempembedding_False_epochs_50/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/Github/GResearch/G-ResearchCryptoPrediction/models/checkpoints/tf_model_features_1_seqlen_128_tempembedding_False_epochs_50/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r891/891 [==============================] - 232s 261ms/step - loss: -0.1331 - mae: 0.0152 - correlation: 0.2813 - val_loss: -0.1205 - val_mae: 0.0166 - val_correlation: 0.2576 - lr: 4.0060e-04\n",
            "\n",
            "Epoch 8: LearningRateScheduler setting learning rate to 0.0004672000000000001.\n",
            "Epoch 8/50\n",
            "891/891 [==============================] - ETA: 0s - loss: -0.1375 - mae: 0.0157 - correlation: 0.2906\n",
            "Epoch 8: val_loss did not improve from -0.12048\n",
            "891/891 [==============================] - 220s 247ms/step - loss: -0.1375 - mae: 0.0157 - correlation: 0.2906 - val_loss: -0.1014 - val_mae: 0.0139 - val_correlation: 0.2167 - lr: 4.6720e-04\n",
            "\n",
            "Epoch 9: LearningRateScheduler setting learning rate to 0.0005338000000000001.\n",
            "Epoch 9/50\n",
            "891/891 [==============================] - ETA: 0s - loss: -0.1420 - mae: 0.0159 - correlation: 0.3000\n",
            "Epoch 9: val_loss did not improve from -0.12048\n",
            "891/891 [==============================] - 220s 247ms/step - loss: -0.1420 - mae: 0.0159 - correlation: 0.3000 - val_loss: -0.0956 - val_mae: 0.0258 - val_correlation: 0.2171 - lr: 5.3380e-04\n",
            "\n",
            "Epoch 10: LearningRateScheduler setting learning rate to 0.0006004000000000001.\n",
            "Epoch 10/50\n",
            "891/891 [==============================] - ETA: 0s - loss: -0.1428 - mae: 0.0154 - correlation: 0.3011\n",
            "Epoch 10: val_loss did not improve from -0.12048\n",
            "891/891 [==============================] - 220s 247ms/step - loss: -0.1428 - mae: 0.0154 - correlation: 0.3011 - val_loss: -0.0973 - val_mae: 0.0298 - val_correlation: 0.2243 - lr: 6.0040e-04\n",
            "\n",
            "Epoch 11: LearningRateScheduler setting learning rate to 0.0006670000000000001.\n",
            "Epoch 11/50\n",
            "891/891 [==============================] - ETA: 0s - loss: -0.1426 - mae: 0.0152 - correlation: 0.3003\n",
            "Epoch 11: val_loss improved from -0.12048 to -0.12750, saving model to /content/gdrive/MyDrive/Github/GResearch/G-ResearchCryptoPrediction/models/checkpoints/tf_model_features_1_seqlen_128_tempembedding_False_epochs_50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as query_layer_call_fn, query_layer_call_and_return_conditional_losses, key_layer_call_fn, key_layer_call_and_return_conditional_losses, value_layer_call_fn while saving (showing 5 of 96). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/Github/GResearch/G-ResearchCryptoPrediction/models/checkpoints/tf_model_features_1_seqlen_128_tempembedding_False_epochs_50/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/Github/GResearch/G-ResearchCryptoPrediction/models/checkpoints/tf_model_features_1_seqlen_128_tempembedding_False_epochs_50/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r891/891 [==============================] - 233s 262ms/step - loss: -0.1426 - mae: 0.0152 - correlation: 0.3003 - val_loss: -0.1275 - val_mae: 0.0236 - val_correlation: 0.2786 - lr: 6.6700e-04\n",
            "\n",
            "Epoch 12: LearningRateScheduler setting learning rate to 0.0007336.\n",
            "Epoch 12/50\n",
            "891/891 [==============================] - ETA: 0s - loss: -0.1496 - mae: 0.0163 - correlation: 0.3155\n",
            "Epoch 12: val_loss did not improve from -0.12750\n",
            "891/891 [==============================] - 220s 247ms/step - loss: -0.1496 - mae: 0.0163 - correlation: 0.3155 - val_loss: -0.1210 - val_mae: 0.0252 - val_correlation: 0.2673 - lr: 7.3360e-04\n",
            "\n",
            "Epoch 13: LearningRateScheduler setting learning rate to 0.0008002000000000002.\n",
            "Epoch 13/50\n",
            "891/891 [==============================] - ETA: 0s - loss: -0.1487 - mae: 0.0158 - correlation: 0.3133\n",
            "Epoch 13: val_loss did not improve from -0.12750\n",
            "891/891 [==============================] - 221s 248ms/step - loss: -0.1487 - mae: 0.0158 - correlation: 0.3133 - val_loss: -0.1029 - val_mae: 0.0281 - val_correlation: 0.2339 - lr: 8.0020e-04\n",
            "\n",
            "Epoch 14: LearningRateScheduler setting learning rate to 0.0008668000000000001.\n",
            "Epoch 14/50\n",
            "891/891 [==============================] - ETA: 0s - loss: -0.1459 - mae: 0.0184 - correlation: 0.3103\n",
            "Epoch 14: val_loss did not improve from -0.12750\n",
            "891/891 [==============================] - 221s 248ms/step - loss: -0.1459 - mae: 0.0184 - correlation: 0.3103 - val_loss: -0.0833 - val_mae: 0.0868 - val_correlation: 0.2533 - lr: 8.6680e-04\n",
            "\n",
            "Epoch 15: LearningRateScheduler setting learning rate to 0.0009334000000000001.\n",
            "Epoch 15/50\n",
            "891/891 [==============================] - ETA: 0s - loss: -0.1470 - mae: 0.0169 - correlation: 0.3109\n",
            "Epoch 15: val_loss did not improve from -0.12750\n",
            "891/891 [==============================] - 221s 248ms/step - loss: -0.1470 - mae: 0.0169 - correlation: 0.3109 - val_loss: -0.1196 - val_mae: 0.0358 - val_correlation: 0.2750 - lr: 9.3340e-04\n",
            "\n",
            "Epoch 16: LearningRateScheduler setting learning rate to 0.001.\n",
            "Epoch 16/50\n",
            "891/891 [==============================] - ETA: 0s - loss: -0.1508 - mae: 0.0164 - correlation: 0.3180\n",
            "Epoch 16: val_loss improved from -0.12750 to -0.12965, saving model to /content/gdrive/MyDrive/Github/GResearch/G-ResearchCryptoPrediction/models/checkpoints/tf_model_features_1_seqlen_128_tempembedding_False_epochs_50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as query_layer_call_fn, query_layer_call_and_return_conditional_losses, key_layer_call_fn, key_layer_call_and_return_conditional_losses, value_layer_call_fn while saving (showing 5 of 96). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/Github/GResearch/G-ResearchCryptoPrediction/models/checkpoints/tf_model_features_1_seqlen_128_tempembedding_False_epochs_50/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/Github/GResearch/G-ResearchCryptoPrediction/models/checkpoints/tf_model_features_1_seqlen_128_tempembedding_False_epochs_50/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r891/891 [==============================] - 233s 261ms/step - loss: -0.1508 - mae: 0.0164 - correlation: 0.3180 - val_loss: -0.1296 - val_mae: 0.0205 - val_correlation: 0.2798 - lr: 0.0010\n",
            "\n",
            "Epoch 17: LearningRateScheduler setting learning rate to 0.0009905.\n",
            "Epoch 17/50\n",
            "891/891 [==============================] - ETA: 0s - loss: -0.1532 - mae: 0.0141 - correlation: 0.3205\n",
            "Epoch 17: val_loss improved from -0.12965 to -0.13356, saving model to /content/gdrive/MyDrive/Github/GResearch/G-ResearchCryptoPrediction/models/checkpoints/tf_model_features_1_seqlen_128_tempembedding_False_epochs_50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as query_layer_call_fn, query_layer_call_and_return_conditional_losses, key_layer_call_fn, key_layer_call_and_return_conditional_losses, value_layer_call_fn while saving (showing 5 of 96). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/Github/GResearch/G-ResearchCryptoPrediction/models/checkpoints/tf_model_features_1_seqlen_128_tempembedding_False_epochs_50/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/Github/GResearch/G-ResearchCryptoPrediction/models/checkpoints/tf_model_features_1_seqlen_128_tempembedding_False_epochs_50/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r891/891 [==============================] - 232s 260ms/step - loss: -0.1532 - mae: 0.0141 - correlation: 0.3205 - val_loss: -0.1336 - val_mae: 0.0218 - val_correlation: 0.2889 - lr: 9.9050e-04\n",
            "\n",
            "Epoch 18: LearningRateScheduler setting learning rate to 0.0009809999999999999.\n",
            "Epoch 18/50\n",
            "891/891 [==============================] - ETA: 0s - loss: -0.1548 - mae: 0.0137 - correlation: 0.3234\n",
            "Epoch 18: val_loss improved from -0.13356 to -0.15191, saving model to /content/gdrive/MyDrive/Github/GResearch/G-ResearchCryptoPrediction/models/checkpoints/tf_model_features_1_seqlen_128_tempembedding_False_epochs_50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as query_layer_call_fn, query_layer_call_and_return_conditional_losses, key_layer_call_fn, key_layer_call_and_return_conditional_losses, value_layer_call_fn while saving (showing 5 of 96). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/Github/GResearch/G-ResearchCryptoPrediction/models/checkpoints/tf_model_features_1_seqlen_128_tempembedding_False_epochs_50/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/Github/GResearch/G-ResearchCryptoPrediction/models/checkpoints/tf_model_features_1_seqlen_128_tempembedding_False_epochs_50/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r891/891 [==============================] - 232s 261ms/step - loss: -0.1548 - mae: 0.0137 - correlation: 0.3234 - val_loss: -0.1519 - val_mae: 0.0193 - val_correlation: 0.3231 - lr: 9.8100e-04\n",
            "\n",
            "Epoch 19: LearningRateScheduler setting learning rate to 0.0009715.\n",
            "Epoch 19/50\n",
            "891/891 [==============================] - ETA: 0s - loss: -0.1533 - mae: 0.0145 - correlation: 0.3210\n",
            "Epoch 19: val_loss did not improve from -0.15191\n",
            "891/891 [==============================] - 220s 247ms/step - loss: -0.1533 - mae: 0.0145 - correlation: 0.3210 - val_loss: -0.1490 - val_mae: 0.0245 - val_correlation: 0.3224 - lr: 9.7150e-04\n",
            "\n",
            "Epoch 20: LearningRateScheduler setting learning rate to 0.000962.\n",
            "Epoch 20/50\n",
            "891/891 [==============================] - ETA: 0s - loss: -0.1574 - mae: 0.0122 - correlation: 0.3271\n",
            "Epoch 20: val_loss did not improve from -0.15191\n",
            "891/891 [==============================] - 219s 246ms/step - loss: -0.1574 - mae: 0.0122 - correlation: 0.3271 - val_loss: -0.1112 - val_mae: 0.0231 - val_correlation: 0.2455 - lr: 9.6200e-04\n",
            "\n",
            "Epoch 21: LearningRateScheduler setting learning rate to 0.0009525.\n",
            "Epoch 21/50\n",
            "247/891 [=======>......................] - ETA: 2:27 - loss: -0.1678 - mae: 0.0150 - correlation: 0.3506"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tf.keras.utils.plot_model(get_model(), show_shapes=True)"
      ],
      "metadata": {
        "papermill": {
          "duration": 6.240302,
          "end_time": "2021-11-29T18:09:55.791092",
          "exception": false,
          "start_time": "2021-11-29T18:09:49.55079",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2022-02-17T00:59:13.605398Z",
          "iopub.execute_input": "2022-02-17T00:59:13.605663Z",
          "iopub.status.idle": "2022-02-17T00:59:13.610915Z",
          "shell.execute_reply.started": "2022-02-17T00:59:13.605628Z",
          "shell.execute_reply": "2022-02-17T00:59:13.608967Z"
        },
        "trusted": true,
        "id": "xUVEr5uZgdaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load custom model from gdrive and compile\n",
        "model_filepath = \"/content/gdrive/MyDrive/Github/GResearch/G-ResearchCryptoPrediction/models/checkpoints/tf_model_features_1_seqlen_128_tempembedding_False_epochs_50\"\n",
        "history_filepath = \"/content/gdrive/MyDrive/Github/GResearch/G-ResearchCryptoPrediction/history/tf_model_features_1_seqlen_128_tempembedding_False.log\"\n",
        "model = keras.models.load_model(model_filepath, compile=False) # ,custom_objects={'loss_func':loss_func})\n",
        "model.compile(\n",
        "    loss=loss_func,\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=1e-2),\n",
        "    metrics=[\"mae\", tfp.stats.correlation],)\n",
        "history = pd.read_csv(history_filepath, sep=',', engine='python')"
      ],
      "metadata": {
        "papermill": {
          "duration": 791.992821,
          "end_time": "2021-11-29T18:23:07.875583",
          "exception": false,
          "start_time": "2021-11-29T18:09:55.882762",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2022-02-17T00:59:13.613208Z",
          "iopub.execute_input": "2022-02-17T00:59:13.613664Z",
          "iopub.status.idle": "2022-02-17T00:59:13.619716Z",
          "shell.execute_reply.started": "2022-02-17T00:59:13.613625Z",
          "shell.execute_reply": "2022-02-17T00:59:13.618913Z"
        },
        "trusted": true,
        "id": "-P7xiiNsgdax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "NMxVpq3LyREN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  plt.plot(history.history['val_loss'], label= \"validation_loss\")\n",
        "  plt.plot(history.history['loss'], label= \"train_loss\")\n",
        "  plt.legend(loc=\"upper left\", bbox_to_anchor=(1, 0.5))\n",
        "  plt.show()\n",
        "except AttributeError:\n",
        "  plt.plot(history['val_loss'], label= \"validation_loss\")\n",
        "  plt.plot(history['loss'], label= \"train_loss\")\n",
        "  plt.legend(loc=\"upper left\", bbox_to_anchor=(1, 0.5))\n",
        "  plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T00:59:13.620719Z",
          "iopub.execute_input": "2022-02-17T00:59:13.620977Z",
          "iopub.status.idle": "2022-02-17T00:59:13.834846Z",
          "shell.execute_reply.started": "2022-02-17T00:59:13.620926Z",
          "shell.execute_reply": "2022-02-17T00:59:13.834207Z"
        },
        "trusted": true,
        "id": "o5Xag6hqgday"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['val_mae'], label= \"validation metric (mae)\")\n",
        "plt.plot(history.history['mae'], label= \"train metric (mae)\")\n",
        "plt.legend(loc=\"upper left\", bbox_to_anchor=(1, 0.5))\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T00:59:13.836128Z",
          "iopub.execute_input": "2022-02-17T00:59:13.836366Z",
          "iopub.status.idle": "2022-02-17T00:59:14.056202Z",
          "shell.execute_reply.started": "2022-02-17T00:59:13.836333Z",
          "shell.execute_reply": "2022-02-17T00:59:14.055496Z"
        },
        "trusted": true,
        "id": "nfdEIm0mgdaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history.history.keys()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T00:59:14.057348Z",
          "iopub.execute_input": "2022-02-17T00:59:14.058255Z",
          "iopub.status.idle": "2022-02-17T00:59:14.063936Z",
          "shell.execute_reply.started": "2022-02-17T00:59:14.058214Z",
          "shell.execute_reply": "2022-02-17T00:59:14.063131Z"
        },
        "trusted": true,
        "id": "IWgwFtE0gdaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['val_correlation'], label= \"validation metric W-Corr\")\n",
        "plt.plot(history.history['correlation'], label= \"train metric W-Corr\")\n",
        "plt.legend(loc=\"upper left\", bbox_to_anchor=(1, 0.5))\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T00:59:14.065698Z",
          "iopub.execute_input": "2022-02-17T00:59:14.066047Z",
          "iopub.status.idle": "2022-02-17T00:59:14.281173Z",
          "shell.execute_reply.started": "2022-02-17T00:59:14.066013Z",
          "shell.execute_reply": "2022-02-17T00:59:14.280453Z"
        },
        "trusted": true,
        "id": "eUm5j_C1gda0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create predictions on training set\n",
        "predictions = model.predict(train_generator)\n",
        "y_true = []\n",
        "for x,y in train_generator: y_true.append(y)\n",
        "y_true = np.squeeze(np.concatenate(y_true), axis=-1)\n",
        "\n",
        "\n",
        "print(predictions.shape, y_true.shape)\n",
        "assert predictions.shape == y_true.shape, f\"{predictions.shape}, {y_true.shape}\"\n",
        "\n",
        "# Evaluate predictions on validation set\n",
        "print(\"Window Size: \", WINDOW_SIZE)\n",
        "print(\"Prediction length: \", prediction_length)\n",
        "print(\"Epochs: \", EPOCHS)\n",
        "\n",
        "print('---------------------')\n",
        "print('Asset:    Corr. coef.')\n",
        "print('---------------------')\n",
        "asset_w_corr = []\n",
        "asset_corrs = []\n",
        "asset_mae = []\n",
        "y_true = np.squeeze(y_true)\n",
        "y_pred = np.squeeze(predictions)\n",
        "real_target_ind = np.argwhere(y_true!=0)\n",
        "# asset_id = list(assets_order.keys())[i]\n",
        "# asset_name = assets[assets.Asset_ID == asset_id]['Asset_Name'].item()\n",
        "mae_asset = mae(y_true, y_pred)\n",
        "asset_corr = np.corrcoef(np.nan_to_num(y_pred.flatten()), np.nan_to_num(y_true.flatten()))[0,1]\n",
        "\n",
        "print(f\"corr: {asset_corr:.4f}\")\n",
        "print(f\"mae: {mae_asset:.4f}\")\n",
        "print(\"\\n\")\n",
        "print(f\"Predictions Min: {predictions.min()}, Predictions Max: {predictions.max()}, Predictions mean:{predictions.mean()}, Predictions var:{predictions.var()}\") \n",
        "print(f\"y_true Min: {y_true.min()}, y_true Max: {y_true.max()}, y_true mean:{y_true.mean()}, y_true var:{y_true.var()}\") "
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T00:59:14.283396Z",
          "iopub.execute_input": "2022-02-17T00:59:14.284233Z",
          "iopub.status.idle": "2022-02-17T00:59:25.446139Z",
          "shell.execute_reply.started": "2022-02-17T00:59:14.284191Z",
          "shell.execute_reply": "2022-02-17T00:59:25.44524Z"
        },
        "trusted": true,
        "id": "05KQCO5-gda2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create predictions on validation set\n",
        "predictions = model.predict(val_generator)\n",
        "y_true = []\n",
        "for x,y in val_generator: y_true.append(y)\n",
        "y_true = np.squeeze(np.concatenate(y_true), axis=-1)\n",
        "\n",
        "\n",
        "print(predictions.shape, y_true.shape)\n",
        "assert predictions.shape == y_true.shape, f\"{predictions.shape}, {y_true.shape}\"\n",
        "\n",
        "# Evaluate predictions on validation set\n",
        "print(\"Window Size: \", WINDOW_SIZE)\n",
        "print(\"Prediction length: \", prediction_length)\n",
        "print(\"Epochs: \", EPOCHS)\n",
        "\n",
        "print('---------------------')\n",
        "print('Asset:    Corr. coef.')\n",
        "print('---------------------')\n",
        "asset_w_corr = []\n",
        "asset_corrs = []\n",
        "asset_mae = []\n",
        "y_true = np.squeeze(y_true)\n",
        "y_pred = np.squeeze(predictions)\n",
        "real_target_ind = np.argwhere(y_true!=0)\n",
        "# asset_id = list(assets_order.keys())[i]\n",
        "# asset_name = assets[assets.Asset_ID == asset_id]['Asset_Name'].item()\n",
        "mae_asset = mae(y_true, y_pred)\n",
        "asset_corr = np.corrcoef(np.nan_to_num(y_pred.flatten()), np.nan_to_num(y_true.flatten()))[0,1]\n",
        "\n",
        "print(f\"corr: {asset_corr:.4f}\")\n",
        "print(f\"mae: {mae_asset:.4f}\")\n",
        "print(\"\\n\")\n",
        "print(f\"Predictions Min: {predictions.min()}, Predictions Max: {predictions.max()}, Predictions mean:{predictions.mean()}, Predictions var:{predictions.var()}\") \n",
        "print(f\"y_true Min: {y_true.min()}, y_true Max: {y_true.max()}, y_true mean:{y_true.mean()}, y_true var:{y_true.var()}\") "
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T00:59:25.448301Z",
          "iopub.execute_input": "2022-02-17T00:59:25.448786Z",
          "iopub.status.idle": "2022-02-17T00:59:26.72903Z",
          "shell.execute_reply.started": "2022-02-17T00:59:25.448742Z",
          "shell.execute_reply": "2022-02-17T00:59:26.727478Z"
        },
        "trusted": true,
        "id": "VOXCGhkQgda3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create predictions on validation set\n",
        "predictions = model.predict(test_generator)\n",
        "y_true = []\n",
        "for x,y in test_generator: y_true.append(y)\n",
        "y_true = np.squeeze(np.concatenate(y_true), axis=-1)\n",
        "\n",
        "\n",
        "print(predictions.shape, y_true.shape)\n",
        "assert predictions.shape == y_true.shape, f\"{predictions.shape}, {y_true.shape}\"\n",
        "\n",
        "# Evaluate predictions on validation set\n",
        "print(\"Window Size: \", WINDOW_SIZE)\n",
        "print(\"Prediction length: \", prediction_length)\n",
        "print(\"Epochs: \", EPOCHS)\n",
        "\n",
        "asset_w_corr = []\n",
        "asset_corrs = []\n",
        "asset_mae = []\n",
        "y_true = np.squeeze(y_true)\n",
        "predictions = np.squeeze(predictions)\n",
        "real_target_ind = np.argwhere(y_true!=0)\n",
        "# asset_id = list(assets_order.keys())[i]\n",
        "# asset_name = assets[assets.Asset_ID == asset_id]['Asset_Name'].item()\n",
        "mae_asset = mae(y_true, predictions)\n",
        "asset_corr = np.corrcoef(np.nan_to_num(y_pred.flatten()), np.nan_to_num(y_true.flatten()))[0,1]\n",
        "\n",
        "print('---------------------')\n",
        "print('Asset:    Corr. coef.')\n",
        "print('---------------------')\n",
        "print(f\"corr: {asset_corr:.4f}\")\n",
        "print(f\"mae: {mae_asset:.4f}\")\n",
        "print(\"\\n\")\n",
        "print(f\"Predictions Min: {predictions.min()}, Predictions Max: {predictions.max()}, Predictions mean:{predictions.mean()}, Predictions var:{predictions.var()}\") \n",
        "print(f\"y_true Min: {y_true.min()}, y_true Max: {y_true.max()}, y_true mean:{y_true.mean()}, y_true var:{y_true.var()}\") \n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T00:59:26.731599Z",
          "iopub.execute_input": "2022-02-17T00:59:26.731937Z",
          "iopub.status.idle": "2022-02-17T00:59:27.980154Z",
          "shell.execute_reply.started": "2022-02-17T00:59:26.731852Z",
          "shell.execute_reply": "2022-02-17T00:59:27.979435Z"
        },
        "trusted": true,
        "id": "9VGActtpgda6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "oxHwOrV8gda7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Eq5SN190gda8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "MnRNd5Digda8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "jx43e9xXgda9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "IHhEifvKgda9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "qld_i0lRgda_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "EJkCv_SOgda_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}