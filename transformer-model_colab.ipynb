{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "transformer-model_colab.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# VERSION \n",
        "\n",
        "**Transfromer Colab v1**"
      ],
      "metadata": {
        "id": "u04TwdXA9xSW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Todos\n",
        "\n",
        "- Save model\n",
        "- check for other necessray callbacks\n",
        "- run version 8 with 16 shift\n",
        "- Embedding\n",
        "    - evaluate embedding\n",
        "    - run new embedding version on full asset data\n",
        "    - search an try alternatives\n",
        "        - add sin / cos signals\n",
        "- Check for other LR scheduler\n",
        "- Read for decoder necessety "
      ],
      "metadata": {
        "id": "IeHbanPQgdZ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Kernel"
      ],
      "metadata": {
        "id": "bqI4QtfZgdaC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datatable\n",
        "!pip install tensorflow-addons\n",
        "\n",
        "# memory footprint support libraries/code\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isn’t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        " process = psutil.Process(os.getpid())\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm() \n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5r1geHXmBGX",
        "outputId": "4fa92de7-854b-4e27-f6a8-b6892d976a29"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datatable\n",
            "  Downloading datatable-1.0.0-cp37-cp37m-manylinux_2_12_x86_64.whl (96.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 96.9 MB 118 kB/s \n",
            "\u001b[?25hInstalling collected packages: datatable\n",
            "Successfully installed datatable-1.0.0\n",
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.16.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.16.1\n",
            "Collecting gputil\n",
            "  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n",
            "Building wheels for collected packages: gputil\n",
            "  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gputil: filename=GPUtil-1.4.0-py3-none-any.whl size=7411 sha256=7f668a9567a6bfffcefeb25d6c1512685587db7672af92f2c5ccab840ac4dea8\n",
            "  Stored in directory: /root/.cache/pip/wheels/6e/f8/83/534c52482d6da64622ddbf72cd93c35d2ef2881b78fd08ff0c\n",
            "Successfully built gputil\n",
            "Installing collected packages: gputil\n",
            "Successfully installed gputil-1.4.0\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.7/dist-packages (0.5.1)\n",
            "Gen RAM Free: 11.9 GB  | Proc size: 146.1 MB\n",
            "GPU RAM Free: 11441MB | Used: 0MB | Util   0% | Total 11441MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Only if necessary to kill the session again\n",
        "\n",
        "# !kill -9 -1\n"
      ],
      "metadata": {
        "id": "Rgjhemutvhtc"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %load_ext google.colab.data_table#To diable the display\n",
        "# %unload_ext google.colab.data_table"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "_kg_hide-output": true,
        "papermill": {
          "duration": 0.072966,
          "end_time": "2021-11-29T18:05:23.845682",
          "exception": false,
          "start_time": "2021-11-29T18:05:23.772716",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2022-02-17T00:37:26.676103Z",
          "iopub.execute_input": "2022-02-17T00:37:26.676697Z",
          "iopub.status.idle": "2022-02-17T00:37:26.700579Z",
          "shell.execute_reply.started": "2022-02-17T00:37:26.676605Z",
          "shell.execute_reply": "2022-02-17T00:37:26.69995Z"
        },
        "trusted": true,
        "id": "V7tOVJGxgdaF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gc\n",
        "import datatable as dt\n",
        "import traceback\n",
        "import pdb\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from IPython.core.display import display, HTML, Javascript\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
        "from tensorflow_addons.layers import MultiHeadAttention\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd, numpy as np\n",
        "from functools import partial\n",
        "from scipy.stats import pearsonr\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import RobustScaler, MinMaxScaler\n",
        "from sklearn.metrics import mean_absolute_error as mae\n",
        "\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "%cd /content/gdrive/MyDrive/Github/GResearch/\n",
        "# if not os.path.exists(\"../input/g-research-crypto-forecasting/\"): os.chdir('/t/Datasets/kaggle_crypto/internal')"
      ],
      "metadata": {
        "papermill": {
          "duration": 9.261664,
          "end_time": "2021-11-29T18:05:34.914352",
          "exception": false,
          "start_time": "2021-11-29T18:05:25.652688",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2022-02-17T00:37:27.449865Z",
          "iopub.execute_input": "2022-02-17T00:37:27.450127Z",
          "iopub.status.idle": "2022-02-17T00:37:34.411247Z",
          "shell.execute_reply.started": "2022-02-17T00:37:27.450097Z",
          "shell.execute_reply": "2022-02-17T00:37:34.410533Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fUeURtRcgdaH",
        "outputId": "396ee18e-968b-4589-d1fe-6f2025a4289e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "/content/gdrive/MyDrive/Github/GResearch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = \"GPU\" #or \"TPU\"\n",
        "\n",
        "SEED = 42\n",
        "\n",
        "# LOAD STRICT? YES=1 NO=0 | see: https://www.kaggle.com/julian3833/proposal-for-a-meaningful-lb-strict-lgbm\n",
        "LOAD_STRICT = True\n",
        "\n",
        "# WHICH YEARS TO INCLUDE? YES=1 NO=0\n",
        "INC2021 = 0\n",
        "INC2020 = 0\n",
        "INC2019 = 0\n",
        "INC2018 = 0\n",
        "INC2017 = 0\n",
        "INCCOMP = 1\n",
        "INCSUPP = 0\n",
        "\n",
        "# TRAINING PARAMETERS\n",
        "DEBUG = True\n",
        "SINGLE_ASSET = True\n",
        "asset_id=3\n",
        "N_ASSETS = 14\n",
        "num_shift = 15\n",
        "TEMP_EMBEDDING = False\n",
        "TIME2VEC_DIM=3\n",
        "WINDOW_SIZE = 16\n",
        "prediction_length = 1\n",
        "BATCH_SIZE = 128\n",
        "PCT_VALIDATION = 10 # last 10% of the data are used as validation set\n",
        "PCT_TEST = 10 # last 10% of the data are used as validation set"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.050546,
          "end_time": "2021-11-29T18:05:35.179716",
          "exception": false,
          "start_time": "2021-11-29T18:05:35.12917",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2022-02-17T00:37:34.412864Z",
          "iopub.execute_input": "2022-02-17T00:37:34.413166Z",
          "iopub.status.idle": "2022-02-17T00:37:34.422577Z",
          "shell.execute_reply.started": "2022-02-17T00:37:34.413132Z",
          "shell.execute_reply": "2022-02-17T00:37:34.421333Z"
        },
        "trusted": true,
        "id": "qQD2DffPgdaK"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if DEVICE == \"TPU\":\n",
        "    print(\"connecting to TPU...\")\n",
        "    try:\n",
        "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "        print('Running on TPU ', tpu.master())\n",
        "    except ValueError:\n",
        "        tpu = None\n",
        "    if tpu:\n",
        "        try:\n",
        "            print(\"initializing  TPU ...\")\n",
        "            tf.config.experimental_connect_to_cluster(tpu)\n",
        "            tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "            strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "            print(\"TPU initialized\")\n",
        "        except: print(\"failed to initialize TPU\")\n",
        "    else: DEVICE = \"GPU\"\n",
        "\n",
        "if DEVICE != \"TPU\": strategy = tf.distribute.get_strategy()\n",
        "if DEVICE == \"GPU\": print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
        "AUTO     = tf.data.experimental.AUTOTUNE\n",
        "REPLICAS = strategy.num_replicas_in_sync"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "papermill": {
          "duration": 0.069363,
          "end_time": "2021-11-29T18:05:35.290484",
          "exception": false,
          "start_time": "2021-11-29T18:05:35.221121",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2022-02-17T00:37:34.424968Z",
          "iopub.execute_input": "2022-02-17T00:37:34.425933Z",
          "iopub.status.idle": "2022-02-17T00:37:34.593804Z",
          "shell.execute_reply.started": "2022-02-17T00:37:34.425866Z",
          "shell.execute_reply": "2022-02-17T00:37:34.593104Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgeI6XZUgdaM",
        "outputId": "719b9f48-f8ea-4b1e-a23c-efa313c1d7db"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_target(df):\n",
        "    \"\"\"\n",
        "    \n",
        "    \"\"\"\n",
        "    R=list()\n",
        "    c=list(df['Close'])\n",
        "    for i in range(df.shape[0]):\n",
        "        future=c[min([i+16,df.shape[0]-1])]\n",
        "        past=c[min([i+1,df.shape[0]-1])]\n",
        "        R.append(future/past)\n",
        "    df['R']=R\n",
        "    df['R']=np.log(df['R'])\n",
        "    df['pred']=np.exp(df['R'])-1\n",
        "    return df\n",
        "\n",
        "\n",
        "# we will use weighted correlation as function to evaluate our model performance\n",
        "# https://stackoverflow.com/questions/38641691/weighted-correlation-coefficient-with-pandas\n",
        "def wmean(x, w):\n",
        "    return np.sum(x * w) / np.sum(w)\n",
        "\n",
        "def wcov(x, y, w):\n",
        "    return np.sum(w * (x - wmean(x, w)) * (y - wmean(y, w))) / np.sum(w)\n",
        "\n",
        "def wcorr(x, y, w):\n",
        "    return wcov(x, y, w) / np.sqrt(wcov(x, x, w) * wcov(y, y, w))\n",
        "\n",
        "def wcorr(x, y, w=1):\n",
        "    return wcov(x, y, w) / np.sqrt(wcov(x, x, w) * wcov(y, y, w))\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# ----------------------------------------------------------------\n",
        "# def tf_wmean(x, w):\n",
        "#     w_sum = tf.math.reduce_sum(w)\n",
        "#     xw_sum = tf.math.reduce_sum(tf.math.multiply(x, w))\n",
        "#     return  tf.math.divide(xw_sum, w_sum)\n",
        "# \n",
        "# def tf_wcov(x, y, w):\n",
        "#     w_sum = tf.math.reduce_sum(w)\n",
        "#     xyw = tf.math.multiply(x - tf_wmean(x, w), y - tf_wmean(y, w))\n",
        "#     w_xy_sum= tf.math.reduce_sum(tf.math.multiply(w, xyw))\n",
        "#     return  tf.math.divide(w_xy_sum, w_sum)\n",
        "# \n",
        "# def tf_wcorr(x, y, w=1):\n",
        "#     if not w:\n",
        "#         w=1.\n",
        "#     x = tf.cast(x, tf.float32)\n",
        "#     y = tf.cast(y, tf.float32)\n",
        "#     w = tf.cast(w, tf.float32)\n",
        "#     \n",
        "#     mul_xwcov_y_wcov = tf.math.multiply(tf_wcov(x, x, w), tf_wcov(y, y, w))\n",
        "#     return tf.math.divide(tf_wcov(x, y, w), tf.math.sqrt(mul_xwcov_y_wcov))\n",
        "\n",
        "\n",
        "\n",
        "def corr_loss(y_true, y_pred):\n",
        "    x = tf.cast(y_true, tf.float32)\n",
        "    y = tf.cast(y_pred, tf.float32)\n",
        "    mx = K.mean(x)\n",
        "    my = K.mean(y)\n",
        "    xm, ym = x-mx, y-my\n",
        "    r_num = K.sum(tf.multiply(xm,ym))\n",
        "    r_den = K.sqrt(tf.multiply(K.sum(K.square(xm)), K.sum(K.square(ym))))\n",
        "    r = r_num / r_den\n",
        "    r = K.maximum(K.minimum(r, 1.0), -1.0)\n",
        "    return - r\n",
        "\n",
        "def combined_loss(y_true, y_pred, weight1=0.5, weight2=None):\n",
        "    if not weight2:\n",
        "        weight2 = 1 - weight1\n",
        "    loss1 = corr_loss(y_true, y_pred)\n",
        "    mae_loss_ = tf.keras.losses.MeanAbsoluteError()\n",
        "    loss2 = mae_loss_(y_true, y_pred)\n",
        "    return loss1*weight1 + loss2*weight2\n",
        "\n",
        "\n",
        "def competition_weighted_correlation(a, b, weights=1):\n",
        "\n",
        "    #w = np.ravel(weights)\n",
        "    #a = np.ravel(a)\n",
        "    #b = np.ravel(b)\n",
        "    a = tf.cast(a, dtype=tf.float32)\n",
        "    b = tf.cast(b, dtype=tf.float32)\n",
        "    \n",
        "    len_a = tf.cast(tf.size(a), dtype=tf.float32)\n",
        "    len_b = tf.cast(tf.size(b), dtype=tf.float32)\n",
        "    #sum_w = tf.reduce_sum(w)\n",
        "    mean_a = tf.cast(tf.math.reduce_mean(a), dtype=tf.float32)\n",
        "    mean_b = tf.cast(tf.math.reduce_mean(b), dtype=tf.float32)\n",
        "    var_a = tf.cast(tf.math.reduce_sum(tf.math.square(a - mean_a)) / len_a, dtype=tf.float32)\n",
        "    var_b = tf.cast(tf.math.reduce_sum(tf.math.square(b - mean_b)) / len_b, dtype=tf.float32)\n",
        "\n",
        "    cov = tf.math.reduce_sum((a * b)) / len_a - (mean_a * mean_b)\n",
        "    corr = cov / tf.math.sqrt(var_a * var_b)\n",
        "\n",
        "    return corr"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T00:37:34.597079Z",
          "iopub.execute_input": "2022-02-17T00:37:34.597276Z",
          "iopub.status.idle": "2022-02-17T00:37:34.61722Z",
          "shell.execute_reply.started": "2022-02-17T00:37:34.59725Z",
          "shell.execute_reply": "2022-02-17T00:37:34.616206Z"
        },
        "trusted": true,
        "id": "LqMReNYPgdaO"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extra_data_files = {0: '', \n",
        "                    2: '', \n",
        "                    1: '', \n",
        "                    3: '', \n",
        "                    4: '', \n",
        "                    5: '', \n",
        "                    6: '', \n",
        "                    7: '', \n",
        "                    8: '', \n",
        "                    9: '', \n",
        "                    11:'', \n",
        "                    10:'', \n",
        "                    12:'', \n",
        "                    13:''}\n",
        "\n",
        "# Uncomment to load the original csv [slower]\n",
        "# orig_df_train = pd.read_csv(data_path + 'train.csv') \n",
        "# supp_df_train = pd.read_csv(data_path + 'supplemental_train.csv')\n",
        "# df_asset_details = pd.read_csv(data_path  + 'asset_details.csv').sort_values(\"Asset_ID\")\n",
        "\n",
        "orig_df_train = dt.fread('orig_train.jay').to_pandas()\n",
        "df_asset_details = dt.fread('orig_asset_details.jay').to_pandas()\n",
        "supp_df_train = dt.fread('orig_supplemental_train.jay').to_pandas()\n",
        "assets_details = dt.fread('orig_asset_details.jay').to_pandas()\n",
        "asset_weight_dict = {assets_details['Asset_ID'].tolist()[idx]: assets_details['Weight'].tolist()[idx] for idx in range(len(assets_details))}\n",
        "asset_name_dict = {assets_details['Asset_ID'].tolist()[idx]: assets_details['Asset_Name'].tolist()[idx] for idx in range(len(assets_details))}\n",
        "\n",
        "def load_training_data_for_asset(asset_id, load_jay = True):\n",
        "    dfs = []\n",
        "    if INCCOMP: dfs.append(orig_df_train[orig_df_train[\"Asset_ID\"] == asset_id].copy())\n",
        "    if INCSUPP: dfs.append(supp_df_train[supp_df_train[\"Asset_ID\"] == asset_id].copy())\n",
        "    \n",
        "    if load_jay:\n",
        "        if INC2017 and os.path.exists(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2017) + '.csv'): dfs.append(dt.fread(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2017) + '.jay').to_pandas())\n",
        "        if INC2018 and os.path.exists(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2018) + '.csv'): dfs.append(dt.fread(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2018) + '.jay').to_pandas())\n",
        "        if INC2019 and os.path.exists(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2019) + '.csv'): dfs.append(dt.fread(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2019) + '.jay').to_pandas())\n",
        "        if INC2020 and os.path.exists(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2020) + '.csv'): dfs.append(dt.fread(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2020) + '.jay').to_pandas())\n",
        "        if INC2021 and os.path.exists(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2021) + '.csv'): dfs.append(dt.fread(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2021) + '.jay').to_pandas())\n",
        "    else: \n",
        "        if INC2017 and os.path.exists(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2017) + '.csv'): dfs.append(pd.read_csv(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2017) + '.csv'))\n",
        "        if INC2018 and os.path.exists(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2018) + '.csv'): dfs.append(pd.read_csv(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2018) + '.csv'))\n",
        "        if INC2019 and os.path.exists(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2019) + '.csv'): dfs.append(pd.read_csv(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2019) + '.csv'))\n",
        "        if INC2020 and os.path.exists(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2020) + '.csv'): dfs.append(pd.read_csv(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2020) + '.csv'))\n",
        "        if INC2021 and os.path.exists(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2021) + '.csv'): dfs.append(pd.read_csv(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2021) + '.csv'))\n",
        "    df = pd.concat(dfs, axis = 0) if len(dfs) > 1 else dfs[0]\n",
        "    df['date'] = pd.to_datetime(df['timestamp'], unit = 's')\n",
        "    if LOAD_STRICT: df = df.loc[df['date'] < \"2021-06-13 00:00:00\"]    \n",
        "    df = df.sort_values('date')\n",
        "    return df\n",
        "\n",
        "def load_data_for_all_assets():\n",
        "    dfs = []\n",
        "    for asset_id in list(extra_data_files.keys()): dfs.append(load_training_data_for_asset(asset_id))\n",
        "    return pd.concat(dfs)"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "papermill": {
          "duration": 24.69094,
          "end_time": "2021-11-29T18:06:00.13746",
          "exception": false,
          "start_time": "2021-11-29T18:05:35.44652",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2022-02-17T00:37:34.619083Z",
          "iopub.execute_input": "2022-02-17T00:37:34.61937Z",
          "iopub.status.idle": "2022-02-17T00:37:57.756015Z",
          "shell.execute_reply.started": "2022-02-17T00:37:34.619333Z",
          "shell.execute_reply": "2022-02-17T00:37:57.755291Z"
        },
        "trusted": true,
        "id": "n8CHJ_6MgdaQ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = dt.fread('orig_example_test.jay').to_pandas()\n",
        "sample_prediction_df = dt.fread('orig_example_sample_submission.jay').to_pandas()\n",
        "assets = dt.fread('orig_asset_details.jay').to_pandas()\n",
        "assets_order = dt.fread('orig_supplemental_train.jay').to_pandas().Asset_ID[:N_ASSETS]\n",
        "\n",
        "assets_order = dict((t,i) for i,t in enumerate(assets_order))\n",
        "print(\"Loaded all data!\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T00:37:57.758407Z",
          "iopub.execute_input": "2022-02-17T00:37:57.758916Z",
          "iopub.status.idle": "2022-02-17T00:37:57.865418Z",
          "shell.execute_reply.started": "2022-02-17T00:37:57.758876Z",
          "shell.execute_reply": "2022-02-17T00:37:57.864602Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFd-WzivgdaT",
        "outputId": "2d9ee4e7-2d31-43f2-fee5-1880a18bcdb2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded all data!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = load_data_for_all_assets().sort_values('timestamp').set_index(\"timestamp\")\n",
        "if DEBUG: \n",
        "    train = train[-500000:]  # [-2221694:]  #\n",
        "else:\n",
        "    train = train[1000000:]\n",
        "print(train.shape)"
      ],
      "metadata": {
        "papermill": {
          "duration": 17.031431,
          "end_time": "2021-11-29T18:06:17.210444",
          "exception": false,
          "start_time": "2021-11-29T18:06:00.179013",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2022-02-17T00:37:57.866835Z",
          "iopub.execute_input": "2022-02-17T00:37:57.867087Z",
          "iopub.status.idle": "2022-02-17T00:38:08.067318Z",
          "shell.execute_reply.started": "2022-02-17T00:37:57.867053Z",
          "shell.execute_reply": "2022-02-17T00:38:08.066523Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVNOqB16gdaV",
        "outputId": "8bce9c67-63ff-4ed7-d65b-bda7ca81ee84"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(500000, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Memory saving function credit to https://www.kaggle.com/gemartin/load-data-reduce-memory-usage\n",
        "def reduce_mem_usage(df):\n",
        "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
        "        to reduce memory usage.\n",
        "    \"\"\"\n",
        "    start_mem = df.memory_usage().sum() / 1024**2\n",
        "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
        "\n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtype.name\n",
        "        \n",
        "        if col_type not in ['object', 'category', 'datetime64[ns, UTC]', 'datetime64[ns]']:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)  \n",
        "            else:\n",
        "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)\n",
        "\n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
        "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "_kg_hide-output": true,
        "papermill": {
          "duration": 0.054816,
          "end_time": "2021-11-29T18:06:17.597492",
          "exception": false,
          "start_time": "2021-11-29T18:06:17.542676",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2022-02-17T00:38:08.068529Z",
          "iopub.execute_input": "2022-02-17T00:38:08.069014Z",
          "iopub.status.idle": "2022-02-17T00:38:08.083878Z",
          "shell.execute_reply.started": "2022-02-17T00:38:08.068972Z",
          "shell.execute_reply": "2022-02-17T00:38:08.08305Z"
        },
        "trusted": true,
        "id": "n1itrPqGgdaW"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def upper_shadow(df): return df['High'] - np.maximum(df['Close'], df['Open'])\n",
        "def lower_shadow(df): return np.minimum(df['Close'], df['Open']) - df['Low']\n",
        "\n",
        "def get_features(df, row = False):\n",
        "    df_feat = df\n",
        "    df_feat['spread'] = df_feat['High'] - df_feat['Low']\n",
        "    df_feat['mean_trade'] = df_feat['Volume']/df_feat['Count']\n",
        "    df_feat['log_price_change'] = np.log(df_feat['Close']/df_feat['Open'])\n",
        "    df_feat['upper_Shadow'] = upper_shadow(df_feat)\n",
        "    df_feat['lower_Shadow'] = lower_shadow(df_feat)\n",
        "    df_feat[\"high_div_low\"] = df_feat[\"High\"] / df_feat[\"Low\"]\n",
        "    df_feat['UPS'] = (df_feat['High'] - np.maximum(df_feat['Close'], df_feat['Open']))\n",
        "    df_feat['LOS'] = (np.minimum(df_feat['Close'], df_feat['Open']) - df_feat['Low'])\n",
        "    df_feat['LOGVOL'] = np.log(1. + df_feat['Volume'])\n",
        "    df_feat['LOGCNT'] = np.log(1. + df_feat['Count'])\n",
        "    return df_feat\n",
        "\n",
        "\n",
        "# A utility function to build features around lags.\n",
        "def get_features_hist(df_feat, row=False):\n",
        "    \n",
        "    ### features to consider. See potential features...\n",
        "    ### note that we predicting returns 15 minutes ahead. This minutes price data is therefore not sufficient. We must roll the variables, 15, 30, 90, 250, 1250\n",
        "       \n",
        "    # df_feat = df[['Open', 'High', 'Low', 'Close', 'Volume', 'Count', 'VWAP', 'lr15', 'm', 'lr16', 'Asset_ID']].copy()\n",
        "\n",
        "    if df_feat.shape[0]<3750:\n",
        "        df_feat['beta_num'] = np.nan\n",
        "        df_feat['m2'] = np.nan\n",
        "    else:\n",
        "        df_feat['beta_num'] = (df_feat['lr15']*df_feat['m']).rolling(3750).mean().values\n",
        "        df_feat['m2'] = (df_feat['m']*df_feat['m']).rolling(3750).mean().values\n",
        "        \n",
        "    if row:\n",
        "        # first .iloc as far back as we need, compute feature, then downsize until .iloc[-1]\n",
        "        df_feat = df_feat.iloc[-1]\n",
        "#         mean_price = df_feat[['Open', 'High', 'Low', 'Close']].mean()\n",
        "#         med_price = df_feat[['Open', 'High', 'Low', 'Close']].median()\n",
        "        df_feat['upper_shadow'] = df_feat['High'] / df_feat[['Close', 'Open']].max()\n",
        "        df_feat['lower_shadow'] = df_feat[['Close', 'Open']].min() / df_feat['Low']\n",
        "    else:\n",
        "#         mean_price = df_feat[['Open', 'High', 'Low', 'Close']].mean(axis=1)\n",
        "#         med_price = df_feat[['Open', 'High', 'Low', 'Close']].median(axis=1)\n",
        "        df_feat['upper_shadow'] = df_feat['High'] / df_feat[['Close', 'Open']].max(axis=1)\n",
        "        df_feat['lower_shadow'] = df_feat[['Close', 'Open']].min(axis=1) / df_feat['Low']\n",
        "        # df_feat = df_feat.drop('Asset_ID', axis=1)\n",
        "        \n",
        "    df_feat['beta'] = np.nan_to_num(df_feat['beta_num'] / df_feat['m2'], nan=0., posinf=0., neginf=0.)\n",
        "    df_feat['target_lag'] = df_feat['lr15'] - df_feat['beta']*df_feat['m']  # first 15 entries of target_lagg are NaN\n",
        "\n",
        "        ### Sense checks\n",
        "#         print((df_feat['Target'] - df_feat.groupby('Asset_ID')['target_lag'].shift(-16)).abs().mean())\n",
        "#         print(df_feat.loc[(df_feat.Target.isnull())&(df_feat.target_lag.notnull())].shape)\n",
        "#         print(df_feat.loc[(df_feat.Target.notnull())&(df_feat.target_lag.isnull())].shape)        \n",
        "        \n",
        "    df_feat['open2close'] = df_feat['Close'] / df_feat['Open']\n",
        "    df_feat['high2low'] = df_feat['High'] / df_feat['Low']\n",
        "           \n",
        "#     df_feat['high2mean'] = df_feat['High'] / mean_price\n",
        "#     df_feat['low2mean'] = df_feat['Low'] / mean_price\n",
        "#     df_feat['high2median'] = df_feat['High'] / med_price\n",
        "#     df_feat['low2median'] = df_feat['Low'] / med_price\n",
        "    df_feat['volume2count'] = df_feat['Volume'] / (df_feat['Count'] + 1)\n",
        "    df_feat['close2vwap'] = df_feat['Close'] / df_feat['VWAP']\n",
        "    \n",
        "    return df_feat.replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "def log_return(series, periods=1):\n",
        "    return np.log(series).diff(periods=periods)\n",
        "\n",
        "def timestamp_to_date(timestamp):\n",
        "    return(datetime.fromtimestamp(timestamp))\n",
        "\n",
        "def get_lead_features():\n",
        "    # get_features from ethereum\n",
        "    pass\n",
        "\n"
      ],
      "metadata": {
        "papermill": {
          "duration": 37.064504,
          "end_time": "2021-11-29T18:06:54.699951",
          "exception": false,
          "start_time": "2021-11-29T18:06:17.635447",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2022-02-17T00:38:08.085233Z",
          "iopub.execute_input": "2022-02-17T00:38:08.085625Z",
          "iopub.status.idle": "2022-02-17T00:38:08.113078Z",
          "shell.execute_reply.started": "2022-02-17T00:38:08.085585Z",
          "shell.execute_reply": "2022-02-17T00:38:08.112226Z"
        },
        "trusted": true,
        "id": "nRgybcglgdaX"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## FILTER DATAFRAME IF ONLY WORKING WITH ONE ASSET\n",
        "if SINGLE_ASSET:\n",
        "    train = train[train[\"Asset_ID\"]==asset_id]\n",
        "gc.collect()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T00:38:08.116698Z",
          "iopub.execute_input": "2022-02-17T00:38:08.117242Z",
          "iopub.status.idle": "2022-02-17T00:38:08.295694Z",
          "shell.execute_reply.started": "2022-02-17T00:38:08.117207Z",
          "shell.execute_reply": "2022-02-17T00:38:08.29475Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dg3jyBJJgdaa",
        "outputId": "83f07f95-a1cf-4761-f37e-151a0f81071e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train[['Count', 'Open', 'High', 'Low', 'Close', 'Volume', 'VWAP', 'Target']] = train[['Count', 'Open', 'High', 'Low', 'Close', 'Volume', 'VWAP', 'Target']].astype(np.float32)\n",
        "print(train.shape)\n",
        "train['Target'] = train['Target'].fillna(0)\n",
        "VWAP_max = np.max(train[np.isfinite(train.VWAP)].VWAP)\n",
        "VWAP_min = np.min(train[np.isfinite(train.VWAP)].VWAP)\n",
        "train['VWAP'] = np.nan_to_num(train.VWAP, posinf=VWAP_max, neginf=VWAP_min)\n",
        "df = train[['Asset_ID', 'Target']].copy()\n",
        "times = dict((t,i) for i,t in enumerate(df.index.unique()))\n",
        "df['id'] = df.index.map(times)\n",
        "df['id'] = df['id'].astype(str) + '_' + df['Asset_ID'].astype(str)\n",
        "ids = df.id.copy()\n",
        "del df\n",
        "\n",
        "train = train.sort_index()\n",
        "ind = train.index.unique()\n",
        "def reindex(df):\n",
        "    df = df.reindex(range(ind[0],ind[-1]+60,60),method='nearest')\n",
        "    df = df.fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
        "    return df\n",
        "train = train.groupby('Asset_ID').apply(reindex).reset_index(0, drop=True).sort_index()\n",
        "gc.collect()\n",
        "print(train.shape)\n",
        "\n",
        "# Matching records and marking generated rows as 'non-real'\n",
        "print(train.shape)\n",
        "train['group_num'] = train.index.map(times)\n",
        "train = train.dropna(subset=['group_num'])\n",
        "train['group_num'] = train['group_num'].astype('int')\n",
        "train['id'] = train['group_num'].astype(str) + '_' + train['Asset_ID'].astype(str)\n",
        "train['is_real'] = train.id.isin(ids) * 1\n",
        "train = train.drop('id', axis=1)\n",
        "print(train.shape)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T00:38:08.296841Z",
          "iopub.execute_input": "2022-02-17T00:38:08.297078Z",
          "iopub.status.idle": "2022-02-17T00:38:08.760404Z",
          "shell.execute_reply.started": "2022-02-17T00:38:08.297033Z",
          "shell.execute_reply": "2022-02-17T00:38:08.759366Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qeyD6yKfgdab",
        "outputId": "04b110f7-0cc3-43de-86c6-f9e5a8463a98"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(35716, 10)\n",
            "(35717, 10)\n",
            "(35717, 10)\n",
            "(35716, 12)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:28: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:29: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:30: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train['asset_order'] = train.Asset_ID.map(assets_order)\n",
        "train = train.sort_values(by=['group_num', 'asset_order'])\n",
        "# train = reduce_mem_usage(train)\n",
        "gc.collect()"
      ],
      "metadata": {
        "papermill": {
          "duration": 38.374972,
          "end_time": "2021-11-29T18:09:38.027192",
          "exception": false,
          "start_time": "2021-11-29T18:08:59.65222",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2022-02-17T00:38:08.762072Z",
          "iopub.execute_input": "2022-02-17T00:38:08.762339Z",
          "iopub.status.idle": "2022-02-17T00:38:08.938219Z",
          "shell.execute_reply.started": "2022-02-17T00:38:08.762303Z",
          "shell.execute_reply": "2022-02-17T00:38:08.93743Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3PG8wEFtgdad",
        "outputId": "0a38270b-e15c-4aa5-d2fc-29b77d9b205d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if train.index.name == \"timestamp\":\n",
        "    train = train.reset_index()\n",
        "train['lr15'] = train.groupby('Asset_ID')['Close'].apply(lambda x: log_return(x, 15))\n",
        "train['lr16'] = train.groupby('Asset_ID')['Close'].apply(lambda x: log_return(x, 16))\n",
        "train = train.merge(df_asset_details[['Asset_ID','Weight']], how='left', on = 'Asset_ID')\n",
        "train['m'] = train['lr15']*train['Weight']\n",
        "train['m'] = train.groupby('timestamp')['m'].transform('sum') / np.sum(df_asset_details['Weight'])\n",
        "\n",
        "train.head(20)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T00:38:08.939735Z",
          "iopub.execute_input": "2022-02-17T00:38:08.940042Z",
          "iopub.status.idle": "2022-02-17T00:38:09.00107Z",
          "shell.execute_reply.started": "2022-02-17T00:38:08.939984Z",
          "shell.execute_reply": "2022-02-17T00:38:09.000329Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WbOjRhcpgdae",
        "outputId": "89fbbc55-e49f-41fd-ae5a-9925f5ff293d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-deb7a5b0-6fe6-49e3-9d85-d6963ec08c1b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>timestamp</th>\n",
              "      <th>Asset_ID</th>\n",
              "      <th>Count</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Volume</th>\n",
              "      <th>VWAP</th>\n",
              "      <th>Target</th>\n",
              "      <th>date</th>\n",
              "      <th>group_num</th>\n",
              "      <th>is_real</th>\n",
              "      <th>asset_order</th>\n",
              "      <th>lr15</th>\n",
              "      <th>lr16</th>\n",
              "      <th>Weight</th>\n",
              "      <th>m</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1621399380</td>\n",
              "      <td>3</td>\n",
              "      <td>13863.0</td>\n",
              "      <td>1.642700</td>\n",
              "      <td>1.649500</td>\n",
              "      <td>1.611000</td>\n",
              "      <td>1.623511</td>\n",
              "      <td>10208457.00</td>\n",
              "      <td>1.627059</td>\n",
              "      <td>0.006264</td>\n",
              "      <td>2021-05-19 04:43:00</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.406719</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1621399440</td>\n",
              "      <td>3</td>\n",
              "      <td>9173.0</td>\n",
              "      <td>1.623793</td>\n",
              "      <td>1.654195</td>\n",
              "      <td>1.615200</td>\n",
              "      <td>1.645859</td>\n",
              "      <td>6408643.00</td>\n",
              "      <td>1.640546</td>\n",
              "      <td>0.010261</td>\n",
              "      <td>2021-05-19 04:44:00</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.406719</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1621399500</td>\n",
              "      <td>3</td>\n",
              "      <td>9524.0</td>\n",
              "      <td>1.645627</td>\n",
              "      <td>1.673242</td>\n",
              "      <td>1.617400</td>\n",
              "      <td>1.664102</td>\n",
              "      <td>6283014.50</td>\n",
              "      <td>1.647340</td>\n",
              "      <td>0.016606</td>\n",
              "      <td>2021-05-19 04:45:00</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.406719</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1621399560</td>\n",
              "      <td>3</td>\n",
              "      <td>7761.0</td>\n",
              "      <td>1.663457</td>\n",
              "      <td>1.669406</td>\n",
              "      <td>1.631900</td>\n",
              "      <td>1.640231</td>\n",
              "      <td>4516063.00</td>\n",
              "      <td>1.650989</td>\n",
              "      <td>0.013718</td>\n",
              "      <td>2021-05-19 04:46:00</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.406719</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1621399620</td>\n",
              "      <td>3</td>\n",
              "      <td>6559.0</td>\n",
              "      <td>1.640461</td>\n",
              "      <td>1.664600</td>\n",
              "      <td>1.635679</td>\n",
              "      <td>1.658525</td>\n",
              "      <td>4456316.50</td>\n",
              "      <td>1.651940</td>\n",
              "      <td>0.021053</td>\n",
              "      <td>2021-05-19 04:47:00</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.406719</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1621399680</td>\n",
              "      <td>3</td>\n",
              "      <td>5590.0</td>\n",
              "      <td>1.658125</td>\n",
              "      <td>1.671800</td>\n",
              "      <td>1.643700</td>\n",
              "      <td>1.657311</td>\n",
              "      <td>3973633.75</td>\n",
              "      <td>1.655983</td>\n",
              "      <td>0.023950</td>\n",
              "      <td>2021-05-19 04:48:00</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.406719</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1621399740</td>\n",
              "      <td>3</td>\n",
              "      <td>8552.0</td>\n",
              "      <td>1.657763</td>\n",
              "      <td>1.660800</td>\n",
              "      <td>1.620500</td>\n",
              "      <td>1.639733</td>\n",
              "      <td>4245567.50</td>\n",
              "      <td>1.643210</td>\n",
              "      <td>0.021678</td>\n",
              "      <td>2021-05-19 04:49:00</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.406719</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1621399800</td>\n",
              "      <td>3</td>\n",
              "      <td>7144.0</td>\n",
              "      <td>1.640014</td>\n",
              "      <td>1.656900</td>\n",
              "      <td>1.628995</td>\n",
              "      <td>1.648322</td>\n",
              "      <td>3585431.25</td>\n",
              "      <td>1.644723</td>\n",
              "      <td>0.026200</td>\n",
              "      <td>2021-05-19 04:50:00</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.406719</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1621399860</td>\n",
              "      <td>3</td>\n",
              "      <td>8980.0</td>\n",
              "      <td>1.647378</td>\n",
              "      <td>1.649000</td>\n",
              "      <td>1.615000</td>\n",
              "      <td>1.628970</td>\n",
              "      <td>5622760.00</td>\n",
              "      <td>1.628477</td>\n",
              "      <td>0.027583</td>\n",
              "      <td>2021-05-19 04:51:00</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.406719</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1621399920</td>\n",
              "      <td>3</td>\n",
              "      <td>10643.0</td>\n",
              "      <td>1.630148</td>\n",
              "      <td>1.635000</td>\n",
              "      <td>1.600000</td>\n",
              "      <td>1.610441</td>\n",
              "      <td>7331379.50</td>\n",
              "      <td>1.614531</td>\n",
              "      <td>0.028171</td>\n",
              "      <td>2021-05-19 04:52:00</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.406719</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>1621399980</td>\n",
              "      <td>3</td>\n",
              "      <td>9123.0</td>\n",
              "      <td>1.610199</td>\n",
              "      <td>1.620600</td>\n",
              "      <td>1.590000</td>\n",
              "      <td>1.599522</td>\n",
              "      <td>6849127.00</td>\n",
              "      <td>1.604222</td>\n",
              "      <td>0.022879</td>\n",
              "      <td>2021-05-19 04:53:00</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.406719</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>1621400040</td>\n",
              "      <td>3</td>\n",
              "      <td>8817.0</td>\n",
              "      <td>1.598771</td>\n",
              "      <td>1.608200</td>\n",
              "      <td>1.585420</td>\n",
              "      <td>1.594127</td>\n",
              "      <td>5408449.00</td>\n",
              "      <td>1.597099</td>\n",
              "      <td>0.030317</td>\n",
              "      <td>2021-05-19 04:54:00</td>\n",
              "      <td>11</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.406719</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>1621400100</td>\n",
              "      <td>3</td>\n",
              "      <td>9643.0</td>\n",
              "      <td>1.593979</td>\n",
              "      <td>1.609100</td>\n",
              "      <td>1.580200</td>\n",
              "      <td>1.602591</td>\n",
              "      <td>6437179.50</td>\n",
              "      <td>1.597654</td>\n",
              "      <td>0.033699</td>\n",
              "      <td>2021-05-19 04:55:00</td>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.406719</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>1621400160</td>\n",
              "      <td>3</td>\n",
              "      <td>9587.0</td>\n",
              "      <td>1.602049</td>\n",
              "      <td>1.620000</td>\n",
              "      <td>1.583223</td>\n",
              "      <td>1.604161</td>\n",
              "      <td>6343381.00</td>\n",
              "      <td>1.606019</td>\n",
              "      <td>0.038886</td>\n",
              "      <td>2021-05-19 04:56:00</td>\n",
              "      <td>13</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.406719</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>1621400220</td>\n",
              "      <td>3</td>\n",
              "      <td>8442.0</td>\n",
              "      <td>1.604467</td>\n",
              "      <td>1.635300</td>\n",
              "      <td>1.590800</td>\n",
              "      <td>1.625749</td>\n",
              "      <td>4911103.00</td>\n",
              "      <td>1.616691</td>\n",
              "      <td>0.028503</td>\n",
              "      <td>2021-05-19 04:57:00</td>\n",
              "      <td>14</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.406719</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>1621400280</td>\n",
              "      <td>3</td>\n",
              "      <td>9203.0</td>\n",
              "      <td>1.626747</td>\n",
              "      <td>1.654680</td>\n",
              "      <td>1.614000</td>\n",
              "      <td>1.646023</td>\n",
              "      <td>5982300.00</td>\n",
              "      <td>1.636194</td>\n",
              "      <td>0.025401</td>\n",
              "      <td>2021-05-19 04:58:00</td>\n",
              "      <td>15</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.013771</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.406719</td>\n",
              "      <td>0.001484</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>1621400340</td>\n",
              "      <td>3</td>\n",
              "      <td>7288.0</td>\n",
              "      <td>1.644431</td>\n",
              "      <td>1.665100</td>\n",
              "      <td>1.635800</td>\n",
              "      <td>1.660292</td>\n",
              "      <td>5016601.50</td>\n",
              "      <td>1.654719</td>\n",
              "      <td>0.014743</td>\n",
              "      <td>2021-05-19 04:59:00</td>\n",
              "      <td>16</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.008731</td>\n",
              "      <td>0.022402</td>\n",
              "      <td>4.406719</td>\n",
              "      <td>0.000941</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>1621400400</td>\n",
              "      <td>3</td>\n",
              "      <td>7925.0</td>\n",
              "      <td>1.660577</td>\n",
              "      <td>1.690000</td>\n",
              "      <td>1.658100</td>\n",
              "      <td>1.683223</td>\n",
              "      <td>5371792.50</td>\n",
              "      <td>1.670483</td>\n",
              "      <td>0.008765</td>\n",
              "      <td>2021-05-19 05:00:00</td>\n",
              "      <td>17</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.011425</td>\n",
              "      <td>0.022448</td>\n",
              "      <td>4.406719</td>\n",
              "      <td>0.001232</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>1621400460</td>\n",
              "      <td>3</td>\n",
              "      <td>10511.0</td>\n",
              "      <td>1.684452</td>\n",
              "      <td>1.715800</td>\n",
              "      <td>1.682200</td>\n",
              "      <td>1.705574</td>\n",
              "      <td>7358236.50</td>\n",
              "      <td>1.701093</td>\n",
              "      <td>0.010435</td>\n",
              "      <td>2021-05-19 05:01:00</td>\n",
              "      <td>18</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.039064</td>\n",
              "      <td>0.024616</td>\n",
              "      <td>4.406719</td>\n",
              "      <td>0.004211</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>1621400520</td>\n",
              "      <td>3</td>\n",
              "      <td>10550.0</td>\n",
              "      <td>1.704350</td>\n",
              "      <td>1.707674</td>\n",
              "      <td>1.676489</td>\n",
              "      <td>1.680776</td>\n",
              "      <td>6785659.50</td>\n",
              "      <td>1.687330</td>\n",
              "      <td>0.003379</td>\n",
              "      <td>2021-05-19 05:02:00</td>\n",
              "      <td>19</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.013327</td>\n",
              "      <td>0.024418</td>\n",
              "      <td>4.406719</td>\n",
              "      <td>0.001437</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-deb7a5b0-6fe6-49e3-9d85-d6963ec08c1b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-deb7a5b0-6fe6-49e3-9d85-d6963ec08c1b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-deb7a5b0-6fe6-49e3-9d85-d6963ec08c1b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "     timestamp  Asset_ID    Count      Open      High       Low     Close  \\\n",
              "0   1621399380         3  13863.0  1.642700  1.649500  1.611000  1.623511   \n",
              "1   1621399440         3   9173.0  1.623793  1.654195  1.615200  1.645859   \n",
              "2   1621399500         3   9524.0  1.645627  1.673242  1.617400  1.664102   \n",
              "3   1621399560         3   7761.0  1.663457  1.669406  1.631900  1.640231   \n",
              "4   1621399620         3   6559.0  1.640461  1.664600  1.635679  1.658525   \n",
              "5   1621399680         3   5590.0  1.658125  1.671800  1.643700  1.657311   \n",
              "6   1621399740         3   8552.0  1.657763  1.660800  1.620500  1.639733   \n",
              "7   1621399800         3   7144.0  1.640014  1.656900  1.628995  1.648322   \n",
              "8   1621399860         3   8980.0  1.647378  1.649000  1.615000  1.628970   \n",
              "9   1621399920         3  10643.0  1.630148  1.635000  1.600000  1.610441   \n",
              "10  1621399980         3   9123.0  1.610199  1.620600  1.590000  1.599522   \n",
              "11  1621400040         3   8817.0  1.598771  1.608200  1.585420  1.594127   \n",
              "12  1621400100         3   9643.0  1.593979  1.609100  1.580200  1.602591   \n",
              "13  1621400160         3   9587.0  1.602049  1.620000  1.583223  1.604161   \n",
              "14  1621400220         3   8442.0  1.604467  1.635300  1.590800  1.625749   \n",
              "15  1621400280         3   9203.0  1.626747  1.654680  1.614000  1.646023   \n",
              "16  1621400340         3   7288.0  1.644431  1.665100  1.635800  1.660292   \n",
              "17  1621400400         3   7925.0  1.660577  1.690000  1.658100  1.683223   \n",
              "18  1621400460         3  10511.0  1.684452  1.715800  1.682200  1.705574   \n",
              "19  1621400520         3  10550.0  1.704350  1.707674  1.676489  1.680776   \n",
              "\n",
              "         Volume      VWAP    Target                date  group_num  is_real  \\\n",
              "0   10208457.00  1.627059  0.006264 2021-05-19 04:43:00          0        1   \n",
              "1    6408643.00  1.640546  0.010261 2021-05-19 04:44:00          1        1   \n",
              "2    6283014.50  1.647340  0.016606 2021-05-19 04:45:00          2        1   \n",
              "3    4516063.00  1.650989  0.013718 2021-05-19 04:46:00          3        1   \n",
              "4    4456316.50  1.651940  0.021053 2021-05-19 04:47:00          4        1   \n",
              "5    3973633.75  1.655983  0.023950 2021-05-19 04:48:00          5        1   \n",
              "6    4245567.50  1.643210  0.021678 2021-05-19 04:49:00          6        1   \n",
              "7    3585431.25  1.644723  0.026200 2021-05-19 04:50:00          7        1   \n",
              "8    5622760.00  1.628477  0.027583 2021-05-19 04:51:00          8        1   \n",
              "9    7331379.50  1.614531  0.028171 2021-05-19 04:52:00          9        1   \n",
              "10   6849127.00  1.604222  0.022879 2021-05-19 04:53:00         10        1   \n",
              "11   5408449.00  1.597099  0.030317 2021-05-19 04:54:00         11        1   \n",
              "12   6437179.50  1.597654  0.033699 2021-05-19 04:55:00         12        1   \n",
              "13   6343381.00  1.606019  0.038886 2021-05-19 04:56:00         13        1   \n",
              "14   4911103.00  1.616691  0.028503 2021-05-19 04:57:00         14        1   \n",
              "15   5982300.00  1.636194  0.025401 2021-05-19 04:58:00         15        1   \n",
              "16   5016601.50  1.654719  0.014743 2021-05-19 04:59:00         16        1   \n",
              "17   5371792.50  1.670483  0.008765 2021-05-19 05:00:00         17        1   \n",
              "18   7358236.50  1.701093  0.010435 2021-05-19 05:01:00         18        1   \n",
              "19   6785659.50  1.687330  0.003379 2021-05-19 05:02:00         19        1   \n",
              "\n",
              "    asset_order      lr15      lr16    Weight         m  \n",
              "0             0       NaN       NaN  4.406719  0.000000  \n",
              "1             0       NaN       NaN  4.406719  0.000000  \n",
              "2             0       NaN       NaN  4.406719  0.000000  \n",
              "3             0       NaN       NaN  4.406719  0.000000  \n",
              "4             0       NaN       NaN  4.406719  0.000000  \n",
              "5             0       NaN       NaN  4.406719  0.000000  \n",
              "6             0       NaN       NaN  4.406719  0.000000  \n",
              "7             0       NaN       NaN  4.406719  0.000000  \n",
              "8             0       NaN       NaN  4.406719  0.000000  \n",
              "9             0       NaN       NaN  4.406719  0.000000  \n",
              "10            0       NaN       NaN  4.406719  0.000000  \n",
              "11            0       NaN       NaN  4.406719  0.000000  \n",
              "12            0       NaN       NaN  4.406719  0.000000  \n",
              "13            0       NaN       NaN  4.406719  0.000000  \n",
              "14            0       NaN       NaN  4.406719  0.000000  \n",
              "15            0  0.013771       NaN  4.406719  0.001484  \n",
              "16            0  0.008731  0.022402  4.406719  0.000941  \n",
              "17            0  0.011425  0.022448  4.406719  0.001232  \n",
              "18            0  0.039064  0.024616  4.406719  0.004211  \n",
              "19            0  0.013327  0.024418  4.406719  0.001437  "
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_split_indices(indices):\n",
        "    train_end_idx = int(len(indices) * (1 - (PCT_VALIDATION+PCT_TEST)/100))\n",
        "    val_end_idx = int(len(indices) * (1 - (PCT_TEST)/100))\n",
        "    return indices[train_end_idx], indices[val_end_idx]\n",
        "\n",
        "train_end_idx = {}\n",
        "val_end_idx = {}\n",
        "if not SINGLE_ASSET:\n",
        "    for i in range(N_ASSETS):\n",
        "        X_asset_indices = train[train[\"Asset_ID\"]==i].index\n",
        "        train_end_idx[i], val_end_idx[i] = get_split_indices(X_asset_indices)\n",
        "else:\n",
        "    X_asset_indices = train[train[\"Asset_ID\"]==asset_id].index\n",
        "    train_end_idx[asset_id], val_end_idx[asset_id] = get_split_indices(X_asset_indices)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T00:38:09.002308Z",
          "iopub.execute_input": "2022-02-17T00:38:09.00274Z",
          "iopub.status.idle": "2022-02-17T00:38:09.013502Z",
          "shell.execute_reply.started": "2022-02-17T00:38:09.002701Z",
          "shell.execute_reply": "2022-02-17T00:38:09.012775Z"
        },
        "trusted": true,
        "id": "QBrweJbVgdaf"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create train data of shape: Batch Size X Window Size X Num Assets --> bBATCH_SIZEon mean over all features\n",
        "# create train lables of shape: batch size X Num Assets  -->  based on target of t(win_len)\n",
        "# given data and index, train batches + labels are created\n",
        "\n",
        "\n",
        "class sample_generator(keras.utils.Sequence):\n",
        "    def __init__(self, x_set, y_set, batch_size, length, prediction_length):\n",
        "        self.x, self.y = x_set, y_set\n",
        "        self.batch_size = batch_size\n",
        "        self.length = length\n",
        "        self.prediction_length = prediction_length\n",
        "        self.size = len(x_set)\n",
        "    def __len__(self): return int((len(self.x)-self.length) / float(self.batch_size))\n",
        "    def __getitem__(self, idx):\n",
        "        batch_x=[]\n",
        "        batch_y=[]\n",
        "        for i in range(self.batch_size):\n",
        "            start_ind = self.batch_size*idx + i\n",
        "            end_ind = start_ind + self.length \n",
        "            if (end_ind+self.prediction_length-1) <= self.size:\n",
        "                batch_x.append(self.x[start_ind : end_ind])\n",
        "                batch_y.append(self.y[end_ind -1: end_ind+self.prediction_length-1]) \n",
        "        x,y = np.array(batch_x), np.array(batch_y)\n",
        "        assert x.shape == (BATCH_SIZE, WINDOW_SIZE,len(features)), f\"Shape Missmatch of train data generator X at idx {str(idx)} and i {str(i)}\"\n",
        "        assert x.shape == (BATCH_SIZE, WINDOW_SIZE,len(features)), f\"Shape Missmatch of train data generator Y at idx {str(idx)} and i {str(i)}\"\n",
        "        return x,y\n",
        "\n",
        "class multivariate_sample_generator(keras.utils.Sequence):\n",
        "    # generator which supports multivariate ex-features input into nbeats\n",
        "    # -> Single asset per sequence\n",
        "    # --> x_sequence.shape = batch_size x Window_size x 1\n",
        "    # --> y_sequence.shape = batch_size x 1\n",
        "    # --> e_sequence.shape = batch_size x Window_size x num_ex_var    \n",
        "    def __init__(self, x_set, y_set, e_set, batch_size, length, prediction_length):\n",
        "        self.x, self.y, self.e = x_set, y_set, e_set\n",
        "        self.batch_size = batch_size\n",
        "        self.length = length\n",
        "        self.prediction_length = prediction_length\n",
        "        self.size = len(x_set)\n",
        "        self.num_assets = self.x.shape[1]\n",
        "    def __len__(self): return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
        "    def __getitem__(self, idx):\n",
        "        # idx: how many times the generator is already called\n",
        "        batch_x=[]\n",
        "        batch_y=[]\n",
        "        batch_e=[]\n",
        "        num_sample=0\n",
        "        while num_sample < self.batch_size:\n",
        "            start_ind = self.batch_size//self.num_assets*idx + num_sample//self.num_assets\n",
        "            end_ind = start_ind + self.length \n",
        "            if (end_ind+self.prediction_length-1) <= self.size:\n",
        "                for a in range(self.num_assets):\n",
        "                    batch_x.append(self.x[start_ind : end_ind, a])\n",
        "                    batch_e.append(self.e[start_ind : end_ind, a, :])\n",
        "                    batch_y.append(self.y[end_ind -1: end_ind+self.prediction_length-1, a]) \n",
        "                    num_sample += 1\n",
        "                    if num_sample >= self.batch_size: break;  ## TODO: FIND BETTER SOLUTION (here we throw aray samples if we break out of loop)\n",
        "        return [np.array(batch_x).reshape(-1,self.length,1), np.array(batch_e)], np.array(batch_y)\n",
        "    \n",
        "\n",
        "def assert_data_validity_generator(gen, batch_size, test_shift_size=1, num_tests=200):\n",
        "    for i in range(num_tests):\n",
        "        num_batch = np.random.randint(0, len(gen), dtype=int)\n",
        "        num_sample = np.random.randint(0, batch_size-1, dtype=int)\n",
        "        assert gen[num_batch][0][num_sample+test_shift_size,-1,0] == gen[num_batch][1][num_sample,0,0], \"Data Missmatch between Series and Target in generator data\"\n",
        "    print(\"All tests passed.\")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T00:38:09.014888Z",
          "iopub.execute_input": "2022-02-17T00:38:09.015352Z",
          "iopub.status.idle": "2022-02-17T00:38:09.035233Z",
          "shell.execute_reply.started": "2022-02-17T00:38:09.015315Z",
          "shell.execute_reply": "2022-02-17T00:38:09.034448Z"
        },
        "trusted": true,
        "id": "n8iZNJFggdag"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_asset = train.copy()\n",
        "X_asset = get_features_hist(X_asset)\n",
        "X_asset[\"Target_shifted\"] = X_asset[\"Target\"].shift(num_shift)\n",
        "X_asset[\"Target\"].plot()\n",
        "\n",
        "train_end_idx[asset_id], val_end_idx[asset_id]\n",
        "features = [\"Target_shifted\"]\n",
        "# features = [\"Target_shifted\", \"m\", \"upper_shadow\", \"lower_shadow\", \"open2close\", \"high2low\", \"volume2count\", \"close2vwap\"] \n",
        "target = [\"Target\"]\n",
        "X_asset.loc[X_asset.is_real==0, features] = 0 \n",
        "X_asset[features] = X_asset[features].fillna(0)\n",
        "scaler = MinMaxScaler()\n",
        "X_asset[features] = scaler.fit_transform(X_asset[features])  # TODO: NEEDS TO BE SEPARATED FOR TRAIN TEST\n",
        " \n",
        "train_generator = sample_generator(X_asset.loc[:train_end_idx[asset_id], features], \n",
        "                                   X_asset.loc[:train_end_idx[asset_id], target], \n",
        "                                   length = WINDOW_SIZE, batch_size = BATCH_SIZE, prediction_length=prediction_length)\n",
        "val_generator = sample_generator(X_asset.loc[train_end_idx[asset_id]:val_end_idx[asset_id], features], \n",
        "                                 X_asset.loc[train_end_idx[asset_id]:val_end_idx[asset_id], target], \n",
        "                                 length = WINDOW_SIZE, batch_size = BATCH_SIZE, prediction_length=prediction_length)\n",
        "test_generator = sample_generator(X_asset.loc[val_end_idx[asset_id]:, features], \n",
        "                                  X_asset.loc[val_end_idx[asset_id]:, target], \n",
        "                                  length = WINDOW_SIZE, batch_size = BATCH_SIZE, prediction_length=prediction_length)\n",
        "\n",
        "print(\"BATCH_SIZE: \", BATCH_SIZE)\n",
        "print(f'Train batch shape: {train_generator[0][0].shape}')\n",
        "print(f'Target batch shape: {train_generator[0][1].shape}')\n",
        "\n",
        "print(\"Num batches of train, validation and test generator: \", len(train_generator),\", \", len(val_generator),\", \", len(test_generator))\n",
        "print(\"Num Training Samples: \", len(train_generator)*BATCH_SIZE)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T00:50:00.197725Z",
          "iopub.execute_input": "2022-02-17T00:50:00.198607Z",
          "iopub.status.idle": "2022-02-17T00:50:00.483976Z",
          "shell.execute_reply.started": "2022-02-17T00:50:00.198534Z",
          "shell.execute_reply": "2022-02-17T00:50:00.482539Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "id": "tQRkhrE-gdah",
        "outputId": "dac46250-3667-4f11-b95b-711dea8b090d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BATCH_SIZE:  128\n",
            "Train batch shape: (128, 16, 1)\n",
            "Target batch shape: (128, 1, 1)\n",
            "Num batches of train, validation and test generator:  223 ,  27 ,  27\n",
            "Num Training Samples:  28544\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <span class=\"title-section w3-xxlarge\" id=\"training\">Training 🏋️</span>\n",
        "<hr>\n",
        "\n",
        "Our model will be trained for the number of FOLDS and EPOCHS you chose in the configuration above. Each fold the model with lowest validation loss will be saved and used to predict OOF and test. Adjust the variable `VERBOSE`. The variable `VERBOSE=1 or 2` will display the training and validation loss for each epoch as text. "
      ],
      "metadata": {
        "papermill": {
          "duration": 0.038491,
          "end_time": "2021-11-29T18:09:46.955605",
          "exception": false,
          "start_time": "2021-11-29T18:09:46.917114",
          "status": "completed"
        },
        "tags": [],
        "id": "7SIUAeqjgdai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Time2Vec(keras.layers.Layer):\n",
        "    def __init__(self, kernel_size=1):\n",
        "        super(Time2Vec, self).__init__(trainable=True, name='Time2VecLayer')\n",
        "        self.k = kernel_size\n",
        "    \n",
        "    def build(self, input_shape):  # build automatically executed before layer is called for the first time --  mostly used to instantiate weights\n",
        "        # trend\n",
        "        self.wb = self.add_weight(name='wb',shape=(input_shape[1],),initializer='uniform',trainable=True)\n",
        "        self.bb = self.add_weight(name='bb',shape=(input_shape[1],),initializer='uniform',trainable=True)\n",
        "        # periodic\n",
        "        self.wa = self.add_weight(name='wa',shape=(1, input_shape[1], self.k),initializer='uniform',trainable=True)\n",
        "        self.ba = self.add_weight(name='ba',shape=(1, input_shape[1], self.k),initializer='uniform',trainable=True)\n",
        "        super(Time2Vec, self).build(input_shape)\n",
        "    \n",
        "    def call(self, inputs, **kwargs):  # where the layer logic lives\n",
        "        bias = self.wb * inputs + self.bb\n",
        "        dp = K.dot(inputs, self.wa) + self.ba\n",
        "        wgts = K.sin(dp) # or K.cos(.)\n",
        "\n",
        "        ret = K.concatenate([K.expand_dims(bias, -1), wgts], -1)\n",
        "        ret = K.reshape(ret, (-1, inputs.shape[1]*(self.k+1)))\n",
        "        return ret\n",
        "    \n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], input_shape[1]*(self.k + 1))\n",
        "    \n",
        "\n",
        "# https://towardsdatascience.com/time2vec-for-time-series-features-encoding-a03a4f3f937e\n",
        "class T2V(keras.layers.Layer):\n",
        "    \n",
        "    def __init__(self, output_dim=None, **kwargs):\n",
        "        self.output_dim = output_dim\n",
        "        super(T2V, self).__init__(**kwargs)\n",
        "        \n",
        "    def build(self, input_shape):        \n",
        "        self.W = self.add_weight(name='W',\n",
        "                      shape=(input_shape[-1], self.output_dim),\n",
        "                      initializer='uniform',\n",
        "                      trainable=True)        \n",
        "        self.P = self.add_weight(name='P',\n",
        "                      shape=(input_shape[1], self.output_dim),\n",
        "                      initializer='uniform',\n",
        "                      trainable=True)        \n",
        "        self.w = self.add_weight(name='w',\n",
        "                      shape=(input_shape[1], 1),\n",
        "                      initializer='uniform',\n",
        "                      trainable=True)        \n",
        "        self.p = self.add_weight(name='p',\n",
        "                      shape=(input_shape[1], 1),\n",
        "                      initializer='uniform',\n",
        "                      trainable=True)        \n",
        "        super(T2V, self).build(input_shape)\n",
        "        \n",
        "    def call(self, x):\n",
        "        \n",
        "        original = self.w * x + self.p\n",
        "        sin_trans = K.sin(K.dot(x, self.W) + self.P)\n",
        "        \n",
        "        return K.concatenate([sin_trans, original], -1)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T00:50:00.525214Z",
          "iopub.execute_input": "2022-02-17T00:50:00.52544Z",
          "iopub.status.idle": "2022-02-17T00:50:00.540886Z",
          "shell.execute_reply.started": "2022-02-17T00:50:00.525406Z",
          "shell.execute_reply": "2022-02-17T00:50:00.54001Z"
        },
        "trusted": true,
        "id": "xbUPXzfdgdaj"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://towardsdatascience.com/the-time-series-transformer-2a521a0efad3\n",
        "\n",
        "class AttentionBlock(keras.Model):\n",
        "    def __init__(self, name='AttentionBlock', num_heads=2, head_size=128, ff_dim=None, dropout=0, **kwargs):\n",
        "        super().__init__(name=name, **kwargs)\n",
        "\n",
        "        if ff_dim is None:\n",
        "            ff_dim = head_size\n",
        "\n",
        "        self.attention = MultiHeadAttention(num_heads=num_heads, head_size=head_size, dropout=dropout)\n",
        "        self.attention_dropout = keras.layers.Dropout(dropout)\n",
        "        self.attention_norm = keras.layers.BatchNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.ff_conv1 = keras.layers.Conv1D(filters=ff_dim, kernel_size=1, activation='relu')\n",
        "        # self.ff_conv2 at build()\n",
        "        self.ff_dropout = keras.layers.Dropout(dropout)\n",
        "        self.ff_norm = keras.layers.BatchNormalization(epsilon=1e-6)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.ff_conv2 = keras.layers.Conv1D(filters=input_shape[-1], kernel_size=1) \n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.attention([inputs, inputs])\n",
        "        x = self.attention_dropout(x)\n",
        "        x = self.attention_norm(inputs + x)\n",
        "\n",
        "        x = self.ff_conv1(x)\n",
        "        x = self.ff_conv2(x)\n",
        "        x = self.ff_dropout(x)\n",
        "\n",
        "        x = self.ff_norm(inputs + x)\n",
        "        return x\n",
        "    \n",
        "\n",
        "class ModelTrunk(keras.Model):\n",
        "    def __init__(self, name='ModelTrunk', time2vec_dim=1, num_heads=2, head_size=128, ff_dim=None, num_layers=1, dropout=0, **kwargs):\n",
        "        super().__init__(name=name, **kwargs)\n",
        "        self.time2vec = Time2Vec(kernel_size=time2vec_dim)\n",
        "        if ff_dim is None:\n",
        "            ff_dim = head_size\n",
        "        self.dropout = dropout\n",
        "        self.attention_layers = [AttentionBlock(num_heads=num_heads, head_size=head_size, ff_dim=ff_dim, dropout=dropout) for _ in range(num_layers)]\n",
        "\n",
        "        \n",
        "    def call(self, inputs):\n",
        "        time_embedding = keras.layers.TimeDistributed(self.time2vec)(inputs)\n",
        "        x = K.concatenate([inputs, time_embedding], -1)\n",
        "        for attention_layer in self.attention_layers:\n",
        "            x = attention_layer(x)\n",
        "\n",
        "        return K.reshape(x, (-1, x.shape[1] * x.shape[2])) # flat vector of features out\n",
        "    \n",
        "\n",
        "def build_model_new(\n",
        "    input_shape,  # shape of time series sample\n",
        "    head_size, # size of multi head attention\n",
        "    num_heads,  # number of multi-head attention \n",
        "    ff_dim,  # \n",
        "    num_transformer_blocks, # \n",
        "    mlp_units, # list of N dense layers with i neurons\n",
        "    dropout=0,  # transformer block dropout rate\n",
        "    mlp_dropout=0,  # \n",
        "    time2vec_dim=3\n",
        "):\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    \n",
        "    x = ModelTrunk(num_heads=num_heads, head_size=head_size, ff_dim=ff_dim, num_layers=num_transformer_blocks, time2vec_dim=time2vec_dim, dropout=dropout)(inputs)\n",
        "\n",
        "    # x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
        "    for dim in mlp_units:\n",
        "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
        "        x = layers.Dropout(mlp_dropout)(x)\n",
        "    outputs = layers.Dense(1, kernel_initializer=\"normal\")(x)\n",
        "    return keras.Model(inputs, outputs)\n",
        "\n",
        "\n",
        "# https://github.com/tatp22/multidim-positional-encoding/blob/master/positional_encodings/tf_positional_encodings.py\n",
        "class TFPositionalEncoding1D(tf.keras.layers.Layer):\n",
        "    def __init__(self, channels: int, dtype=tf.float32):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            channels int: The last dimension of the tensor you want to apply pos emb to.\n",
        "        Keyword Args:\n",
        "            dtype: output type of the encodings. Default is \"tf.float32\".\n",
        "        \"\"\"\n",
        "        super(TFPositionalEncoding1D, self).__init__()\n",
        "\n",
        "        self.channels = int(np.ceil(channels / 2) * 2)\n",
        "        self.inv_freq = np.float32(\n",
        "            1\n",
        "            / np.power(\n",
        "                10000, np.arange(0, self.channels, 2) / np.float32(self.channels)\n",
        "            )\n",
        "        )\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, inputs):\n",
        "        \"\"\"\n",
        "        :param tensor: A 3d tensor of size (batch_size, x, ch)\n",
        "        :return: Positional Encoding Matrix of size (batch_size, x, ch)\n",
        "        \"\"\"\n",
        "        if len(inputs.shape) != 3:\n",
        "            raise RuntimeError(\"The input tensor has to be 3d!\")\n",
        "        _, x, org_channels = inputs.shape\n",
        "\n",
        "        dtype = self.inv_freq.dtype\n",
        "        pos_x = tf.range(x, dtype=dtype)\n",
        "        sin_inp_x = tf.einsum(\"i,j->ij\", pos_x, self.inv_freq)\n",
        "        emb = tf.expand_dims(tf.concat((tf.sin(sin_inp_x), tf.cos(sin_inp_x)), -1), 0)\n",
        "        emb = emb[0]  # A bit of a hack\n",
        "        return tf.repeat(emb[None, :, :org_channels], tf.shape(inputs)[0], axis=0)        \n",
        "        "
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T00:50:00.705448Z",
          "iopub.execute_input": "2022-02-17T00:50:00.705827Z",
          "iopub.status.idle": "2022-02-17T00:50:00.731486Z",
          "shell.execute_reply.started": "2022-02-17T00:50:00.705792Z",
          "shell.execute_reply": "2022-02-17T00:50:00.730033Z"
        },
        "trusted": true,
        "id": "5MMg1hmCgdal"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://keras.io/examples/timeseries/timeseries_transformer_classification/\n",
        "\n",
        "\n",
        "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
        "    # Normalization and Attention\n",
        "    x = layers.BatchNormalization(epsilon=1e-6)(inputs)\n",
        "    x = layers.MultiHeadAttention(\n",
        "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
        "    )(x, x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    res = x + inputs\n",
        "\n",
        "    # Feed Forward Part\n",
        "    x = layers.BatchNormalization(epsilon=1e-6)(res)\n",
        "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
        "    return x + res\n",
        "\n",
        "# self.time2vec = Time2Vec(kernel_size=time2vec_dim)\n",
        "\n",
        "def build_model(\n",
        "    input_shape,  # shape of time series sample\n",
        "    head_size, # size of multi head attention\n",
        "    num_heads,  # number of multi-head attention \n",
        "    ff_dim,  # \n",
        "    num_transformer_blocks, # \n",
        "    mlp_units, # list of N dense layers with i neurons\n",
        "    dropout=0,  # transformer block dropout rate\n",
        "    mlp_dropout=0,  # \n",
        "    temp_embedding=False\n",
        "):\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    \n",
        "    if temp_embedding:\n",
        "      time_embedding = TFPositionalEncoding1D(channels=input_shape[-1])(inputs)\n",
        "      x = tf.keras.layers.Add()([inputs, time_embedding])\n",
        "    else:\n",
        "      x = inputs\n",
        "    \n",
        "    for _ in range(num_transformer_blocks):\n",
        "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
        "\n",
        "    x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
        "    for dim in mlp_units:\n",
        "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
        "        x = layers.Dropout(mlp_dropout)(x)\n",
        "    outputs = layers.Dense(1, kernel_initializer=\"normal\")(x)\n",
        "    return keras.Model(inputs, outputs)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T00:50:00.893052Z",
          "iopub.execute_input": "2022-02-17T00:50:00.893667Z",
          "iopub.status.idle": "2022-02-17T00:50:00.904069Z",
          "shell.execute_reply.started": "2022-02-17T00:50:00.893633Z",
          "shell.execute_reply": "2022-02-17T00:50:00.903093Z"
        },
        "trusted": true,
        "id": "gI2DksYwgdap"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = train_generator[0][0].shape[1:]\n",
        "\n",
        "model = build_model(\n",
        "    input_shape,\n",
        "    head_size=256,\n",
        "    num_heads=4,\n",
        "    ff_dim=4,\n",
        "    num_transformer_blocks=8,\n",
        "    mlp_units=[128, 128],\n",
        "    mlp_dropout=0.1,\n",
        "    dropout=0.1,\n",
        "    temp_embedding=TEMP_EMBEDDING\n",
        ")\n",
        "\n",
        "# tfp.stats.correlation\n",
        "#tf_corr_new = tfp.stats.correlation(x, y=None, sample_axis=0, event_axis=-1, keepdims=False, name=None)\n",
        "# tf.contrib.metrics.streaming_pearson_correlation(logits,labels)\n",
        "\n",
        "# model.compile(\n",
        "#     optimizer=\"Adam\",\n",
        "#     loss={\"head1\": \"mse\", \"head2\": \"mse\"},\n",
        "#     loss_weights={\"head1\": HEAD1_WEIGHT, \"head2\": HEAD2_WEIGHT},\n",
        "#     metrics={\"head1\": [\"mae\"], \"head2\": [\"mae\"]}\n",
        "# )\n",
        "# \n",
        "loss_func = combined_loss\n",
        "model.compile(\n",
        "    loss=loss_func,\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=1e-2),\n",
        "    metrics=[\"mae\", tfp.stats.correlation],\n",
        ")\n",
        "model.summary()\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T00:50:01.053911Z",
          "iopub.execute_input": "2022-02-17T00:50:01.054523Z",
          "iopub.status.idle": "2022-02-17T00:50:01.738473Z",
          "shell.execute_reply.started": "2022-02-17T00:50:01.054489Z",
          "shell.execute_reply": "2022-02-17T00:50:01.737796Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1sSkWOGIgdat",
        "outputId": "7b9a2fce-fed8-4559-dab9-f7e438dc3915"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 16, 1)]      0           []                               \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 16, 1)       4           ['input_1[0][0]']                \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " multi_head_attention (MultiHea  (None, 16, 1)       7169        ['batch_normalization[0][0]',    \n",
            " dAttention)                                                      'batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 16, 1)        0           ['multi_head_attention[0][0]']   \n",
            "                                                                                                  \n",
            " tf.__operators__.add (TFOpLamb  (None, 16, 1)       0           ['dropout[0][0]',                \n",
            " da)                                                              'input_1[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 16, 1)       4           ['tf.__operators__.add[0][0]']   \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv1d (Conv1D)                (None, 16, 4)        8           ['batch_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)            (None, 16, 4)        0           ['conv1d[0][0]']                 \n",
            "                                                                                                  \n",
            " conv1d_1 (Conv1D)              (None, 16, 1)        5           ['dropout_1[0][0]']              \n",
            "                                                                                                  \n",
            " tf.__operators__.add_1 (TFOpLa  (None, 16, 1)       0           ['conv1d_1[0][0]',               \n",
            " mbda)                                                            'tf.__operators__.add[0][0]']   \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 16, 1)       4           ['tf.__operators__.add_1[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " multi_head_attention_1 (MultiH  (None, 16, 1)       7169        ['batch_normalization_2[0][0]',  \n",
            " eadAttention)                                                    'batch_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)            (None, 16, 1)        0           ['multi_head_attention_1[0][0]'] \n",
            "                                                                                                  \n",
            " tf.__operators__.add_2 (TFOpLa  (None, 16, 1)       0           ['dropout_2[0][0]',              \n",
            " mbda)                                                            'tf.__operators__.add_1[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 16, 1)       4           ['tf.__operators__.add_2[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv1d_2 (Conv1D)              (None, 16, 4)        8           ['batch_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_3 (Dropout)            (None, 16, 4)        0           ['conv1d_2[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_3 (Conv1D)              (None, 16, 1)        5           ['dropout_3[0][0]']              \n",
            "                                                                                                  \n",
            " tf.__operators__.add_3 (TFOpLa  (None, 16, 1)       0           ['conv1d_3[0][0]',               \n",
            " mbda)                                                            'tf.__operators__.add_2[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_4 (BatchNo  (None, 16, 1)       4           ['tf.__operators__.add_3[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " multi_head_attention_2 (MultiH  (None, 16, 1)       7169        ['batch_normalization_4[0][0]',  \n",
            " eadAttention)                                                    'batch_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_4 (Dropout)            (None, 16, 1)        0           ['multi_head_attention_2[0][0]'] \n",
            "                                                                                                  \n",
            " tf.__operators__.add_4 (TFOpLa  (None, 16, 1)       0           ['dropout_4[0][0]',              \n",
            " mbda)                                                            'tf.__operators__.add_3[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_5 (BatchNo  (None, 16, 1)       4           ['tf.__operators__.add_4[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv1d_4 (Conv1D)              (None, 16, 4)        8           ['batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_5 (Dropout)            (None, 16, 4)        0           ['conv1d_4[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_5 (Conv1D)              (None, 16, 1)        5           ['dropout_5[0][0]']              \n",
            "                                                                                                  \n",
            " tf.__operators__.add_5 (TFOpLa  (None, 16, 1)       0           ['conv1d_5[0][0]',               \n",
            " mbda)                                                            'tf.__operators__.add_4[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_6 (BatchNo  (None, 16, 1)       4           ['tf.__operators__.add_5[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " multi_head_attention_3 (MultiH  (None, 16, 1)       7169        ['batch_normalization_6[0][0]',  \n",
            " eadAttention)                                                    'batch_normalization_6[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_6 (Dropout)            (None, 16, 1)        0           ['multi_head_attention_3[0][0]'] \n",
            "                                                                                                  \n",
            " tf.__operators__.add_6 (TFOpLa  (None, 16, 1)       0           ['dropout_6[0][0]',              \n",
            " mbda)                                                            'tf.__operators__.add_5[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_7 (BatchNo  (None, 16, 1)       4           ['tf.__operators__.add_6[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv1d_6 (Conv1D)              (None, 16, 4)        8           ['batch_normalization_7[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_7 (Dropout)            (None, 16, 4)        0           ['conv1d_6[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_7 (Conv1D)              (None, 16, 1)        5           ['dropout_7[0][0]']              \n",
            "                                                                                                  \n",
            " tf.__operators__.add_7 (TFOpLa  (None, 16, 1)       0           ['conv1d_7[0][0]',               \n",
            " mbda)                                                            'tf.__operators__.add_6[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_8 (BatchNo  (None, 16, 1)       4           ['tf.__operators__.add_7[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " multi_head_attention_4 (MultiH  (None, 16, 1)       7169        ['batch_normalization_8[0][0]',  \n",
            " eadAttention)                                                    'batch_normalization_8[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_8 (Dropout)            (None, 16, 1)        0           ['multi_head_attention_4[0][0]'] \n",
            "                                                                                                  \n",
            " tf.__operators__.add_8 (TFOpLa  (None, 16, 1)       0           ['dropout_8[0][0]',              \n",
            " mbda)                                                            'tf.__operators__.add_7[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_9 (BatchNo  (None, 16, 1)       4           ['tf.__operators__.add_8[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv1d_8 (Conv1D)              (None, 16, 4)        8           ['batch_normalization_9[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_9 (Dropout)            (None, 16, 4)        0           ['conv1d_8[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_9 (Conv1D)              (None, 16, 1)        5           ['dropout_9[0][0]']              \n",
            "                                                                                                  \n",
            " tf.__operators__.add_9 (TFOpLa  (None, 16, 1)       0           ['conv1d_9[0][0]',               \n",
            " mbda)                                                            'tf.__operators__.add_8[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_10 (BatchN  (None, 16, 1)       4           ['tf.__operators__.add_9[0][0]'] \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_5 (MultiH  (None, 16, 1)       7169        ['batch_normalization_10[0][0]', \n",
            " eadAttention)                                                    'batch_normalization_10[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_10 (Dropout)           (None, 16, 1)        0           ['multi_head_attention_5[0][0]'] \n",
            "                                                                                                  \n",
            " tf.__operators__.add_10 (TFOpL  (None, 16, 1)       0           ['dropout_10[0][0]',             \n",
            " ambda)                                                           'tf.__operators__.add_9[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_11 (BatchN  (None, 16, 1)       4           ['tf.__operators__.add_10[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv1d_10 (Conv1D)             (None, 16, 4)        8           ['batch_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_11 (Dropout)           (None, 16, 4)        0           ['conv1d_10[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_11 (Conv1D)             (None, 16, 1)        5           ['dropout_11[0][0]']             \n",
            "                                                                                                  \n",
            " tf.__operators__.add_11 (TFOpL  (None, 16, 1)       0           ['conv1d_11[0][0]',              \n",
            " ambda)                                                           'tf.__operators__.add_10[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_12 (BatchN  (None, 16, 1)       4           ['tf.__operators__.add_11[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_6 (MultiH  (None, 16, 1)       7169        ['batch_normalization_12[0][0]', \n",
            " eadAttention)                                                    'batch_normalization_12[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_12 (Dropout)           (None, 16, 1)        0           ['multi_head_attention_6[0][0]'] \n",
            "                                                                                                  \n",
            " tf.__operators__.add_12 (TFOpL  (None, 16, 1)       0           ['dropout_12[0][0]',             \n",
            " ambda)                                                           'tf.__operators__.add_11[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_13 (BatchN  (None, 16, 1)       4           ['tf.__operators__.add_12[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv1d_12 (Conv1D)             (None, 16, 4)        8           ['batch_normalization_13[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_13 (Dropout)           (None, 16, 4)        0           ['conv1d_12[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_13 (Conv1D)             (None, 16, 1)        5           ['dropout_13[0][0]']             \n",
            "                                                                                                  \n",
            " tf.__operators__.add_13 (TFOpL  (None, 16, 1)       0           ['conv1d_13[0][0]',              \n",
            " ambda)                                                           'tf.__operators__.add_12[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_14 (BatchN  (None, 16, 1)       4           ['tf.__operators__.add_13[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_7 (MultiH  (None, 16, 1)       7169        ['batch_normalization_14[0][0]', \n",
            " eadAttention)                                                    'batch_normalization_14[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_14 (Dropout)           (None, 16, 1)        0           ['multi_head_attention_7[0][0]'] \n",
            "                                                                                                  \n",
            " tf.__operators__.add_14 (TFOpL  (None, 16, 1)       0           ['dropout_14[0][0]',             \n",
            " ambda)                                                           'tf.__operators__.add_13[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_15 (BatchN  (None, 16, 1)       4           ['tf.__operators__.add_14[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv1d_14 (Conv1D)             (None, 16, 4)        8           ['batch_normalization_15[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_15 (Dropout)           (None, 16, 4)        0           ['conv1d_14[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_15 (Conv1D)             (None, 16, 1)        5           ['dropout_15[0][0]']             \n",
            "                                                                                                  \n",
            " tf.__operators__.add_15 (TFOpL  (None, 16, 1)       0           ['conv1d_15[0][0]',              \n",
            " ambda)                                                           'tf.__operators__.add_14[0][0]']\n",
            "                                                                                                  \n",
            " global_average_pooling1d (Glob  (None, 16)          0           ['tf.__operators__.add_15[0][0]']\n",
            " alAveragePooling1D)                                                                              \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 128)          2176        ['global_average_pooling1d[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_16 (Dropout)           (None, 128)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 128)          16512       ['dropout_16[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_17 (Dropout)           (None, 128)          0           ['dense_1[0][0]']                \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 1)            129         ['dropout_17[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 76,337\n",
            "Trainable params: 76,305\n",
            "Non-trainable params: 32\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def lr_scheduler(epoch, lr, warmup_epochs=15, decay_epochs=100, initial_lr=1e-6, base_lr=1e-3, min_lr=5e-5):\n",
        "    if epoch <= warmup_epochs:\n",
        "        pct = epoch / warmup_epochs\n",
        "        return ((base_lr - initial_lr) * pct) + initial_lr\n",
        "\n",
        "    if epoch > warmup_epochs and epoch < warmup_epochs+decay_epochs:\n",
        "        pct = 1 - ((epoch - warmup_epochs) / decay_epochs)\n",
        "        return ((base_lr - min_lr) * pct) + min_lr\n",
        "\n",
        "    return min_lr\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T00:50:01.748404Z",
          "iopub.execute_input": "2022-02-17T00:50:01.748891Z",
          "iopub.status.idle": "2022-02-17T00:50:01.757265Z",
          "shell.execute_reply.started": "2022-02-17T00:50:01.748849Z",
          "shell.execute_reply": "2022-02-17T00:50:01.756451Z"
        },
        "trusted": true,
        "id": "6KLf5ADwgdav"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_filepath = f\"/content/gdrive/MyDrive/Github/GResearch/G-ResearchCryptoPrediction/models/checkpoints/\"+ \\\n",
        "  f\"tf_model_features_{len(features)}_seqlen_{WINDOW_SIZE}_tempembedding_{TEMP_EMBEDDING}\"    # +\"cp_{epoch:02d}\"\n",
        "history_filepath = f\"/content/gdrive/MyDrive/Github/GResearch/G-ResearchCryptoPrediction/history/\"+ \\\n",
        "  f\"tf_model_features_{len(features)}_seqlen_{WINDOW_SIZE}_tempembedding_{TEMP_EMBEDDING}.log\"\n",
        "  \n",
        "callbacks = [keras.callbacks.EarlyStopping(monitor='val_correlation', mode='max',patience=5, restore_best_weights=True)]\n",
        "callbacks += [keras.callbacks.LearningRateScheduler(lr_scheduler, verbose=1)]\n",
        "callbacks += [keras.callbacks.ModelCheckpoint(model_filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False)]\n",
        "callbacks += [keras.callbacks.CSVLogger(history_filepath, separator=',', append=False)]\n",
        "\n",
        "EPOCHS = 20\n",
        "history = model.fit(\n",
        "            train_generator, \n",
        "            validation_data = (val_generator),\n",
        "            epochs=EPOCHS,\n",
        "            batch_size=BATCH_SIZE,\n",
        "            callbacks=callbacks)\n",
        "\n",
        "model.evaluate(test_generator, verbose=1)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T00:50:01.76068Z",
          "iopub.execute_input": "2022-02-17T00:50:01.760949Z",
          "iopub.status.idle": "2022-02-17T00:59:13.604094Z",
          "shell.execute_reply.started": "2022-02-17T00:50:01.76092Z",
          "shell.execute_reply": "2022-02-17T00:59:13.603416Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxX1sJJKgdaw",
        "outputId": "303773e7-e924-4fde-e913-f0a68c4264fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1: LearningRateScheduler setting learning rate to 1e-06.\n",
            "Epoch 1/20\n",
            "223/223 [==============================] - ETA: 0s - loss: 0.0061 - mae: 0.0285 - correlation: 0.0162\n",
            "Epoch 1: val_loss improved from inf to -0.04104, saving model to /content/gdrive/MyDrive/Github/GResearch/G-ResearchCryptoPrediction/models/checkpoints/tf_model_features_1_seqlen_16_tempembedding_False\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as query_layer_call_fn, query_layer_call_and_return_conditional_losses, key_layer_call_fn, key_layer_call_and_return_conditional_losses, value_layer_call_fn while saving (showing 5 of 96). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/Github/GResearch/G-ResearchCryptoPrediction/models/checkpoints/tf_model_features_1_seqlen_16_tempembedding_False/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/Github/GResearch/G-ResearchCryptoPrediction/models/checkpoints/tf_model_features_1_seqlen_16_tempembedding_False/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r223/223 [==============================] - 57s 254ms/step - loss: 0.0061 - mae: 0.0285 - correlation: 0.0162 - val_loss: -0.0410 - val_mae: 0.0088 - val_correlation: 0.0909 - lr: 1.0000e-06\n",
            "\n",
            "Epoch 2: LearningRateScheduler setting learning rate to 6.76e-05.\n",
            "Epoch 2/20\n",
            "223/223 [==============================] - ETA: 0s - loss: -0.0337 - mae: 0.0254 - correlation: 0.0929\n",
            "Epoch 2: val_loss improved from -0.04104 to -0.09734, saving model to /content/gdrive/MyDrive/Github/GResearch/G-ResearchCryptoPrediction/models/checkpoints/tf_model_features_1_seqlen_16_tempembedding_False\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as query_layer_call_fn, query_layer_call_and_return_conditional_losses, key_layer_call_fn, key_layer_call_and_return_conditional_losses, value_layer_call_fn while saving (showing 5 of 96). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/Github/GResearch/G-ResearchCryptoPrediction/models/checkpoints/tf_model_features_1_seqlen_16_tempembedding_False/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/Github/GResearch/G-ResearchCryptoPrediction/models/checkpoints/tf_model_features_1_seqlen_16_tempembedding_False/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r223/223 [==============================] - 44s 196ms/step - loss: -0.0337 - mae: 0.0254 - correlation: 0.0929 - val_loss: -0.0973 - val_mae: 0.0263 - val_correlation: 0.2210 - lr: 6.7600e-05\n",
            "\n",
            "Epoch 3: LearningRateScheduler setting learning rate to 0.0001342.\n",
            "Epoch 3/20\n",
            "223/223 [==============================] - ETA: 0s - loss: -0.0454 - mae: 0.0331 - correlation: 0.1239\n",
            "Epoch 3: val_loss did not improve from -0.09734\n",
            "223/223 [==============================] - 27s 120ms/step - loss: -0.0454 - mae: 0.0331 - correlation: 0.1239 - val_loss: -0.0895 - val_mae: 0.0262 - val_correlation: 0.2053 - lr: 1.3420e-04\n",
            "\n",
            "Epoch 4: LearningRateScheduler setting learning rate to 0.00020080000000000003.\n",
            "Epoch 4/20\n",
            "223/223 [==============================] - ETA: 0s - loss: -0.0545 - mae: 0.0290 - correlation: 0.1380\n",
            "Epoch 4: val_loss did not improve from -0.09734\n",
            "223/223 [==============================] - 27s 119ms/step - loss: -0.0545 - mae: 0.0290 - correlation: 0.1380 - val_loss: -0.0819 - val_mae: 0.0241 - val_correlation: 0.1879 - lr: 2.0080e-04\n",
            "\n",
            "Epoch 5: LearningRateScheduler setting learning rate to 0.00026740000000000005.\n",
            "Epoch 5/20\n",
            "223/223 [==============================] - ETA: 0s - loss: -0.0593 - mae: 0.0271 - correlation: 0.1457\n",
            "Epoch 5: val_loss did not improve from -0.09734\n",
            "223/223 [==============================] - 26s 118ms/step - loss: -0.0593 - mae: 0.0271 - correlation: 0.1457 - val_loss: -0.0890 - val_mae: 0.0135 - val_correlation: 0.1916 - lr: 2.6740e-04\n",
            "\n",
            "Epoch 6: LearningRateScheduler setting learning rate to 0.00033400000000000004.\n",
            "Epoch 6/20\n",
            "208/223 [==========================>...] - ETA: 1s - loss: -0.0611 - mae: 0.0240 - correlation: 0.1462"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tf.keras.utils.plot_model(get_model(), show_shapes=True)"
      ],
      "metadata": {
        "papermill": {
          "duration": 6.240302,
          "end_time": "2021-11-29T18:09:55.791092",
          "exception": false,
          "start_time": "2021-11-29T18:09:49.55079",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2022-02-17T00:59:13.605398Z",
          "iopub.execute_input": "2022-02-17T00:59:13.605663Z",
          "iopub.status.idle": "2022-02-17T00:59:13.610915Z",
          "shell.execute_reply.started": "2022-02-17T00:59:13.605628Z",
          "shell.execute_reply": "2022-02-17T00:59:13.608967Z"
        },
        "trusted": true,
        "id": "xUVEr5uZgdaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load custom model from gdrive and compile\n",
        "# model = keras.models.load_model(model_filepath, compile=False) # ,custom_objects={'loss_func':loss_func})\n",
        "# model.compile(\n",
        "#     loss=loss_func,\n",
        "#     optimizer=keras.optimizers.Adam(learning_rate=1e-2),\n",
        "#     metrics=[\"mae\", tfp.stats.correlation],\n",
        "# )\n",
        "\n",
        "# history = pd.read_csv('training.log', sep=',', engine='python')"
      ],
      "metadata": {
        "papermill": {
          "duration": 791.992821,
          "end_time": "2021-11-29T18:23:07.875583",
          "exception": false,
          "start_time": "2021-11-29T18:09:55.882762",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2022-02-17T00:59:13.613208Z",
          "iopub.execute_input": "2022-02-17T00:59:13.613664Z",
          "iopub.status.idle": "2022-02-17T00:59:13.619716Z",
          "shell.execute_reply.started": "2022-02-17T00:59:13.613625Z",
          "shell.execute_reply": "2022-02-17T00:59:13.618913Z"
        },
        "trusted": true,
        "id": "-P7xiiNsgdax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "NMxVpq3LyREN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['val_loss'], label= \"validation_loss\")\n",
        "plt.plot(history.history['loss'], label= \"train_loss\")\n",
        "plt.legend(loc=\"upper left\", bbox_to_anchor=(1, 0.5))\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T00:59:13.620719Z",
          "iopub.execute_input": "2022-02-17T00:59:13.620977Z",
          "iopub.status.idle": "2022-02-17T00:59:13.834846Z",
          "shell.execute_reply.started": "2022-02-17T00:59:13.620926Z",
          "shell.execute_reply": "2022-02-17T00:59:13.834207Z"
        },
        "trusted": true,
        "id": "o5Xag6hqgday"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['val_mae'], label= \"validation metric (mae)\")\n",
        "plt.plot(history.history['mae'], label= \"train metric (mae)\")\n",
        "plt.legend(loc=\"upper left\", bbox_to_anchor=(1, 0.5))\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T00:59:13.836128Z",
          "iopub.execute_input": "2022-02-17T00:59:13.836366Z",
          "iopub.status.idle": "2022-02-17T00:59:14.056202Z",
          "shell.execute_reply.started": "2022-02-17T00:59:13.836333Z",
          "shell.execute_reply": "2022-02-17T00:59:14.055496Z"
        },
        "trusted": true,
        "id": "nfdEIm0mgdaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history.history.keys()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T00:59:14.057348Z",
          "iopub.execute_input": "2022-02-17T00:59:14.058255Z",
          "iopub.status.idle": "2022-02-17T00:59:14.063936Z",
          "shell.execute_reply.started": "2022-02-17T00:59:14.058214Z",
          "shell.execute_reply": "2022-02-17T00:59:14.063131Z"
        },
        "trusted": true,
        "id": "IWgwFtE0gdaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['val_correlation'], label= \"validation metric W-Corr\")\n",
        "plt.plot(history.history['correlation'], label= \"train metric W-Corr\")\n",
        "plt.legend(loc=\"upper left\", bbox_to_anchor=(1, 0.5))\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T00:59:14.065698Z",
          "iopub.execute_input": "2022-02-17T00:59:14.066047Z",
          "iopub.status.idle": "2022-02-17T00:59:14.281173Z",
          "shell.execute_reply.started": "2022-02-17T00:59:14.066013Z",
          "shell.execute_reply": "2022-02-17T00:59:14.280453Z"
        },
        "trusted": true,
        "id": "eUm5j_C1gda0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create predictions on training set\n",
        "predictions = model.predict(train_generator)\n",
        "y_true = []\n",
        "for x,y in train_generator: y_true.append(y)\n",
        "y_true = np.squeeze(np.concatenate(y_true), axis=-1)\n",
        "\n",
        "\n",
        "print(predictions.shape, y_true.shape)\n",
        "assert predictions.shape == y_true.shape, f\"{predictions.shape}, {y_true.shape}\"\n",
        "\n",
        "# Evaluate predictions on validation set\n",
        "print(\"Window Size: \", WINDOW_SIZE)\n",
        "print(\"Prediction length: \", prediction_length)\n",
        "print(\"Epochs: \", EPOCHS)\n",
        "\n",
        "print('---------------------')\n",
        "print('Asset:    Corr. coef.')\n",
        "print('---------------------')\n",
        "asset_w_corr = []\n",
        "asset_corrs = []\n",
        "asset_mae = []\n",
        "y_true = np.squeeze(y_true)\n",
        "y_pred = np.squeeze(predictions)\n",
        "real_target_ind = np.argwhere(y_true!=0)\n",
        "# asset_id = list(assets_order.keys())[i]\n",
        "# asset_name = assets[assets.Asset_ID == asset_id]['Asset_Name'].item()\n",
        "mae_asset = mae(y_true, y_pred)\n",
        "asset_corr = np.corrcoef(np.nan_to_num(y_pred.flatten()), np.nan_to_num(y_true.flatten()))[0,1]\n",
        "\n",
        "print(f\"corr: {asset_corr:.4f}\")\n",
        "print(f\"mae: {mae_asset:.4f}\")\n",
        "print(\"\\n\")\n",
        "print(f\"Predictions Min: {predictions.min()}, Predictions Max: {predictions.max()}, Predictions mean:{predictions.mean()}, Predictions var:{predictions.var()}\") \n",
        "print(f\"y_true Min: {y_true.min()}, y_true Max: {y_true.max()}, y_true mean:{y_true.mean()}, y_true var:{y_true.var()}\") "
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T00:59:14.283396Z",
          "iopub.execute_input": "2022-02-17T00:59:14.284233Z",
          "iopub.status.idle": "2022-02-17T00:59:25.446139Z",
          "shell.execute_reply.started": "2022-02-17T00:59:14.284191Z",
          "shell.execute_reply": "2022-02-17T00:59:25.44524Z"
        },
        "trusted": true,
        "id": "05KQCO5-gda2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create predictions on validation set\n",
        "predictions = model.predict(val_generator)\n",
        "y_true = []\n",
        "for x,y in val_generator: y_true.append(y)\n",
        "y_true = np.squeeze(np.concatenate(y_true), axis=-1)\n",
        "\n",
        "\n",
        "print(predictions.shape, y_true.shape)\n",
        "assert predictions.shape == y_true.shape, f\"{predictions.shape}, {y_true.shape}\"\n",
        "\n",
        "# Evaluate predictions on validation set\n",
        "print(\"Window Size: \", WINDOW_SIZE)\n",
        "print(\"Prediction length: \", prediction_length)\n",
        "print(\"Epochs: \", EPOCHS)\n",
        "\n",
        "print('---------------------')\n",
        "print('Asset:    Corr. coef.')\n",
        "print('---------------------')\n",
        "asset_w_corr = []\n",
        "asset_corrs = []\n",
        "asset_mae = []\n",
        "y_true = np.squeeze(y_true)\n",
        "y_pred = np.squeeze(predictions)\n",
        "real_target_ind = np.argwhere(y_true!=0)\n",
        "# asset_id = list(assets_order.keys())[i]\n",
        "# asset_name = assets[assets.Asset_ID == asset_id]['Asset_Name'].item()\n",
        "mae_asset = mae(y_true, y_pred)\n",
        "asset_corr = np.corrcoef(np.nan_to_num(y_pred.flatten()), np.nan_to_num(y_true.flatten()))[0,1]\n",
        "\n",
        "print(f\"corr: {asset_corr:.4f}\")\n",
        "print(f\"mae: {mae_asset:.4f}\")\n",
        "print(\"\\n\")\n",
        "print(f\"Predictions Min: {predictions.min()}, Predictions Max: {predictions.max()}, Predictions mean:{predictions.mean()}, Predictions var:{predictions.var()}\") \n",
        "print(f\"y_true Min: {y_true.min()}, y_true Max: {y_true.max()}, y_true mean:{y_true.mean()}, y_true var:{y_true.var()}\") "
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T00:59:25.448301Z",
          "iopub.execute_input": "2022-02-17T00:59:25.448786Z",
          "iopub.status.idle": "2022-02-17T00:59:26.72903Z",
          "shell.execute_reply.started": "2022-02-17T00:59:25.448742Z",
          "shell.execute_reply": "2022-02-17T00:59:26.727478Z"
        },
        "trusted": true,
        "id": "VOXCGhkQgda3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create predictions on validation set\n",
        "predictions = model.predict(test_generator)\n",
        "y_true = []\n",
        "for x,y in test_generator: y_true.append(y)\n",
        "y_true = np.squeeze(np.concatenate(y_true), axis=-1)\n",
        "\n",
        "\n",
        "print(predictions.shape, y_true.shape)\n",
        "assert predictions.shape == y_true.shape, f\"{predictions.shape}, {y_true.shape}\"\n",
        "\n",
        "# Evaluate predictions on validation set\n",
        "print(\"Window Size: \", WINDOW_SIZE)\n",
        "print(\"Prediction length: \", prediction_length)\n",
        "print(\"Epochs: \", EPOCHS)\n",
        "\n",
        "asset_w_corr = []\n",
        "asset_corrs = []\n",
        "asset_mae = []\n",
        "y_true = np.squeeze(y_true)\n",
        "predictions = np.squeeze(predictions)\n",
        "real_target_ind = np.argwhere(y_true!=0)\n",
        "# asset_id = list(assets_order.keys())[i]\n",
        "# asset_name = assets[assets.Asset_ID == asset_id]['Asset_Name'].item()\n",
        "mae_asset = mae(y_true, predictions)\n",
        "asset_corr = np.corrcoef(np.nan_to_num(y_pred.flatten()), np.nan_to_num(y_true.flatten()))[0,1]\n",
        "\n",
        "print('---------------------')\n",
        "print('Asset:    Corr. coef.')\n",
        "print('---------------------')\n",
        "print(f\"corr: {asset_corr:.4f}\")\n",
        "print(f\"mae: {mae_asset:.4f}\")\n",
        "print(\"\\n\")\n",
        "print(f\"Predictions Min: {predictions.min()}, Predictions Max: {predictions.max()}, Predictions mean:{predictions.mean()}, Predictions var:{predictions.var()}\") \n",
        "print(f\"y_true Min: {y_true.min()}, y_true Max: {y_true.max()}, y_true mean:{y_true.mean()}, y_true var:{y_true.var()}\") \n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T00:59:26.731599Z",
          "iopub.execute_input": "2022-02-17T00:59:26.731937Z",
          "iopub.status.idle": "2022-02-17T00:59:27.980154Z",
          "shell.execute_reply.started": "2022-02-17T00:59:26.731852Z",
          "shell.execute_reply": "2022-02-17T00:59:27.979435Z"
        },
        "trusted": true,
        "id": "9VGActtpgda6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "oxHwOrV8gda7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Eq5SN190gda8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "MnRNd5Digda8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "jx43e9xXgda9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "IHhEifvKgda9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "qld_i0lRgda_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "EJkCv_SOgda_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}