{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "transformer-model_colab.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Niggl0n/G-ResearchCryptoPrediction/blob/main/transformer-model_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Todos\n",
        "\n",
        "- Save model\n",
        "- check for other necessray callbacks\n",
        "- run version 8 with 16 shift\n",
        "- Embedding\n",
        "    - evaluate embedding\n",
        "    - run new embedding version on full asset data\n",
        "    - search an try alternatives\n",
        "        - add sin / cos signals\n",
        "- Check for other LR scheduler\n",
        "- Read for decoder necessety "
      ],
      "metadata": {
        "id": "IeHbanPQgdZ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Kernel"
      ],
      "metadata": {
        "id": "bqI4QtfZgdaC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datatable\n",
        "!pip install tensorflow-addons"
      ],
      "metadata": {
        "id": "i5r1geHXmBGX",
        "outputId": "d77bcbcb-c762-4e6e-b6b0-bd1af961ddd2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datatable\n",
            "  Downloading datatable-1.0.0-cp37-cp37m-manylinux_2_12_x86_64.whl (96.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 96.9 MB 96 kB/s \n",
            "\u001b[?25hInstalling collected packages: datatable\n",
            "Successfully installed datatable-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "%cd /content/gdrive/MyDrive/Github/GResearch/\n",
        "\n",
        "# if not os.path.exists(\"../input/g-research-crypto-forecasting/\"): os.chdir('/t/Datasets/kaggle_crypto/internal')\n"
      ],
      "metadata": {
        "id": "YhH5N8XcghNi",
        "outputId": "a7049985-6b1a-425d-9cf8-0ec828866e8d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "/content/gdrive/MyDrive/Github/GResearch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "_kg_hide-output": true,
        "papermill": {
          "duration": 0.072966,
          "end_time": "2021-11-29T18:05:23.845682",
          "exception": false,
          "start_time": "2021-11-29T18:05:23.772716",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2022-02-17T00:37:26.676103Z",
          "iopub.execute_input": "2022-02-17T00:37:26.676697Z",
          "iopub.status.idle": "2022-02-17T00:37:26.700579Z",
          "shell.execute_reply.started": "2022-02-17T00:37:26.676605Z",
          "shell.execute_reply": "2022-02-17T00:37:26.69995Z"
        },
        "trusted": true,
        "id": "V7tOVJGxgdaF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gc\n",
        "import datatable as dt\n",
        "import traceback\n",
        "import pdb\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from IPython.core.display import display, HTML, Javascript\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
        "from tensorflow_addons.layers import MultiHeadAttention\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd, numpy as np\n",
        "from functools import partial\n",
        "from scipy.stats import pearsonr\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.preprocessing import RobustScaler, MinMaxScaler\n",
        "from sklearn.metrics import mean_absolute_error as mae\n",
        "\n",
        "pd.set_option('display.max_columns', None)"
      ],
      "metadata": {
        "papermill": {
          "duration": 9.261664,
          "end_time": "2021-11-29T18:05:34.914352",
          "exception": false,
          "start_time": "2021-11-29T18:05:25.652688",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2022-02-17T00:37:27.449865Z",
          "iopub.execute_input": "2022-02-17T00:37:27.450127Z",
          "iopub.status.idle": "2022-02-17T00:37:34.411247Z",
          "shell.execute_reply.started": "2022-02-17T00:37:27.450097Z",
          "shell.execute_reply": "2022-02-17T00:37:34.410533Z"
        },
        "trusted": true,
        "id": "fUeURtRcgdaH"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = \"GPU\" #or \"TPU\"\n",
        "\n",
        "SEED = 42\n",
        "\n",
        "# LOAD STRICT? YES=1 NO=0 | see: https://www.kaggle.com/julian3833/proposal-for-a-meaningful-lb-strict-lgbm\n",
        "LOAD_STRICT = True\n",
        "\n",
        "# WHICH YEARS TO INCLUDE? YES=1 NO=0\n",
        "INC2021 = 0\n",
        "INC2020 = 0\n",
        "INC2019 = 0\n",
        "INC2018 = 0\n",
        "INC2017 = 0\n",
        "INCCOMP = 1\n",
        "INCSUPP = 0\n",
        "\n",
        "# TRAINING PARAMETERS\n",
        "DEBUG = True\n",
        "SINGLE_ASSET = True\n",
        "asset_id=3\n",
        "N_ASSETS = 14\n",
        "num_shift = 15\n",
        "TIME2VEC_DIM=3\n",
        "WINDOW_SIZE = 64\n",
        "prediction_length = 1\n",
        "BATCH_SIZE = 64\n",
        "PCT_VALIDATION = 10 # last 10% of the data are used as validation set\n",
        "PCT_TEST = 10 # last 10% of the data are used as validation set"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.050546,
          "end_time": "2021-11-29T18:05:35.179716",
          "exception": false,
          "start_time": "2021-11-29T18:05:35.12917",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2022-02-17T00:37:34.412864Z",
          "iopub.execute_input": "2022-02-17T00:37:34.413166Z",
          "iopub.status.idle": "2022-02-17T00:37:34.422577Z",
          "shell.execute_reply.started": "2022-02-17T00:37:34.413132Z",
          "shell.execute_reply": "2022-02-17T00:37:34.421333Z"
        },
        "trusted": true,
        "id": "qQD2DffPgdaK"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if DEVICE == \"TPU\":\n",
        "    print(\"connecting to TPU...\")\n",
        "    try:\n",
        "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "        print('Running on TPU ', tpu.master())\n",
        "    except ValueError:\n",
        "        tpu = None\n",
        "    if tpu:\n",
        "        try:\n",
        "            print(\"initializing  TPU ...\")\n",
        "            tf.config.experimental_connect_to_cluster(tpu)\n",
        "            tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "            strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
        "            print(\"TPU initialized\")\n",
        "        except: print(\"failed to initialize TPU\")\n",
        "    else: DEVICE = \"GPU\"\n",
        "\n",
        "if DEVICE != \"TPU\": strategy = tf.distribute.get_strategy()\n",
        "if DEVICE == \"GPU\": print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
        "AUTO     = tf.data.experimental.AUTOTUNE\n",
        "REPLICAS = strategy.num_replicas_in_sync"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "papermill": {
          "duration": 0.069363,
          "end_time": "2021-11-29T18:05:35.290484",
          "exception": false,
          "start_time": "2021-11-29T18:05:35.221121",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2022-02-17T00:37:34.424968Z",
          "iopub.execute_input": "2022-02-17T00:37:34.425933Z",
          "iopub.status.idle": "2022-02-17T00:37:34.593804Z",
          "shell.execute_reply.started": "2022-02-17T00:37:34.425866Z",
          "shell.execute_reply": "2022-02-17T00:37:34.593104Z"
        },
        "trusted": true,
        "id": "cgeI6XZUgdaM",
        "outputId": "03bdb7a0-d480-44d8-bfd8-df5eae922141",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_target(df):\n",
        "    \"\"\"\n",
        "    \n",
        "    \"\"\"\n",
        "    R=list()\n",
        "    c=list(df['Close'])\n",
        "    for i in range(df.shape[0]):\n",
        "        future=c[min([i+16,df.shape[0]-1])]\n",
        "        past=c[min([i+1,df.shape[0]-1])]\n",
        "        R.append(future/past)\n",
        "    df['R']=R\n",
        "    df['R']=np.log(df['R'])\n",
        "    df['pred']=np.exp(df['R'])-1\n",
        "    return df\n",
        "\n",
        "\n",
        "# we will use weighted correlation as function to evaluate our model performance\n",
        "# https://stackoverflow.com/questions/38641691/weighted-correlation-coefficient-with-pandas\n",
        "def wmean(x, w):\n",
        "    return np.sum(x * w) / np.sum(w)\n",
        "\n",
        "def wcov(x, y, w):\n",
        "    return np.sum(w * (x - wmean(x, w)) * (y - wmean(y, w))) / np.sum(w)\n",
        "\n",
        "def wcorr(x, y, w):\n",
        "    return wcov(x, y, w) / np.sqrt(wcov(x, x, w) * wcov(y, y, w))\n",
        "\n",
        "def wcorr(x, y, w=1):\n",
        "    return wcov(x, y, w) / np.sqrt(wcov(x, x, w) * wcov(y, y, w))\n",
        "\n",
        "# ----------------------------------------------------------------\n",
        "# ----------------------------------------------------------------\n",
        "# def tf_wmean(x, w):\n",
        "#     w_sum = tf.math.reduce_sum(w)\n",
        "#     xw_sum = tf.math.reduce_sum(tf.math.multiply(x, w))\n",
        "#     return  tf.math.divide(xw_sum, w_sum)\n",
        "# \n",
        "# def tf_wcov(x, y, w):\n",
        "#     w_sum = tf.math.reduce_sum(w)\n",
        "#     xyw = tf.math.multiply(x - tf_wmean(x, w), y - tf_wmean(y, w))\n",
        "#     w_xy_sum= tf.math.reduce_sum(tf.math.multiply(w, xyw))\n",
        "#     return  tf.math.divide(w_xy_sum, w_sum)\n",
        "# \n",
        "# def tf_wcorr(x, y, w=1):\n",
        "#     if not w:\n",
        "#         w=1.\n",
        "#     x = tf.cast(x, tf.float32)\n",
        "#     y = tf.cast(y, tf.float32)\n",
        "#     w = tf.cast(w, tf.float32)\n",
        "#     \n",
        "#     mul_xwcov_y_wcov = tf.math.multiply(tf_wcov(x, x, w), tf_wcov(y, y, w))\n",
        "#     return tf.math.divide(tf_wcov(x, y, w), tf.math.sqrt(mul_xwcov_y_wcov))\n",
        "\n",
        "\n",
        "\n",
        "def corr_loss(y_true, y_pred):\n",
        "    x = tf.cast(y_true, tf.float32)\n",
        "    y = tf.cast(y_pred, tf.float32)\n",
        "    mx = K.mean(x)\n",
        "    my = K.mean(y)\n",
        "    xm, ym = x-mx, y-my\n",
        "    r_num = K.sum(tf.multiply(xm,ym))\n",
        "    r_den = K.sqrt(tf.multiply(K.sum(K.square(xm)), K.sum(K.square(ym))))\n",
        "    r = r_num / r_den\n",
        "    r = K.maximum(K.minimum(r, 1.0), -1.0)\n",
        "    return - r\n",
        "\n",
        "def combined_loss(y_true, y_pred, weight1=0.5, weight2=None):\n",
        "    if not weight2:\n",
        "        weight2 = 1 - weight1\n",
        "    loss1 = corr_loss(y_true, y_pred)\n",
        "    mae_loss_ = tf.keras.losses.MeanAbsoluteError()\n",
        "    loss2 = mae_loss_(y_true, y_pred)\n",
        "    return loss1*weight1 + loss2*weight2\n",
        "\n",
        "\n",
        "def competition_weighted_correlation(a, b, weights=1):\n",
        "\n",
        "    #w = np.ravel(weights)\n",
        "    #a = np.ravel(a)\n",
        "    #b = np.ravel(b)\n",
        "    a = tf.cast(a, dtype=tf.float32)\n",
        "    b = tf.cast(b, dtype=tf.float32)\n",
        "    \n",
        "    len_a = tf.cast(tf.size(a), dtype=tf.float32)\n",
        "    len_b = tf.cast(tf.size(b), dtype=tf.float32)\n",
        "    #sum_w = tf.reduce_sum(w)\n",
        "    mean_a = tf.cast(tf.math.reduce_mean(a), dtype=tf.float32)\n",
        "    mean_b = tf.cast(tf.math.reduce_mean(b), dtype=tf.float32)\n",
        "    var_a = tf.cast(tf.math.reduce_sum(tf.math.square(a - mean_a)) / len_a, dtype=tf.float32)\n",
        "    var_b = tf.cast(tf.math.reduce_sum(tf.math.square(b - mean_b)) / len_b, dtype=tf.float32)\n",
        "\n",
        "    cov = tf.math.reduce_sum((a * b)) / len_a - (mean_a * mean_b)\n",
        "    corr = cov / tf.math.sqrt(var_a * var_b)\n",
        "\n",
        "    return corr"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T00:37:34.597079Z",
          "iopub.execute_input": "2022-02-17T00:37:34.597276Z",
          "iopub.status.idle": "2022-02-17T00:37:34.61722Z",
          "shell.execute_reply.started": "2022-02-17T00:37:34.59725Z",
          "shell.execute_reply": "2022-02-17T00:37:34.616206Z"
        },
        "trusted": true,
        "id": "LqMReNYPgdaO"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extra_data_files = {0: '', \n",
        "                    2: '', \n",
        "                    1: '', \n",
        "                    3: '', \n",
        "                    4: '', \n",
        "                    5: '', \n",
        "                    6: '', \n",
        "                    7: '', \n",
        "                    8: '', \n",
        "                    9: '', \n",
        "                    11:'', \n",
        "                    10:'', \n",
        "                    12:'', \n",
        "                    13:''}\n",
        "\n",
        "# Uncomment to load the original csv [slower]\n",
        "# orig_df_train = pd.read_csv(data_path + 'train.csv') \n",
        "# supp_df_train = pd.read_csv(data_path + 'supplemental_train.csv')\n",
        "# df_asset_details = pd.read_csv(data_path  + 'asset_details.csv').sort_values(\"Asset_ID\")\n",
        "\n",
        "orig_df_train = dt.fread('orig_train.jay').to_pandas()\n",
        "df_asset_details = dt.fread('orig_asset_details.jay').to_pandas()\n",
        "supp_df_train = dt.fread('orig_supplemental_train.jay').to_pandas()\n",
        "assets_details = dt.fread('orig_asset_details.jay').to_pandas()\n",
        "asset_weight_dict = {assets_details['Asset_ID'].tolist()[idx]: assets_details['Weight'].tolist()[idx] for idx in range(len(assets_details))}\n",
        "asset_name_dict = {assets_details['Asset_ID'].tolist()[idx]: assets_details['Asset_Name'].tolist()[idx] for idx in range(len(assets_details))}\n",
        "\n",
        "def load_training_data_for_asset(asset_id, load_jay = True):\n",
        "    dfs = []\n",
        "    if INCCOMP: dfs.append(orig_df_train[orig_df_train[\"Asset_ID\"] == asset_id].copy())\n",
        "    if INCSUPP: dfs.append(supp_df_train[supp_df_train[\"Asset_ID\"] == asset_id].copy())\n",
        "    \n",
        "    if load_jay:\n",
        "        if INC2017 and os.path.exists(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2017) + '.csv'): dfs.append(dt.fread(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2017) + '.jay').to_pandas())\n",
        "        if INC2018 and os.path.exists(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2018) + '.csv'): dfs.append(dt.fread(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2018) + '.jay').to_pandas())\n",
        "        if INC2019 and os.path.exists(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2019) + '.csv'): dfs.append(dt.fread(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2019) + '.jay').to_pandas())\n",
        "        if INC2020 and os.path.exists(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2020) + '.csv'): dfs.append(dt.fread(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2020) + '.jay').to_pandas())\n",
        "        if INC2021 and os.path.exists(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2021) + '.csv'): dfs.append(dt.fread(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2021) + '.jay').to_pandas())\n",
        "    else: \n",
        "        if INC2017 and os.path.exists(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2017) + '.csv'): dfs.append(pd.read_csv(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2017) + '.csv'))\n",
        "        if INC2018 and os.path.exists(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2018) + '.csv'): dfs.append(pd.read_csv(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2018) + '.csv'))\n",
        "        if INC2019 and os.path.exists(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2019) + '.csv'): dfs.append(pd.read_csv(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2019) + '.csv'))\n",
        "        if INC2020 and os.path.exists(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2020) + '.csv'): dfs.append(pd.read_csv(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2020) + '.csv'))\n",
        "        if INC2021 and os.path.exists(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2021) + '.csv'): dfs.append(pd.read_csv(extra_data_files[asset_id] + '/full_data__' + str(asset_id) + '__' + str(2021) + '.csv'))\n",
        "    df = pd.concat(dfs, axis = 0) if len(dfs) > 1 else dfs[0]\n",
        "    df['date'] = pd.to_datetime(df['timestamp'], unit = 's')\n",
        "    if LOAD_STRICT: df = df.loc[df['date'] < \"2021-06-13 00:00:00\"]    \n",
        "    df = df.sort_values('date')\n",
        "    return df\n",
        "\n",
        "def load_data_for_all_assets():\n",
        "    dfs = []\n",
        "    for asset_id in list(extra_data_files.keys()): dfs.append(load_training_data_for_asset(asset_id))\n",
        "    return pd.concat(dfs)"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "papermill": {
          "duration": 24.69094,
          "end_time": "2021-11-29T18:06:00.13746",
          "exception": false,
          "start_time": "2021-11-29T18:05:35.44652",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2022-02-17T00:37:34.619083Z",
          "iopub.execute_input": "2022-02-17T00:37:34.61937Z",
          "iopub.status.idle": "2022-02-17T00:37:57.756015Z",
          "shell.execute_reply.started": "2022-02-17T00:37:34.619333Z",
          "shell.execute_reply": "2022-02-17T00:37:57.755291Z"
        },
        "trusted": true,
        "id": "n8CHJ_6MgdaQ",
        "outputId": "8a296f03-255d-4ec8-9144-362c65ad6f9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style type='text/css'>\n",
              ".datatable table.frame { margin-bottom: 0; }\n",
              ".datatable table.frame thead { border-bottom: none; }\n",
              ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
              ".datatable .bool    { background: #DDDD99; }\n",
              ".datatable .object  { background: #565656; }\n",
              ".datatable .int     { background: #5D9E5D; }\n",
              ".datatable .float   { background: #4040CC; }\n",
              ".datatable .str     { background: #CC4040; }\n",
              ".datatable .time    { background: #40CC40; }\n",
              ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
              ".datatable .frame tbody td { text-align: left; }\n",
              ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
              ".datatable th:nth-child(2) { padding-left: 12px; }\n",
              ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
              ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
              ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
              ".datatable .sp {  opacity: 0.25;}\n",
              ".datatable .footer { font-size: 9px; }\n",
              ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test = dt.fread('orig_example_test.jay').to_pandas()\n",
        "sample_prediction_df = dt.fread('orig_example_sample_submission.jay').to_pandas()\n",
        "assets = dt.fread('orig_asset_details.jay').to_pandas()\n",
        "assets_order = dt.fread('orig_supplemental_train.jay').to_pandas().Asset_ID[:N_ASSETS]\n",
        "\n",
        "assets_order = dict((t,i) for i,t in enumerate(assets_order))\n",
        "print(\"Loaded all data!\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T00:37:57.758407Z",
          "iopub.execute_input": "2022-02-17T00:37:57.758916Z",
          "iopub.status.idle": "2022-02-17T00:37:57.865418Z",
          "shell.execute_reply.started": "2022-02-17T00:37:57.758876Z",
          "shell.execute_reply": "2022-02-17T00:37:57.864602Z"
        },
        "trusted": true,
        "id": "ZFd-WzivgdaT",
        "outputId": "e0785e5f-3682-476f-8dd0-9aa395c95b2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded all data!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train = load_data_for_all_assets().sort_values('timestamp').set_index(\"timestamp\")\n",
        "if DEBUG: \n",
        "    train = train[-500000:]  # [-2221694:]  #\n",
        "else:\n",
        "    train = train[1000000:]\n",
        "print(train.shape)"
      ],
      "metadata": {
        "papermill": {
          "duration": 17.031431,
          "end_time": "2021-11-29T18:06:17.210444",
          "exception": false,
          "start_time": "2021-11-29T18:06:00.179013",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2022-02-17T00:37:57.866835Z",
          "iopub.execute_input": "2022-02-17T00:37:57.867087Z",
          "iopub.status.idle": "2022-02-17T00:38:08.067318Z",
          "shell.execute_reply.started": "2022-02-17T00:37:57.867053Z",
          "shell.execute_reply": "2022-02-17T00:38:08.066523Z"
        },
        "trusted": true,
        "id": "RVNOqB16gdaV",
        "outputId": "28bde5d4-a0db-4705-b7ec-274645c04ebe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(500000, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Memory saving function credit to https://www.kaggle.com/gemartin/load-data-reduce-memory-usage\n",
        "def reduce_mem_usage(df):\n",
        "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
        "        to reduce memory usage.\n",
        "    \"\"\"\n",
        "    start_mem = df.memory_usage().sum() / 1024**2\n",
        "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
        "\n",
        "    for col in df.columns:\n",
        "        col_type = df[col].dtype.name\n",
        "        \n",
        "        if col_type not in ['object', 'category', 'datetime64[ns, UTC]', 'datetime64[ns]']:\n",
        "            c_min = df[col].min()\n",
        "            c_max = df[col].max()\n",
        "            if str(col_type)[:3] == 'int':\n",
        "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                    df[col] = df[col].astype(np.int16)\n",
        "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                    df[col] = df[col].astype(np.int32)\n",
        "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                    df[col] = df[col].astype(np.int64)  \n",
        "            else:\n",
        "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                    df[col] = df[col].astype(np.float16)\n",
        "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                    df[col] = df[col].astype(np.float32)\n",
        "                else:\n",
        "                    df[col] = df[col].astype(np.float64)\n",
        "\n",
        "    end_mem = df.memory_usage().sum() / 1024**2\n",
        "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
        "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "_kg_hide-output": true,
        "papermill": {
          "duration": 0.054816,
          "end_time": "2021-11-29T18:06:17.597492",
          "exception": false,
          "start_time": "2021-11-29T18:06:17.542676",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2022-02-17T00:38:08.068529Z",
          "iopub.execute_input": "2022-02-17T00:38:08.069014Z",
          "iopub.status.idle": "2022-02-17T00:38:08.083878Z",
          "shell.execute_reply.started": "2022-02-17T00:38:08.068972Z",
          "shell.execute_reply": "2022-02-17T00:38:08.08305Z"
        },
        "trusted": true,
        "id": "n1itrPqGgdaW"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def upper_shadow(df): return df['High'] - np.maximum(df['Close'], df['Open'])\n",
        "def lower_shadow(df): return np.minimum(df['Close'], df['Open']) - df['Low']\n",
        "\n",
        "def get_features(df, row = False):\n",
        "    df_feat = df\n",
        "    df_feat['spread'] = df_feat['High'] - df_feat['Low']\n",
        "    df_feat['mean_trade'] = df_feat['Volume']/df_feat['Count']\n",
        "    df_feat['log_price_change'] = np.log(df_feat['Close']/df_feat['Open'])\n",
        "    df_feat['upper_Shadow'] = upper_shadow(df_feat)\n",
        "    df_feat['lower_Shadow'] = lower_shadow(df_feat)\n",
        "    df_feat[\"high_div_low\"] = df_feat[\"High\"] / df_feat[\"Low\"]\n",
        "    df_feat['UPS'] = (df_feat['High'] - np.maximum(df_feat['Close'], df_feat['Open']))\n",
        "    df_feat['LOS'] = (np.minimum(df_feat['Close'], df_feat['Open']) - df_feat['Low'])\n",
        "    df_feat['LOGVOL'] = np.log(1. + df_feat['Volume'])\n",
        "    df_feat['LOGCNT'] = np.log(1. + df_feat['Count'])\n",
        "    return df_feat\n",
        "\n",
        "\n",
        "# A utility function to build features around lags.\n",
        "def get_features_hist(df_feat, row=False):\n",
        "    \n",
        "    ### features to consider. See potential features...\n",
        "    ### note that we predicting returns 15 minutes ahead. This minutes price data is therefore not sufficient. We must roll the variables, 15, 30, 90, 250, 1250\n",
        "       \n",
        "    # df_feat = df[['Open', 'High', 'Low', 'Close', 'Volume', 'Count', 'VWAP', 'lr15', 'm', 'lr16', 'Asset_ID']].copy()\n",
        "\n",
        "    if df_feat.shape[0]<3750:\n",
        "        df_feat['beta_num'] = np.nan\n",
        "        df_feat['m2'] = np.nan\n",
        "    else:\n",
        "        df_feat['beta_num'] = (df_feat['lr15']*df_feat['m']).rolling(3750).mean().values\n",
        "        df_feat['m2'] = (df_feat['m']*df_feat['m']).rolling(3750).mean().values\n",
        "        \n",
        "    if row:\n",
        "        # first .iloc as far back as we need, compute feature, then downsize until .iloc[-1]\n",
        "        df_feat = df_feat.iloc[-1]\n",
        "#         mean_price = df_feat[['Open', 'High', 'Low', 'Close']].mean()\n",
        "#         med_price = df_feat[['Open', 'High', 'Low', 'Close']].median()\n",
        "        df_feat['upper_shadow'] = df_feat['High'] / df_feat[['Close', 'Open']].max()\n",
        "        df_feat['lower_shadow'] = df_feat[['Close', 'Open']].min() / df_feat['Low']\n",
        "    else:\n",
        "#         mean_price = df_feat[['Open', 'High', 'Low', 'Close']].mean(axis=1)\n",
        "#         med_price = df_feat[['Open', 'High', 'Low', 'Close']].median(axis=1)\n",
        "        df_feat['upper_shadow'] = df_feat['High'] / df_feat[['Close', 'Open']].max(axis=1)\n",
        "        df_feat['lower_shadow'] = df_feat[['Close', 'Open']].min(axis=1) / df_feat['Low']\n",
        "        # df_feat = df_feat.drop('Asset_ID', axis=1)\n",
        "        \n",
        "    df_feat['beta'] = np.nan_to_num(df_feat['beta_num'] / df_feat['m2'], nan=0., posinf=0., neginf=0.)\n",
        "    df_feat['target_lag'] = df_feat['lr15'] - df_feat['beta']*df_feat['m']  # first 15 entries of target_lagg are NaN\n",
        "\n",
        "        ### Sense checks\n",
        "#         print((df_feat['Target'] - df_feat.groupby('Asset_ID')['target_lag'].shift(-16)).abs().mean())\n",
        "#         print(df_feat.loc[(df_feat.Target.isnull())&(df_feat.target_lag.notnull())].shape)\n",
        "#         print(df_feat.loc[(df_feat.Target.notnull())&(df_feat.target_lag.isnull())].shape)        \n",
        "        \n",
        "    df_feat['open2close'] = df_feat['Close'] / df_feat['Open']\n",
        "    df_feat['high2low'] = df_feat['High'] / df_feat['Low']\n",
        "           \n",
        "#     df_feat['high2mean'] = df_feat['High'] / mean_price\n",
        "#     df_feat['low2mean'] = df_feat['Low'] / mean_price\n",
        "#     df_feat['high2median'] = df_feat['High'] / med_price\n",
        "#     df_feat['low2median'] = df_feat['Low'] / med_price\n",
        "    df_feat['volume2count'] = df_feat['Volume'] / (df_feat['Count'] + 1)\n",
        "    df_feat['close2vwap'] = df_feat['Close'] / df_feat['VWAP']\n",
        "    \n",
        "    return df_feat.replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "def log_return(series, periods=1):\n",
        "    return np.log(series).diff(periods=periods)\n",
        "\n",
        "def timestamp_to_date(timestamp):\n",
        "    return(datetime.fromtimestamp(timestamp))\n",
        "\n",
        "def get_lead_features():\n",
        "    # get_features from ethereum\n",
        "    pass\n",
        "\n"
      ],
      "metadata": {
        "papermill": {
          "duration": 37.064504,
          "end_time": "2021-11-29T18:06:54.699951",
          "exception": false,
          "start_time": "2021-11-29T18:06:17.635447",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2022-02-17T00:38:08.085233Z",
          "iopub.execute_input": "2022-02-17T00:38:08.085625Z",
          "iopub.status.idle": "2022-02-17T00:38:08.113078Z",
          "shell.execute_reply.started": "2022-02-17T00:38:08.085585Z",
          "shell.execute_reply": "2022-02-17T00:38:08.112226Z"
        },
        "trusted": true,
        "id": "nRgybcglgdaX"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## FILTER DATAFRAME IF ONLY WORKING WITH ONE ASSET\n",
        "if SINGLE_ASSET:\n",
        "    train = train[train[\"Asset_ID\"]==asset_id]\n",
        "gc.collect()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T00:38:08.116698Z",
          "iopub.execute_input": "2022-02-17T00:38:08.117242Z",
          "iopub.status.idle": "2022-02-17T00:38:08.295694Z",
          "shell.execute_reply.started": "2022-02-17T00:38:08.117207Z",
          "shell.execute_reply": "2022-02-17T00:38:08.29475Z"
        },
        "trusted": true,
        "id": "Dg3jyBJJgdaa",
        "outputId": "b320a69e-e07b-4b17-af25-cdd1e18ef22a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "402"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train[['Count', 'Open', 'High', 'Low', 'Close', 'Volume', 'VWAP', 'Target']] = train[['Count', 'Open', 'High', 'Low', 'Close', 'Volume', 'VWAP', 'Target']].astype(np.float32)\n",
        "print(train.shape)\n",
        "train['Target'] = train['Target'].fillna(0)\n",
        "VWAP_max = np.max(train[np.isfinite(train.VWAP)].VWAP)\n",
        "VWAP_min = np.min(train[np.isfinite(train.VWAP)].VWAP)\n",
        "train['VWAP'] = np.nan_to_num(train.VWAP, posinf=VWAP_max, neginf=VWAP_min)\n",
        "df = train[['Asset_ID', 'Target']].copy()\n",
        "times = dict((t,i) for i,t in enumerate(df.index.unique()))\n",
        "df['id'] = df.index.map(times)\n",
        "df['id'] = df['id'].astype(str) + '_' + df['Asset_ID'].astype(str)\n",
        "ids = df.id.copy()\n",
        "del df\n",
        "\n",
        "train = train.sort_index()\n",
        "ind = train.index.unique()\n",
        "def reindex(df):\n",
        "    df = df.reindex(range(ind[0],ind[-1]+60,60),method='nearest')\n",
        "    df = df.fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
        "    return df\n",
        "train = train.groupby('Asset_ID').apply(reindex).reset_index(0, drop=True).sort_index()\n",
        "gc.collect()\n",
        "print(train.shape)\n",
        "\n",
        "# Matching records and marking generated rows as 'non-real'\n",
        "print(train.shape)\n",
        "train['group_num'] = train.index.map(times)\n",
        "train = train.dropna(subset=['group_num'])\n",
        "train['group_num'] = train['group_num'].astype('int')\n",
        "train['id'] = train['group_num'].astype(str) + '_' + train['Asset_ID'].astype(str)\n",
        "train['is_real'] = train.id.isin(ids) * 1\n",
        "train = train.drop('id', axis=1)\n",
        "print(train.shape)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T00:38:08.296841Z",
          "iopub.execute_input": "2022-02-17T00:38:08.297078Z",
          "iopub.status.idle": "2022-02-17T00:38:08.760404Z",
          "shell.execute_reply.started": "2022-02-17T00:38:08.297033Z",
          "shell.execute_reply": "2022-02-17T00:38:08.759366Z"
        },
        "trusted": true,
        "id": "qeyD6yKfgdab",
        "outputId": "575a4734-2106-4c9e-ff27-b3046f8723b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(35716, 10)\n",
            "(35717, 10)\n",
            "(35717, 10)\n",
            "(35716, 12)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:28: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:29: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:30: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train['asset_order'] = train.Asset_ID.map(assets_order)\n",
        "train = train.sort_values(by=['group_num', 'asset_order'])\n",
        "# train = reduce_mem_usage(train)\n",
        "gc.collect()"
      ],
      "metadata": {
        "papermill": {
          "duration": 38.374972,
          "end_time": "2021-11-29T18:09:38.027192",
          "exception": false,
          "start_time": "2021-11-29T18:08:59.65222",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2022-02-17T00:38:08.762072Z",
          "iopub.execute_input": "2022-02-17T00:38:08.762339Z",
          "iopub.status.idle": "2022-02-17T00:38:08.938219Z",
          "shell.execute_reply.started": "2022-02-17T00:38:08.762303Z",
          "shell.execute_reply": "2022-02-17T00:38:08.93743Z"
        },
        "trusted": true,
        "id": "3PG8wEFtgdad",
        "outputId": "c198f0d0-5a6d-4e8e-8244-837fb5ba9ebe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "53"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if train.index.name == \"timestamp\":\n",
        "    train = train.reset_index()\n",
        "train['lr15'] = train.groupby('Asset_ID')['Close'].apply(lambda x: log_return(x, 15))\n",
        "train['lr16'] = train.groupby('Asset_ID')['Close'].apply(lambda x: log_return(x, 16))\n",
        "train = train.merge(df_asset_details[['Asset_ID','Weight']], how='left', on = 'Asset_ID')\n",
        "train['m'] = train['lr15']*train['Weight']\n",
        "train['m'] = train.groupby('timestamp')['m'].transform('sum') / np.sum(df_asset_details['Weight'])\n",
        "\n",
        "train.head(20)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T00:38:08.939735Z",
          "iopub.execute_input": "2022-02-17T00:38:08.940042Z",
          "iopub.status.idle": "2022-02-17T00:38:09.00107Z",
          "shell.execute_reply.started": "2022-02-17T00:38:08.939984Z",
          "shell.execute_reply": "2022-02-17T00:38:09.000329Z"
        },
        "trusted": true,
        "id": "WbOjRhcpgdae",
        "outputId": "8bb2482e-1dae-45c0-bd08-b4f211fd6e72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 677
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-0e29be68-6b54-42ef-ba80-039b0fd3bee7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>timestamp</th>\n",
              "      <th>Asset_ID</th>\n",
              "      <th>Count</th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Volume</th>\n",
              "      <th>VWAP</th>\n",
              "      <th>Target</th>\n",
              "      <th>date</th>\n",
              "      <th>group_num</th>\n",
              "      <th>is_real</th>\n",
              "      <th>asset_order</th>\n",
              "      <th>lr15</th>\n",
              "      <th>lr16</th>\n",
              "      <th>Weight</th>\n",
              "      <th>m</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1621399380</td>\n",
              "      <td>3</td>\n",
              "      <td>13863.0</td>\n",
              "      <td>1.642700</td>\n",
              "      <td>1.649500</td>\n",
              "      <td>1.611000</td>\n",
              "      <td>1.623511</td>\n",
              "      <td>10208457.00</td>\n",
              "      <td>1.627059</td>\n",
              "      <td>0.006264</td>\n",
              "      <td>2021-05-19 04:43:00</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.406719</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1621399440</td>\n",
              "      <td>3</td>\n",
              "      <td>9173.0</td>\n",
              "      <td>1.623793</td>\n",
              "      <td>1.654195</td>\n",
              "      <td>1.615200</td>\n",
              "      <td>1.645859</td>\n",
              "      <td>6408643.00</td>\n",
              "      <td>1.640546</td>\n",
              "      <td>0.010261</td>\n",
              "      <td>2021-05-19 04:44:00</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.406719</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1621399500</td>\n",
              "      <td>3</td>\n",
              "      <td>9524.0</td>\n",
              "      <td>1.645627</td>\n",
              "      <td>1.673242</td>\n",
              "      <td>1.617400</td>\n",
              "      <td>1.664102</td>\n",
              "      <td>6283014.50</td>\n",
              "      <td>1.647340</td>\n",
              "      <td>0.016606</td>\n",
              "      <td>2021-05-19 04:45:00</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.406719</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1621399560</td>\n",
              "      <td>3</td>\n",
              "      <td>7761.0</td>\n",
              "      <td>1.663457</td>\n",
              "      <td>1.669406</td>\n",
              "      <td>1.631900</td>\n",
              "      <td>1.640231</td>\n",
              "      <td>4516063.00</td>\n",
              "      <td>1.650989</td>\n",
              "      <td>0.013718</td>\n",
              "      <td>2021-05-19 04:46:00</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.406719</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1621399620</td>\n",
              "      <td>3</td>\n",
              "      <td>6559.0</td>\n",
              "      <td>1.640461</td>\n",
              "      <td>1.664600</td>\n",
              "      <td>1.635679</td>\n",
              "      <td>1.658525</td>\n",
              "      <td>4456316.50</td>\n",
              "      <td>1.651940</td>\n",
              "      <td>0.021053</td>\n",
              "      <td>2021-05-19 04:47:00</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.406719</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1621399680</td>\n",
              "      <td>3</td>\n",
              "      <td>5590.0</td>\n",
              "      <td>1.658125</td>\n",
              "      <td>1.671800</td>\n",
              "      <td>1.643700</td>\n",
              "      <td>1.657311</td>\n",
              "      <td>3973633.75</td>\n",
              "      <td>1.655983</td>\n",
              "      <td>0.023950</td>\n",
              "      <td>2021-05-19 04:48:00</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.406719</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1621399740</td>\n",
              "      <td>3</td>\n",
              "      <td>8552.0</td>\n",
              "      <td>1.657763</td>\n",
              "      <td>1.660800</td>\n",
              "      <td>1.620500</td>\n",
              "      <td>1.639733</td>\n",
              "      <td>4245567.50</td>\n",
              "      <td>1.643210</td>\n",
              "      <td>0.021678</td>\n",
              "      <td>2021-05-19 04:49:00</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.406719</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1621399800</td>\n",
              "      <td>3</td>\n",
              "      <td>7144.0</td>\n",
              "      <td>1.640014</td>\n",
              "      <td>1.656900</td>\n",
              "      <td>1.628995</td>\n",
              "      <td>1.648322</td>\n",
              "      <td>3585431.25</td>\n",
              "      <td>1.644723</td>\n",
              "      <td>0.026200</td>\n",
              "      <td>2021-05-19 04:50:00</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.406719</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1621399860</td>\n",
              "      <td>3</td>\n",
              "      <td>8980.0</td>\n",
              "      <td>1.647378</td>\n",
              "      <td>1.649000</td>\n",
              "      <td>1.615000</td>\n",
              "      <td>1.628970</td>\n",
              "      <td>5622760.00</td>\n",
              "      <td>1.628477</td>\n",
              "      <td>0.027583</td>\n",
              "      <td>2021-05-19 04:51:00</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.406719</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1621399920</td>\n",
              "      <td>3</td>\n",
              "      <td>10643.0</td>\n",
              "      <td>1.630148</td>\n",
              "      <td>1.635000</td>\n",
              "      <td>1.600000</td>\n",
              "      <td>1.610441</td>\n",
              "      <td>7331379.50</td>\n",
              "      <td>1.614531</td>\n",
              "      <td>0.028171</td>\n",
              "      <td>2021-05-19 04:52:00</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.406719</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>1621399980</td>\n",
              "      <td>3</td>\n",
              "      <td>9123.0</td>\n",
              "      <td>1.610199</td>\n",
              "      <td>1.620600</td>\n",
              "      <td>1.590000</td>\n",
              "      <td>1.599522</td>\n",
              "      <td>6849127.00</td>\n",
              "      <td>1.604222</td>\n",
              "      <td>0.022879</td>\n",
              "      <td>2021-05-19 04:53:00</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.406719</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>1621400040</td>\n",
              "      <td>3</td>\n",
              "      <td>8817.0</td>\n",
              "      <td>1.598771</td>\n",
              "      <td>1.608200</td>\n",
              "      <td>1.585420</td>\n",
              "      <td>1.594127</td>\n",
              "      <td>5408449.00</td>\n",
              "      <td>1.597099</td>\n",
              "      <td>0.030317</td>\n",
              "      <td>2021-05-19 04:54:00</td>\n",
              "      <td>11</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.406719</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>1621400100</td>\n",
              "      <td>3</td>\n",
              "      <td>9643.0</td>\n",
              "      <td>1.593979</td>\n",
              "      <td>1.609100</td>\n",
              "      <td>1.580200</td>\n",
              "      <td>1.602591</td>\n",
              "      <td>6437179.50</td>\n",
              "      <td>1.597654</td>\n",
              "      <td>0.033699</td>\n",
              "      <td>2021-05-19 04:55:00</td>\n",
              "      <td>12</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.406719</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>1621400160</td>\n",
              "      <td>3</td>\n",
              "      <td>9587.0</td>\n",
              "      <td>1.602049</td>\n",
              "      <td>1.620000</td>\n",
              "      <td>1.583223</td>\n",
              "      <td>1.604161</td>\n",
              "      <td>6343381.00</td>\n",
              "      <td>1.606019</td>\n",
              "      <td>0.038886</td>\n",
              "      <td>2021-05-19 04:56:00</td>\n",
              "      <td>13</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.406719</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>1621400220</td>\n",
              "      <td>3</td>\n",
              "      <td>8442.0</td>\n",
              "      <td>1.604467</td>\n",
              "      <td>1.635300</td>\n",
              "      <td>1.590800</td>\n",
              "      <td>1.625749</td>\n",
              "      <td>4911103.00</td>\n",
              "      <td>1.616691</td>\n",
              "      <td>0.028503</td>\n",
              "      <td>2021-05-19 04:57:00</td>\n",
              "      <td>14</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.406719</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>1621400280</td>\n",
              "      <td>3</td>\n",
              "      <td>9203.0</td>\n",
              "      <td>1.626747</td>\n",
              "      <td>1.654680</td>\n",
              "      <td>1.614000</td>\n",
              "      <td>1.646023</td>\n",
              "      <td>5982300.00</td>\n",
              "      <td>1.636194</td>\n",
              "      <td>0.025401</td>\n",
              "      <td>2021-05-19 04:58:00</td>\n",
              "      <td>15</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.013771</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.406719</td>\n",
              "      <td>0.001484</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>1621400340</td>\n",
              "      <td>3</td>\n",
              "      <td>7288.0</td>\n",
              "      <td>1.644431</td>\n",
              "      <td>1.665100</td>\n",
              "      <td>1.635800</td>\n",
              "      <td>1.660292</td>\n",
              "      <td>5016601.50</td>\n",
              "      <td>1.654719</td>\n",
              "      <td>0.014743</td>\n",
              "      <td>2021-05-19 04:59:00</td>\n",
              "      <td>16</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.008731</td>\n",
              "      <td>0.022402</td>\n",
              "      <td>4.406719</td>\n",
              "      <td>0.000941</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>1621400400</td>\n",
              "      <td>3</td>\n",
              "      <td>7925.0</td>\n",
              "      <td>1.660577</td>\n",
              "      <td>1.690000</td>\n",
              "      <td>1.658100</td>\n",
              "      <td>1.683223</td>\n",
              "      <td>5371792.50</td>\n",
              "      <td>1.670483</td>\n",
              "      <td>0.008765</td>\n",
              "      <td>2021-05-19 05:00:00</td>\n",
              "      <td>17</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.011425</td>\n",
              "      <td>0.022448</td>\n",
              "      <td>4.406719</td>\n",
              "      <td>0.001232</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>1621400460</td>\n",
              "      <td>3</td>\n",
              "      <td>10511.0</td>\n",
              "      <td>1.684452</td>\n",
              "      <td>1.715800</td>\n",
              "      <td>1.682200</td>\n",
              "      <td>1.705574</td>\n",
              "      <td>7358236.50</td>\n",
              "      <td>1.701093</td>\n",
              "      <td>0.010435</td>\n",
              "      <td>2021-05-19 05:01:00</td>\n",
              "      <td>18</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.039064</td>\n",
              "      <td>0.024616</td>\n",
              "      <td>4.406719</td>\n",
              "      <td>0.004211</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>1621400520</td>\n",
              "      <td>3</td>\n",
              "      <td>10550.0</td>\n",
              "      <td>1.704350</td>\n",
              "      <td>1.707674</td>\n",
              "      <td>1.676489</td>\n",
              "      <td>1.680776</td>\n",
              "      <td>6785659.50</td>\n",
              "      <td>1.687330</td>\n",
              "      <td>0.003379</td>\n",
              "      <td>2021-05-19 05:02:00</td>\n",
              "      <td>19</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.013327</td>\n",
              "      <td>0.024418</td>\n",
              "      <td>4.406719</td>\n",
              "      <td>0.001437</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0e29be68-6b54-42ef-ba80-039b0fd3bee7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0e29be68-6b54-42ef-ba80-039b0fd3bee7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0e29be68-6b54-42ef-ba80-039b0fd3bee7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "     timestamp  Asset_ID    Count      Open      High       Low     Close  \\\n",
              "0   1621399380         3  13863.0  1.642700  1.649500  1.611000  1.623511   \n",
              "1   1621399440         3   9173.0  1.623793  1.654195  1.615200  1.645859   \n",
              "2   1621399500         3   9524.0  1.645627  1.673242  1.617400  1.664102   \n",
              "3   1621399560         3   7761.0  1.663457  1.669406  1.631900  1.640231   \n",
              "4   1621399620         3   6559.0  1.640461  1.664600  1.635679  1.658525   \n",
              "5   1621399680         3   5590.0  1.658125  1.671800  1.643700  1.657311   \n",
              "6   1621399740         3   8552.0  1.657763  1.660800  1.620500  1.639733   \n",
              "7   1621399800         3   7144.0  1.640014  1.656900  1.628995  1.648322   \n",
              "8   1621399860         3   8980.0  1.647378  1.649000  1.615000  1.628970   \n",
              "9   1621399920         3  10643.0  1.630148  1.635000  1.600000  1.610441   \n",
              "10  1621399980         3   9123.0  1.610199  1.620600  1.590000  1.599522   \n",
              "11  1621400040         3   8817.0  1.598771  1.608200  1.585420  1.594127   \n",
              "12  1621400100         3   9643.0  1.593979  1.609100  1.580200  1.602591   \n",
              "13  1621400160         3   9587.0  1.602049  1.620000  1.583223  1.604161   \n",
              "14  1621400220         3   8442.0  1.604467  1.635300  1.590800  1.625749   \n",
              "15  1621400280         3   9203.0  1.626747  1.654680  1.614000  1.646023   \n",
              "16  1621400340         3   7288.0  1.644431  1.665100  1.635800  1.660292   \n",
              "17  1621400400         3   7925.0  1.660577  1.690000  1.658100  1.683223   \n",
              "18  1621400460         3  10511.0  1.684452  1.715800  1.682200  1.705574   \n",
              "19  1621400520         3  10550.0  1.704350  1.707674  1.676489  1.680776   \n",
              "\n",
              "         Volume      VWAP    Target                date  group_num  is_real  \\\n",
              "0   10208457.00  1.627059  0.006264 2021-05-19 04:43:00          0        1   \n",
              "1    6408643.00  1.640546  0.010261 2021-05-19 04:44:00          1        1   \n",
              "2    6283014.50  1.647340  0.016606 2021-05-19 04:45:00          2        1   \n",
              "3    4516063.00  1.650989  0.013718 2021-05-19 04:46:00          3        1   \n",
              "4    4456316.50  1.651940  0.021053 2021-05-19 04:47:00          4        1   \n",
              "5    3973633.75  1.655983  0.023950 2021-05-19 04:48:00          5        1   \n",
              "6    4245567.50  1.643210  0.021678 2021-05-19 04:49:00          6        1   \n",
              "7    3585431.25  1.644723  0.026200 2021-05-19 04:50:00          7        1   \n",
              "8    5622760.00  1.628477  0.027583 2021-05-19 04:51:00          8        1   \n",
              "9    7331379.50  1.614531  0.028171 2021-05-19 04:52:00          9        1   \n",
              "10   6849127.00  1.604222  0.022879 2021-05-19 04:53:00         10        1   \n",
              "11   5408449.00  1.597099  0.030317 2021-05-19 04:54:00         11        1   \n",
              "12   6437179.50  1.597654  0.033699 2021-05-19 04:55:00         12        1   \n",
              "13   6343381.00  1.606019  0.038886 2021-05-19 04:56:00         13        1   \n",
              "14   4911103.00  1.616691  0.028503 2021-05-19 04:57:00         14        1   \n",
              "15   5982300.00  1.636194  0.025401 2021-05-19 04:58:00         15        1   \n",
              "16   5016601.50  1.654719  0.014743 2021-05-19 04:59:00         16        1   \n",
              "17   5371792.50  1.670483  0.008765 2021-05-19 05:00:00         17        1   \n",
              "18   7358236.50  1.701093  0.010435 2021-05-19 05:01:00         18        1   \n",
              "19   6785659.50  1.687330  0.003379 2021-05-19 05:02:00         19        1   \n",
              "\n",
              "    asset_order      lr15      lr16    Weight         m  \n",
              "0             0       NaN       NaN  4.406719  0.000000  \n",
              "1             0       NaN       NaN  4.406719  0.000000  \n",
              "2             0       NaN       NaN  4.406719  0.000000  \n",
              "3             0       NaN       NaN  4.406719  0.000000  \n",
              "4             0       NaN       NaN  4.406719  0.000000  \n",
              "5             0       NaN       NaN  4.406719  0.000000  \n",
              "6             0       NaN       NaN  4.406719  0.000000  \n",
              "7             0       NaN       NaN  4.406719  0.000000  \n",
              "8             0       NaN       NaN  4.406719  0.000000  \n",
              "9             0       NaN       NaN  4.406719  0.000000  \n",
              "10            0       NaN       NaN  4.406719  0.000000  \n",
              "11            0       NaN       NaN  4.406719  0.000000  \n",
              "12            0       NaN       NaN  4.406719  0.000000  \n",
              "13            0       NaN       NaN  4.406719  0.000000  \n",
              "14            0       NaN       NaN  4.406719  0.000000  \n",
              "15            0  0.013771       NaN  4.406719  0.001484  \n",
              "16            0  0.008731  0.022402  4.406719  0.000941  \n",
              "17            0  0.011425  0.022448  4.406719  0.001232  \n",
              "18            0  0.039064  0.024616  4.406719  0.004211  \n",
              "19            0  0.013327  0.024418  4.406719  0.001437  "
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_split_indices(indices):\n",
        "    train_end_idx = int(len(indices) * (1 - (PCT_VALIDATION+PCT_TEST)/100))\n",
        "    val_end_idx = int(len(indices) * (1 - (PCT_TEST)/100))\n",
        "    return indices[train_end_idx], indices[val_end_idx]\n",
        "\n",
        "train_end_idx = {}\n",
        "val_end_idx = {}\n",
        "if not SINGLE_ASSET:\n",
        "    for i in range(N_ASSETS):\n",
        "        X_asset_indices = train[train[\"Asset_ID\"]==i].index\n",
        "        train_end_idx[i], val_end_idx[i] = get_split_indices(X_asset_indices)\n",
        "else:\n",
        "    X_asset_indices = train[train[\"Asset_ID\"]==asset_id].index\n",
        "    train_end_idx[asset_id], val_end_idx[asset_id] = get_split_indices(X_asset_indices)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T00:38:09.002308Z",
          "iopub.execute_input": "2022-02-17T00:38:09.00274Z",
          "iopub.status.idle": "2022-02-17T00:38:09.013502Z",
          "shell.execute_reply.started": "2022-02-17T00:38:09.002701Z",
          "shell.execute_reply": "2022-02-17T00:38:09.012775Z"
        },
        "trusted": true,
        "id": "QBrweJbVgdaf"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create train data of shape: Batch Size X Window Size X Num Assets --> bBATCH_SIZEon mean over all features\n",
        "# create train lables of shape: batch size X Num Assets  -->  based on target of t(win_len)\n",
        "# given data and index, train batches + labels are created\n",
        "\n",
        "\n",
        "class sample_generator(keras.utils.Sequence):\n",
        "    def __init__(self, x_set, y_set, batch_size, length, prediction_length):\n",
        "        self.x, self.y = x_set, y_set\n",
        "        self.batch_size = batch_size\n",
        "        self.length = length\n",
        "        self.prediction_length = prediction_length\n",
        "        self.size = len(x_set)\n",
        "    def __len__(self): return int((len(self.x)-self.length) / float(self.batch_size))\n",
        "    def __getitem__(self, idx):\n",
        "        batch_x=[]\n",
        "        batch_y=[]\n",
        "        for i in range(self.batch_size):\n",
        "            start_ind = self.batch_size*idx + i\n",
        "            end_ind = start_ind + self.length \n",
        "            if (end_ind+self.prediction_length-1) <= self.size:\n",
        "                batch_x.append(self.x[start_ind : end_ind])\n",
        "                batch_y.append(self.y[end_ind -1: end_ind+self.prediction_length-1]) \n",
        "        x,y = np.array(batch_x), np.array(batch_y)\n",
        "        assert x.shape == (BATCH_SIZE, WINDOW_SIZE,len(features)), f\"Shape Missmatch of train data generator X at idx {str(idx)} and i {str(i)}\"\n",
        "        assert x.shape == (BATCH_SIZE, WINDOW_SIZE,len(features)), f\"Shape Missmatch of train data generator Y at idx {str(idx)} and i {str(i)}\"\n",
        "        return x,y\n",
        "\n",
        "class multivariate_sample_generator(keras.utils.Sequence):\n",
        "    # generator which supports multivariate ex-features input into nbeats\n",
        "    # -> Single asset per sequence\n",
        "    # --> x_sequence.shape = batch_size x Window_size x 1\n",
        "    # --> y_sequence.shape = batch_size x 1\n",
        "    # --> e_sequence.shape = batch_size x Window_size x num_ex_var    \n",
        "    def __init__(self, x_set, y_set, e_set, batch_size, length, prediction_length):\n",
        "        self.x, self.y, self.e = x_set, y_set, e_set\n",
        "        self.batch_size = batch_size\n",
        "        self.length = length\n",
        "        self.prediction_length = prediction_length\n",
        "        self.size = len(x_set)\n",
        "        self.num_assets = self.x.shape[1]\n",
        "    def __len__(self): return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
        "    def __getitem__(self, idx):\n",
        "        # idx: how many times the generator is already called\n",
        "        batch_x=[]\n",
        "        batch_y=[]\n",
        "        batch_e=[]\n",
        "        num_sample=0\n",
        "        while num_sample < self.batch_size:\n",
        "            start_ind = self.batch_size//self.num_assets*idx + num_sample//self.num_assets\n",
        "            end_ind = start_ind + self.length \n",
        "            if (end_ind+self.prediction_length-1) <= self.size:\n",
        "                for a in range(self.num_assets):\n",
        "                    batch_x.append(self.x[start_ind : end_ind, a])\n",
        "                    batch_e.append(self.e[start_ind : end_ind, a, :])\n",
        "                    batch_y.append(self.y[end_ind -1: end_ind+self.prediction_length-1, a]) \n",
        "                    num_sample += 1\n",
        "                    if num_sample >= self.batch_size: break;  ## TODO: FIND BETTER SOLUTION (here we throw aray samples if we break out of loop)\n",
        "        return [np.array(batch_x).reshape(-1,self.length,1), np.array(batch_e)], np.array(batch_y)\n",
        "    \n",
        "\n",
        "def assert_data_validity_generator(gen, batch_size, test_shift_size=1, num_tests=200):\n",
        "    for i in range(num_tests):\n",
        "        num_batch = np.random.randint(0, len(gen), dtype=int)\n",
        "        num_sample = np.random.randint(0, batch_size-1, dtype=int)\n",
        "        assert gen[num_batch][0][num_sample+test_shift_size,-1,0] == gen[num_batch][1][num_sample,0,0], \"Data Missmatch between Series and Target in generator data\"\n",
        "    print(\"All tests passed.\")\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T00:38:09.014888Z",
          "iopub.execute_input": "2022-02-17T00:38:09.015352Z",
          "iopub.status.idle": "2022-02-17T00:38:09.035233Z",
          "shell.execute_reply.started": "2022-02-17T00:38:09.015315Z",
          "shell.execute_reply": "2022-02-17T00:38:09.034448Z"
        },
        "trusted": true,
        "id": "n8iZNJFggdag"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_asset = train.copy()\n",
        "X_asset = get_features_hist(X_asset)\n",
        "X_asset[\"Target_shifted\"] = X_asset[\"Target\"].shift(num_shift)\n",
        "X_asset[\"Target\"].plot()\n",
        "\n",
        "train_end_idx[asset_id], val_end_idx[asset_id]\n",
        "features = [\"Target_shifted\"]\n",
        "# features = [\"Target_shifted\", \"m\", \"upper_shadow\", \"lower_shadow\", \"open2close\", \"high2low\", \"volume2count\", \"close2vwap\"] \n",
        "target = [\"Target\"]\n",
        "X_asset.loc[X_asset.is_real==0, features] = 0 \n",
        "X_asset[features] = X_asset[features].fillna(0)\n",
        "scaler = MinMaxScaler()\n",
        "X_asset[features] = scaler.fit_transform(X_asset[features])  # TODO: NEEDS TO BE SEPARATED FOR TRAIN TEST\n",
        " \n",
        "train_generator = sample_generator(X_asset.loc[:train_end_idx[asset_id], features], \n",
        "                                   X_asset.loc[:train_end_idx[asset_id], target], \n",
        "                                   length = WINDOW_SIZE, batch_size = BATCH_SIZE, prediction_length=prediction_length)\n",
        "val_generator = sample_generator(X_asset.loc[train_end_idx[asset_id]:val_end_idx[asset_id], features], \n",
        "                                 X_asset.loc[train_end_idx[asset_id]:val_end_idx[asset_id], target], \n",
        "                                 length = WINDOW_SIZE, batch_size = BATCH_SIZE, prediction_length=prediction_length)\n",
        "test_generator = sample_generator(X_asset.loc[val_end_idx[asset_id]:, features], \n",
        "                                  X_asset.loc[val_end_idx[asset_id]:, target], \n",
        "                                  length = WINDOW_SIZE, batch_size = BATCH_SIZE, prediction_length=prediction_length)\n",
        "\n",
        "print(\"BATCH_SIZE: \", BATCH_SIZE)\n",
        "print(f'Train batch shape: {train_generator[0][0].shape}')\n",
        "print(f'Target batch shape: {train_generator[0][1].shape}')\n",
        "\n",
        "print(\"Num batches of train, validation and test generator: \", len(train_generator),\", \", len(val_generator),\", \", len(test_generator))\n",
        "print(\"Num Training Samples: \", len(train_generator)*BATCH_SIZE)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T00:50:00.197725Z",
          "iopub.execute_input": "2022-02-17T00:50:00.198607Z",
          "iopub.status.idle": "2022-02-17T00:50:00.483976Z",
          "shell.execute_reply.started": "2022-02-17T00:50:00.198534Z",
          "shell.execute_reply": "2022-02-17T00:50:00.482539Z"
        },
        "trusted": true,
        "id": "tQRkhrE-gdah",
        "outputId": "672f998c-fe5d-49fb-8a9a-ba1f4bded420",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BATCH_SIZE:  64\n",
            "Train batch shape: (64, 64, 1)\n",
            "Target batch shape: (64, 1, 1)\n",
            "Num batches of train, validation and test generator:  445 ,  54 ,  54\n",
            "Num Training Samples:  28480\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <span class=\"title-section w3-xxlarge\" id=\"training\">Training 🏋️</span>\n",
        "<hr>\n",
        "\n",
        "Our model will be trained for the number of FOLDS and EPOCHS you chose in the configuration above. Each fold the model with lowest validation loss will be saved and used to predict OOF and test. Adjust the variable `VERBOSE`. The variable `VERBOSE=1 or 2` will display the training and validation loss for each epoch as text. "
      ],
      "metadata": {
        "papermill": {
          "duration": 0.038491,
          "end_time": "2021-11-29T18:09:46.955605",
          "exception": false,
          "start_time": "2021-11-29T18:09:46.917114",
          "status": "completed"
        },
        "tags": [],
        "id": "7SIUAeqjgdai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Time2Vec(keras.layers.Layer):\n",
        "    def __init__(self, kernel_size=1):\n",
        "        super(Time2Vec, self).__init__(trainable=True, name='Time2VecLayer')\n",
        "        self.k = kernel_size\n",
        "    \n",
        "    def build(self, input_shape):  # build automatically executed before layer is called for the first time --  mostly used to instantiate weights\n",
        "        # trend\n",
        "        self.wb = self.add_weight(name='wb',shape=(input_shape[1],),initializer='uniform',trainable=True)\n",
        "        self.bb = self.add_weight(name='bb',shape=(input_shape[1],),initializer='uniform',trainable=True)\n",
        "        # periodic\n",
        "        self.wa = self.add_weight(name='wa',shape=(1, input_shape[1], self.k),initializer='uniform',trainable=True)\n",
        "        self.ba = self.add_weight(name='ba',shape=(1, input_shape[1], self.k),initializer='uniform',trainable=True)\n",
        "        super(Time2Vec, self).build(input_shape)\n",
        "    \n",
        "    def call(self, inputs, **kwargs):  # where the layer logic lives\n",
        "        bias = self.wb * inputs + self.bb\n",
        "        dp = K.dot(inputs, self.wa) + self.ba\n",
        "        wgts = K.sin(dp) # or K.cos(.)\n",
        "\n",
        "        ret = K.concatenate([K.expand_dims(bias, -1), wgts], -1)\n",
        "        ret = K.reshape(ret, (-1, inputs.shape[1]*(self.k+1)))\n",
        "        return ret\n",
        "    \n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], input_shape[1]*(self.k + 1))\n",
        "    \n",
        "\n",
        "# https://towardsdatascience.com/time2vec-for-time-series-features-encoding-a03a4f3f937e\n",
        "class T2V(keras.layers.Layer):\n",
        "    \n",
        "    def __init__(self, output_dim=None, **kwargs):\n",
        "        self.output_dim = output_dim\n",
        "        super(T2V, self).__init__(**kwargs)\n",
        "        \n",
        "    def build(self, input_shape):        \n",
        "        self.W = self.add_weight(name='W',\n",
        "                      shape=(input_shape[-1], self.output_dim),\n",
        "                      initializer='uniform',\n",
        "                      trainable=True)        \n",
        "        self.P = self.add_weight(name='P',\n",
        "                      shape=(input_shape[1], self.output_dim),\n",
        "                      initializer='uniform',\n",
        "                      trainable=True)        \n",
        "        self.w = self.add_weight(name='w',\n",
        "                      shape=(input_shape[1], 1),\n",
        "                      initializer='uniform',\n",
        "                      trainable=True)        \n",
        "        self.p = self.add_weight(name='p',\n",
        "                      shape=(input_shape[1], 1),\n",
        "                      initializer='uniform',\n",
        "                      trainable=True)        \n",
        "        super(T2V, self).build(input_shape)\n",
        "        \n",
        "    def call(self, x):\n",
        "        \n",
        "        original = self.w * x + self.p\n",
        "        sin_trans = K.sin(K.dot(x, self.W) + self.P)\n",
        "        \n",
        "        return K.concatenate([sin_trans, original], -1)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T00:50:00.525214Z",
          "iopub.execute_input": "2022-02-17T00:50:00.52544Z",
          "iopub.status.idle": "2022-02-17T00:50:00.540886Z",
          "shell.execute_reply.started": "2022-02-17T00:50:00.525406Z",
          "shell.execute_reply": "2022-02-17T00:50:00.54001Z"
        },
        "trusted": true,
        "id": "xbUPXzfdgdaj"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://towardsdatascience.com/the-time-series-transformer-2a521a0efad3\n",
        "\n",
        "class AttentionBlock(keras.Model):\n",
        "    def __init__(self, name='AttentionBlock', num_heads=2, head_size=128, ff_dim=None, dropout=0, **kwargs):\n",
        "        super().__init__(name=name, **kwargs)\n",
        "\n",
        "        if ff_dim is None:\n",
        "            ff_dim = head_size\n",
        "\n",
        "        self.attention = MultiHeadAttention(num_heads=num_heads, head_size=head_size, dropout=dropout)\n",
        "        self.attention_dropout = keras.layers.Dropout(dropout)\n",
        "        self.attention_norm = keras.layers.BatchNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.ff_conv1 = keras.layers.Conv1D(filters=ff_dim, kernel_size=1, activation='relu')\n",
        "        # self.ff_conv2 at build()\n",
        "        self.ff_dropout = keras.layers.Dropout(dropout)\n",
        "        self.ff_norm = keras.layers.BatchNormalization(epsilon=1e-6)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.ff_conv2 = keras.layers.Conv1D(filters=input_shape[-1], kernel_size=1) \n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.attention([inputs, inputs])\n",
        "        x = self.attention_dropout(x)\n",
        "        x = self.attention_norm(inputs + x)\n",
        "\n",
        "        x = self.ff_conv1(x)\n",
        "        x = self.ff_conv2(x)\n",
        "        x = self.ff_dropout(x)\n",
        "\n",
        "        x = self.ff_norm(inputs + x)\n",
        "        return x\n",
        "    \n",
        "\n",
        "class ModelTrunk(keras.Model):\n",
        "    def __init__(self, name='ModelTrunk', time2vec_dim=1, num_heads=2, head_size=128, ff_dim=None, num_layers=1, dropout=0, **kwargs):\n",
        "        super().__init__(name=name, **kwargs)\n",
        "        self.time2vec = Time2Vec(kernel_size=time2vec_dim)\n",
        "        if ff_dim is None:\n",
        "            ff_dim = head_size\n",
        "        self.dropout = dropout\n",
        "        self.attention_layers = [AttentionBlock(num_heads=num_heads, head_size=head_size, ff_dim=ff_dim, dropout=dropout) for _ in range(num_layers)]\n",
        "\n",
        "        \n",
        "    def call(self, inputs):\n",
        "        time_embedding = keras.layers.TimeDistributed(self.time2vec)(inputs)\n",
        "        x = K.concatenate([inputs, time_embedding], -1)\n",
        "        for attention_layer in self.attention_layers:\n",
        "            x = attention_layer(x)\n",
        "\n",
        "        return K.reshape(x, (-1, x.shape[1] * x.shape[2])) # flat vector of features out\n",
        "    \n",
        "\n",
        "def build_model_new(\n",
        "    input_shape,  # shape of time series sample\n",
        "    head_size, # size of multi head attention\n",
        "    num_heads,  # number of multi-head attention \n",
        "    ff_dim,  # \n",
        "    num_transformer_blocks, # \n",
        "    mlp_units, # list of N dense layers with i neurons\n",
        "    dropout=0,  # transformer block dropout rate\n",
        "    mlp_dropout=0,  # \n",
        "    time2vec_dim=3\n",
        "):\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    \n",
        "    x = ModelTrunk(num_heads=num_heads, head_size=head_size, ff_dim=ff_dim, num_layers=num_transformer_blocks, time2vec_dim=time2vec_dim, dropout=dropout)(inputs)\n",
        "\n",
        "    # x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
        "    for dim in mlp_units:\n",
        "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
        "        x = layers.Dropout(mlp_dropout)(x)\n",
        "    outputs = layers.Dense(1, kernel_initializer=\"normal\")(x)\n",
        "    return keras.Model(inputs, outputs)\n",
        "\n",
        "\n",
        "# https://github.com/tatp22/multidim-positional-encoding/blob/master/positional_encodings/tf_positional_encodings.py\n",
        "class TFPositionalEncoding1D(tf.keras.layers.Layer):\n",
        "    def __init__(self, channels: int, dtype=tf.float32):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            channels int: The last dimension of the tensor you want to apply pos emb to.\n",
        "        Keyword Args:\n",
        "            dtype: output type of the encodings. Default is \"tf.float32\".\n",
        "        \"\"\"\n",
        "        super(TFPositionalEncoding1D, self).__init__()\n",
        "\n",
        "        self.channels = int(np.ceil(channels / 2) * 2)\n",
        "        self.inv_freq = np.float32(\n",
        "            1\n",
        "            / np.power(\n",
        "                10000, np.arange(0, self.channels, 2) / np.float32(self.channels)\n",
        "            )\n",
        "        )\n",
        "\n",
        "    @tf.function\n",
        "    def call(self, inputs):\n",
        "        \"\"\"\n",
        "        :param tensor: A 3d tensor of size (batch_size, x, ch)\n",
        "        :return: Positional Encoding Matrix of size (batch_size, x, ch)\n",
        "        \"\"\"\n",
        "        if len(inputs.shape) != 3:\n",
        "            raise RuntimeError(\"The input tensor has to be 3d!\")\n",
        "        _, x, org_channels = inputs.shape\n",
        "\n",
        "        dtype = self.inv_freq.dtype\n",
        "        pos_x = tf.range(x, dtype=dtype)\n",
        "        sin_inp_x = tf.einsum(\"i,j->ij\", pos_x, self.inv_freq)\n",
        "        emb = tf.expand_dims(tf.concat((tf.sin(sin_inp_x), tf.cos(sin_inp_x)), -1), 0)\n",
        "        emb = emb[0]  # A bit of a hack\n",
        "        return tf.repeat(emb[None, :, :org_channels], tf.shape(inputs)[0], axis=0)        \n",
        "        "
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T00:50:00.705448Z",
          "iopub.execute_input": "2022-02-17T00:50:00.705827Z",
          "iopub.status.idle": "2022-02-17T00:50:00.731486Z",
          "shell.execute_reply.started": "2022-02-17T00:50:00.705792Z",
          "shell.execute_reply": "2022-02-17T00:50:00.730033Z"
        },
        "trusted": true,
        "id": "5MMg1hmCgdal"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://keras.io/examples/timeseries/timeseries_transformer_classification/\n",
        "\n",
        "\n",
        "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
        "    # Normalization and Attention\n",
        "    x = layers.BatchNormalization(epsilon=1e-6)(inputs)\n",
        "    x = layers.MultiHeadAttention(\n",
        "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
        "    )(x, x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    res = x + inputs\n",
        "\n",
        "    # Feed Forward Part\n",
        "    x = layers.BatchNormalization(epsilon=1e-6)(res)\n",
        "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
        "    return x + res\n",
        "\n",
        "# self.time2vec = Time2Vec(kernel_size=time2vec_dim)\n",
        "\n",
        "def build_model(\n",
        "    input_shape,  # shape of time series sample\n",
        "    head_size, # size of multi head attention\n",
        "    num_heads,  # number of multi-head attention \n",
        "    ff_dim,  # \n",
        "    num_transformer_blocks, # \n",
        "    mlp_units, # list of N dense layers with i neurons\n",
        "    dropout=0,  # transformer block dropout rate\n",
        "    mlp_dropout=0,  # \n",
        "    time2vec_dim=1\n",
        "):\n",
        "    inputs = keras.Input(shape=input_shape)\n",
        "    \n",
        "    time_embedding = TFPositionalEncoding1D(channels=input_shape[-1])(inputs)\n",
        "    x = tf.keras.layers.Add()([inputs, time_embedding])\n",
        "    \n",
        "    # x = inputs\n",
        "    for _ in range(num_transformer_blocks):\n",
        "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
        "\n",
        "    x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
        "    for dim in mlp_units:\n",
        "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
        "        x = layers.Dropout(mlp_dropout)(x)\n",
        "    outputs = layers.Dense(1, kernel_initializer=\"normal\")(x)\n",
        "    return keras.Model(inputs, outputs)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T00:50:00.893052Z",
          "iopub.execute_input": "2022-02-17T00:50:00.893667Z",
          "iopub.status.idle": "2022-02-17T00:50:00.904069Z",
          "shell.execute_reply.started": "2022-02-17T00:50:00.893633Z",
          "shell.execute_reply": "2022-02-17T00:50:00.903093Z"
        },
        "trusted": true,
        "id": "gI2DksYwgdap"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = train_generator[0][0].shape[1:]\n",
        "\n",
        "model = build_model(\n",
        "    input_shape,\n",
        "    head_size=256,\n",
        "    num_heads=4,\n",
        "    ff_dim=4,\n",
        "    num_transformer_blocks=8,\n",
        "    mlp_units=[128, 128],\n",
        "    mlp_dropout=0.1,\n",
        "    dropout=0.1,\n",
        "    time2vec_dim=TIME2VEC_DIM\n",
        ")\n",
        "\n",
        "# tfp.stats.correlation\n",
        "#tf_corr_new = tfp.stats.correlation(x, y=None, sample_axis=0, event_axis=-1, keepdims=False, name=None)\n",
        "# tf.contrib.metrics.streaming_pearson_correlation(logits,labels)\n",
        "\n",
        "# model.compile(\n",
        "#     optimizer=\"Adam\",\n",
        "#     loss={\"head1\": \"mse\", \"head2\": \"mse\"},\n",
        "#     loss_weights={\"head1\": HEAD1_WEIGHT, \"head2\": HEAD2_WEIGHT},\n",
        "#     metrics={\"head1\": [\"mae\"], \"head2\": [\"mae\"]}\n",
        "# )\n",
        "# \n",
        "loss_func = combined_loss\n",
        "model.compile(\n",
        "    loss=loss_func,\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=1e-2),\n",
        "    metrics=[\"mae\", tfp.stats.correlation],\n",
        ")\n",
        "model.summary()\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T00:50:01.053911Z",
          "iopub.execute_input": "2022-02-17T00:50:01.054523Z",
          "iopub.status.idle": "2022-02-17T00:50:01.738473Z",
          "shell.execute_reply.started": "2022-02-17T00:50:01.054489Z",
          "shell.execute_reply": "2022-02-17T00:50:01.737796Z"
        },
        "trusted": true,
        "id": "1sSkWOGIgdat",
        "outputId": "92972dad-7702-43e4-ff21-f356aefcb0c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)           [(None, 64, 1)]      0           []                               \n",
            "                                                                                                  \n",
            " tf_positional_encoding1d (TFPo  (None, 64, 1)       0           ['input_2[0][0]']                \n",
            " sitionalEncoding1D)                                                                              \n",
            "                                                                                                  \n",
            " add (Add)                      (None, 64, 1)        0           ['input_2[0][0]',                \n",
            "                                                                  'tf_positional_encoding1d[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 64, 1)       4           ['add[0][0]']                    \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " multi_head_attention (MultiHea  (None, 64, 1)       7169        ['batch_normalization[0][0]',    \n",
            " dAttention)                                                      'batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 64, 1)        0           ['multi_head_attention[0][0]']   \n",
            "                                                                                                  \n",
            " tf.__operators__.add (TFOpLamb  (None, 64, 1)       0           ['dropout[0][0]',                \n",
            " da)                                                              'add[0][0]']                    \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 64, 1)       4           ['tf.__operators__.add[0][0]']   \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv1d (Conv1D)                (None, 64, 4)        8           ['batch_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)            (None, 64, 4)        0           ['conv1d[0][0]']                 \n",
            "                                                                                                  \n",
            " conv1d_1 (Conv1D)              (None, 64, 1)        5           ['dropout_1[0][0]']              \n",
            "                                                                                                  \n",
            " tf.__operators__.add_1 (TFOpLa  (None, 64, 1)       0           ['conv1d_1[0][0]',               \n",
            " mbda)                                                            'tf.__operators__.add[0][0]']   \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 64, 1)       4           ['tf.__operators__.add_1[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " multi_head_attention_1 (MultiH  (None, 64, 1)       7169        ['batch_normalization_2[0][0]',  \n",
            " eadAttention)                                                    'batch_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)            (None, 64, 1)        0           ['multi_head_attention_1[0][0]'] \n",
            "                                                                                                  \n",
            " tf.__operators__.add_2 (TFOpLa  (None, 64, 1)       0           ['dropout_2[0][0]',              \n",
            " mbda)                                                            'tf.__operators__.add_1[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 64, 1)       4           ['tf.__operators__.add_2[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv1d_2 (Conv1D)              (None, 64, 4)        8           ['batch_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_3 (Dropout)            (None, 64, 4)        0           ['conv1d_2[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_3 (Conv1D)              (None, 64, 1)        5           ['dropout_3[0][0]']              \n",
            "                                                                                                  \n",
            " tf.__operators__.add_3 (TFOpLa  (None, 64, 1)       0           ['conv1d_3[0][0]',               \n",
            " mbda)                                                            'tf.__operators__.add_2[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_4 (BatchNo  (None, 64, 1)       4           ['tf.__operators__.add_3[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " multi_head_attention_2 (MultiH  (None, 64, 1)       7169        ['batch_normalization_4[0][0]',  \n",
            " eadAttention)                                                    'batch_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_4 (Dropout)            (None, 64, 1)        0           ['multi_head_attention_2[0][0]'] \n",
            "                                                                                                  \n",
            " tf.__operators__.add_4 (TFOpLa  (None, 64, 1)       0           ['dropout_4[0][0]',              \n",
            " mbda)                                                            'tf.__operators__.add_3[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_5 (BatchNo  (None, 64, 1)       4           ['tf.__operators__.add_4[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv1d_4 (Conv1D)              (None, 64, 4)        8           ['batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_5 (Dropout)            (None, 64, 4)        0           ['conv1d_4[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_5 (Conv1D)              (None, 64, 1)        5           ['dropout_5[0][0]']              \n",
            "                                                                                                  \n",
            " tf.__operators__.add_5 (TFOpLa  (None, 64, 1)       0           ['conv1d_5[0][0]',               \n",
            " mbda)                                                            'tf.__operators__.add_4[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_6 (BatchNo  (None, 64, 1)       4           ['tf.__operators__.add_5[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " multi_head_attention_3 (MultiH  (None, 64, 1)       7169        ['batch_normalization_6[0][0]',  \n",
            " eadAttention)                                                    'batch_normalization_6[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_6 (Dropout)            (None, 64, 1)        0           ['multi_head_attention_3[0][0]'] \n",
            "                                                                                                  \n",
            " tf.__operators__.add_6 (TFOpLa  (None, 64, 1)       0           ['dropout_6[0][0]',              \n",
            " mbda)                                                            'tf.__operators__.add_5[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_7 (BatchNo  (None, 64, 1)       4           ['tf.__operators__.add_6[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv1d_6 (Conv1D)              (None, 64, 4)        8           ['batch_normalization_7[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_7 (Dropout)            (None, 64, 4)        0           ['conv1d_6[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_7 (Conv1D)              (None, 64, 1)        5           ['dropout_7[0][0]']              \n",
            "                                                                                                  \n",
            " tf.__operators__.add_7 (TFOpLa  (None, 64, 1)       0           ['conv1d_7[0][0]',               \n",
            " mbda)                                                            'tf.__operators__.add_6[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_8 (BatchNo  (None, 64, 1)       4           ['tf.__operators__.add_7[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " multi_head_attention_4 (MultiH  (None, 64, 1)       7169        ['batch_normalization_8[0][0]',  \n",
            " eadAttention)                                                    'batch_normalization_8[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_8 (Dropout)            (None, 64, 1)        0           ['multi_head_attention_4[0][0]'] \n",
            "                                                                                                  \n",
            " tf.__operators__.add_8 (TFOpLa  (None, 64, 1)       0           ['dropout_8[0][0]',              \n",
            " mbda)                                                            'tf.__operators__.add_7[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_9 (BatchNo  (None, 64, 1)       4           ['tf.__operators__.add_8[0][0]'] \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " conv1d_8 (Conv1D)              (None, 64, 4)        8           ['batch_normalization_9[0][0]']  \n",
            "                                                                                                  \n",
            " dropout_9 (Dropout)            (None, 64, 4)        0           ['conv1d_8[0][0]']               \n",
            "                                                                                                  \n",
            " conv1d_9 (Conv1D)              (None, 64, 1)        5           ['dropout_9[0][0]']              \n",
            "                                                                                                  \n",
            " tf.__operators__.add_9 (TFOpLa  (None, 64, 1)       0           ['conv1d_9[0][0]',               \n",
            " mbda)                                                            'tf.__operators__.add_8[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_10 (BatchN  (None, 64, 1)       4           ['tf.__operators__.add_9[0][0]'] \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_5 (MultiH  (None, 64, 1)       7169        ['batch_normalization_10[0][0]', \n",
            " eadAttention)                                                    'batch_normalization_10[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_10 (Dropout)           (None, 64, 1)        0           ['multi_head_attention_5[0][0]'] \n",
            "                                                                                                  \n",
            " tf.__operators__.add_10 (TFOpL  (None, 64, 1)       0           ['dropout_10[0][0]',             \n",
            " ambda)                                                           'tf.__operators__.add_9[0][0]'] \n",
            "                                                                                                  \n",
            " batch_normalization_11 (BatchN  (None, 64, 1)       4           ['tf.__operators__.add_10[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv1d_10 (Conv1D)             (None, 64, 4)        8           ['batch_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_11 (Dropout)           (None, 64, 4)        0           ['conv1d_10[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_11 (Conv1D)             (None, 64, 1)        5           ['dropout_11[0][0]']             \n",
            "                                                                                                  \n",
            " tf.__operators__.add_11 (TFOpL  (None, 64, 1)       0           ['conv1d_11[0][0]',              \n",
            " ambda)                                                           'tf.__operators__.add_10[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_12 (BatchN  (None, 64, 1)       4           ['tf.__operators__.add_11[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_6 (MultiH  (None, 64, 1)       7169        ['batch_normalization_12[0][0]', \n",
            " eadAttention)                                                    'batch_normalization_12[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_12 (Dropout)           (None, 64, 1)        0           ['multi_head_attention_6[0][0]'] \n",
            "                                                                                                  \n",
            " tf.__operators__.add_12 (TFOpL  (None, 64, 1)       0           ['dropout_12[0][0]',             \n",
            " ambda)                                                           'tf.__operators__.add_11[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_13 (BatchN  (None, 64, 1)       4           ['tf.__operators__.add_12[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv1d_12 (Conv1D)             (None, 64, 4)        8           ['batch_normalization_13[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_13 (Dropout)           (None, 64, 4)        0           ['conv1d_12[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_13 (Conv1D)             (None, 64, 1)        5           ['dropout_13[0][0]']             \n",
            "                                                                                                  \n",
            " tf.__operators__.add_13 (TFOpL  (None, 64, 1)       0           ['conv1d_13[0][0]',              \n",
            " ambda)                                                           'tf.__operators__.add_12[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_14 (BatchN  (None, 64, 1)       4           ['tf.__operators__.add_13[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " multi_head_attention_7 (MultiH  (None, 64, 1)       7169        ['batch_normalization_14[0][0]', \n",
            " eadAttention)                                                    'batch_normalization_14[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_14 (Dropout)           (None, 64, 1)        0           ['multi_head_attention_7[0][0]'] \n",
            "                                                                                                  \n",
            " tf.__operators__.add_14 (TFOpL  (None, 64, 1)       0           ['dropout_14[0][0]',             \n",
            " ambda)                                                           'tf.__operators__.add_13[0][0]']\n",
            "                                                                                                  \n",
            " batch_normalization_15 (BatchN  (None, 64, 1)       4           ['tf.__operators__.add_14[0][0]']\n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " conv1d_14 (Conv1D)             (None, 64, 4)        8           ['batch_normalization_15[0][0]'] \n",
            "                                                                                                  \n",
            " dropout_15 (Dropout)           (None, 64, 4)        0           ['conv1d_14[0][0]']              \n",
            "                                                                                                  \n",
            " conv1d_15 (Conv1D)             (None, 64, 1)        5           ['dropout_15[0][0]']             \n",
            "                                                                                                  \n",
            " tf.__operators__.add_15 (TFOpL  (None, 64, 1)       0           ['conv1d_15[0][0]',              \n",
            " ambda)                                                           'tf.__operators__.add_14[0][0]']\n",
            "                                                                                                  \n",
            " global_average_pooling1d (Glob  (None, 64)          0           ['tf.__operators__.add_15[0][0]']\n",
            " alAveragePooling1D)                                                                              \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 128)          8320        ['global_average_pooling1d[0][0]'\n",
            "                                                                 ]                                \n",
            "                                                                                                  \n",
            " dropout_16 (Dropout)           (None, 128)          0           ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 128)          16512       ['dropout_16[0][0]']             \n",
            "                                                                                                  \n",
            " dropout_17 (Dropout)           (None, 128)          0           ['dense_1[0][0]']                \n",
            "                                                                                                  \n",
            " dense_2 (Dense)                (None, 1)            129         ['dropout_17[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 82,481\n",
            "Trainable params: 82,449\n",
            "Non-trainable params: 32\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def lr_scheduler(epoch, lr, warmup_epochs=15, decay_epochs=100, initial_lr=1e-6, base_lr=1e-3, min_lr=5e-5):\n",
        "    if epoch <= warmup_epochs:\n",
        "        pct = epoch / warmup_epochs\n",
        "        return ((base_lr - initial_lr) * pct) + initial_lr\n",
        "\n",
        "    if epoch > warmup_epochs and epoch < warmup_epochs+decay_epochs:\n",
        "        pct = 1 - ((epoch - warmup_epochs) / decay_epochs)\n",
        "        return ((base_lr - min_lr) * pct) + min_lr\n",
        "\n",
        "    return min_lr\n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T00:50:01.748404Z",
          "iopub.execute_input": "2022-02-17T00:50:01.748891Z",
          "iopub.status.idle": "2022-02-17T00:50:01.757265Z",
          "shell.execute_reply.started": "2022-02-17T00:50:01.748849Z",
          "shell.execute_reply": "2022-02-17T00:50:01.756451Z"
        },
        "trusted": true,
        "id": "6KLf5ADwgdav"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%ls -a"
      ],
      "metadata": {
        "id": "nM76ByhxnzKA",
        "outputId": "d5fb1e06-13d7-4540-a96e-f2328532b6e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cryptocurrency-extra-data-eos-io.zip\n",
            "cryptocurrency-extra-data-ethereum-classic.zip\n",
            "cryptocurrency-extra-data-ethereum.zip\n",
            "cryptocurrency-extra-data-iota.zip\n",
            "cryptocurrency-extra-data-litecoin.zip\n",
            "cryptocurrency-extra-data-monero.zip\n",
            "cryptocurrency-extra-data-stellar.zip\n",
            "cryptocurrency-extra-data-tron.zip\n",
            "full_data__0__2017.csv\n",
            "full_data__0__2017.jay\n",
            "full_data__0__2018.csv\n",
            "full_data__0__2018.jay\n",
            "full_data__0__2019.csv\n",
            "full_data__0__2019.jay\n",
            "full_data__0__2020.csv\n",
            "full_data__0__2020.jay\n",
            "full_data__0__2021.csv\n",
            "full_data__0__2021.jay\n",
            "full_data__10__2018.csv\n",
            "full_data__10__2018.jay\n",
            "full_data__10__2019.csv\n",
            "full_data__10__2019.jay\n",
            "full_data__10__2020.csv\n",
            "full_data__10__2020.jay\n",
            "full_data__10__2021.csv\n",
            "full_data__10__2021.jay\n",
            "full_data__11__2018.csv\n",
            "full_data__11__2018.jay\n",
            "full_data__11__2019.csv\n",
            "full_data__11__2019.jay\n",
            "full_data__11__2020.csv\n",
            "full_data__11__2020.jay\n",
            "full_data__11__2021.csv\n",
            "full_data__11__2021.jay\n",
            "full_data__1__2017.csv\n",
            "full_data__1__2017.jay\n",
            "full_data__1__2018.csv\n",
            "full_data__1__2018.jay\n",
            "full_data__1__2019.csv\n",
            "full_data__1__2019.jay\n",
            "full_data__1__2020.csv\n",
            "full_data__1__2020.jay\n",
            "full_data__1__2021.csv\n",
            "full_data__1__2021.jay\n",
            "full_data__12__2018.csv\n",
            "full_data__12__2018.jay\n",
            "full_data__12__2019.csv\n",
            "full_data__12__2019.jay\n",
            "full_data__12__2020.csv\n",
            "full_data__12__2020.jay\n",
            "full_data__12__2021.csv\n",
            "full_data__12__2021.jay\n",
            "full_data__13__2018.csv\n",
            "full_data__13__2018.jay\n",
            "full_data__13__2019.csv\n",
            "full_data__13__2019.jay\n",
            "full_data__13__2020.csv\n",
            "full_data__13__2020.jay\n",
            "full_data__13__2021.csv\n",
            "full_data__13__2021.jay\n",
            "full_data__2__2018.csv\n",
            "full_data__2__2018.jay\n",
            "full_data__2__2019.csv\n",
            "full_data__2__2019.jay\n",
            "full_data__2__2020.csv\n",
            "full_data__2__2020.jay\n",
            "full_data__2__2021.csv\n",
            "full_data__2__2021.jay\n",
            "full_data__3__2018.csv\n",
            "full_data__3__2018.jay\n",
            "full_data__3__2019.csv\n",
            "full_data__3__2019.jay\n",
            "full_data__3__2020.csv\n",
            "full_data__3__2020.jay\n",
            "full_data__3__2021.csv\n",
            "full_data__3__2021.jay\n",
            "full_data__4__2019.csv\n",
            "full_data__4__2019.jay\n",
            "full_data__4__2020.csv\n",
            "full_data__4__2020.jay\n",
            "full_data__4__2021.csv\n",
            "full_data__4__2021.jay\n",
            "full_data__5__2018.csv\n",
            "full_data__5__2018.jay\n",
            "full_data__5__2019.csv\n",
            "full_data__5__2019.jay\n",
            "full_data__5__2020.csv\n",
            "full_data__5__2020.jay\n",
            "full_data__5__2021.csv\n",
            "full_data__5__2021.jay\n",
            "full_data__6__2017.csv\n",
            "full_data__6__2017.jay\n",
            "full_data__6__2018.csv\n",
            "full_data__6__2018.jay\n",
            "full_data__6__2019.csv\n",
            "full_data__6__2019.jay\n",
            "full_data__6__2020.csv\n",
            "full_data__6__2020.jay\n",
            "full_data__6__2021.csv\n",
            "full_data__6__2021.jay\n",
            "full_data__7__2018.csv\n",
            "full_data__7__2018.jay\n",
            "full_data__7__2019.csv\n",
            "full_data__7__2019.jay\n",
            "full_data__7__2020.csv\n",
            "full_data__7__2020.jay\n",
            "full_data__7__2021.csv\n",
            "full_data__7__2021.jay\n",
            "full_data__8__2018.csv\n",
            "full_data__8__2018.jay\n",
            "full_data__8__2019.csv\n",
            "full_data__8__2019.jay\n",
            "full_data__8__2020.csv\n",
            "full_data__8__2020.jay\n",
            "full_data__8__2021.csv\n",
            "full_data__8__2021.jay\n",
            "full_data__9__2017.csv\n",
            "full_data__9__2017.jay\n",
            "full_data__9__2018.csv\n",
            "full_data__9__2018.jay\n",
            "full_data__9__2019.csv\n",
            "full_data__9__2019.jay\n",
            "full_data__9__2020.csv\n",
            "full_data__9__2020.jay\n",
            "full_data__9__2021.csv\n",
            "full_data__9__2021.jay\n",
            "\u001b[0m\u001b[01;34mG-ResearchCryptoPrediction\u001b[0m/\n",
            "\u001b[01;34m.ipynb_checkpoints\u001b[0m/\n",
            "kaggle.json\n",
            "orig_asset_details.jay\n",
            "orig_example_sample_submission.jay\n",
            "orig_example_test.jay\n",
            "orig_supplemental_train.jay\n",
            "orig_train.jay\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = [keras.callbacks.EarlyStopping(monitor='val_correlation', mode='max',patience=5, restore_best_weights=True)]\n",
        "callbacks += [keras.callbacks.LearningRateScheduler(lr_scheduler, verbose=1)]\n",
        "filepath = \"/content/gdrive/MyDrive/Github/GResearch/G-ResearchCryptoPrediction/models/transformer_model\"\n",
        "callbacks += [keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False)]\n",
        "\n",
        "EPOCHS = 20\n",
        "history = model.fit(\n",
        "            train_generator, \n",
        "            validation_data = (val_generator),\n",
        "            epochs=EPOCHS,\n",
        "            batch_size=BATCH_SIZE,\n",
        "            callbacks=callbacks)\n",
        "\n",
        "model.evaluate(test_generator, verbose=1)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T00:50:01.76068Z",
          "iopub.execute_input": "2022-02-17T00:50:01.760949Z",
          "iopub.status.idle": "2022-02-17T00:59:13.604094Z",
          "shell.execute_reply.started": "2022-02-17T00:50:01.76092Z",
          "shell.execute_reply": "2022-02-17T00:59:13.603416Z"
        },
        "trusted": true,
        "id": "MxX1sJJKgdaw",
        "outputId": "41ed8587-a842-456b-97ca-7522124f3cfc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1: LearningRateScheduler setting learning rate to 1e-06.\n",
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-196720d36afd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             callbacks=callbacks)\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                 _r=1):\n\u001b[1;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    978\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 980\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    981\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    982\u001b[0m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiltered_flat_args\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2953\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2954\u001b[0m       (graph_function,\n\u001b[0;32m-> 2955\u001b[0;31m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0m\u001b[1;32m   2956\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m   2957\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_call_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3292\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3293\u001b[0m           self._function_cache.add(cache_key, cache_key_deletion_observer,\n\u001b[1;32m   3294\u001b[0m                                    graph_function)\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3138\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3139\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3140\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3141\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3142\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m   1159\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1161\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    675\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1141\u001b[0m                     \u001b[0mrecursive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m                     \u001b[0moptional_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mautograph_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1143\u001b[0;31m                     \u001b[0muser_requested\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1144\u001b[0m                 ))\n\u001b[1;32m   1145\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    437\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverted_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0meffective_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverted_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0meffective_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    329\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mconversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_in_allowlist_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Allowlisted %s: from cache'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_call_unconverted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_status_ctx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDISABLED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    457\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mstep_function\u001b[0;34m(model, iterator)\u001b[0m\n\u001b[1;32m   1008\u001b[0m             run_step, jit_compile=True, experimental_relax_shapes=True)\n\u001b[1;32m   1009\u001b[0m       \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1010\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute_strategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1011\u001b[0m       outputs = reduce_per_replica(\n\u001b[1;32m   1012\u001b[0m           outputs, self.distribute_strategy, reduction='first')\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       fn = autograph.tf_convert(\n\u001b[1;32m   1311\u001b[0m           fn, autograph_ctx.control_status_ctx(), convert_by_default=False)\n\u001b[0;32m-> 1312\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extended\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mcall_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   2886\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2887\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2888\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2889\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2890\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m_call_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   3687\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3688\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mReplicaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplica_id_in_sync_group\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3689\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3691\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_reduce_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestinations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    687\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconversion_ctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 689\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    690\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_requested\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_allowlisted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_call_unconverted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m   \u001b[0;31m# internal_convert_user_code is for example turned off when issuing a dynamic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mrun_step\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    998\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0mrun_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1000\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1001\u001b[0m         \u001b[0;31m# Ensure counter is updated only if `train_step` succeeds.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1002\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_minimum_control_deps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    861\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_target_and_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m     \u001b[0;31m# Run backwards pass.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    864\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(self, loss, var_list, grad_loss, name, tape)\u001b[0m\n\u001b[1;32m    529\u001b[0m     \"\"\"\n\u001b[1;32m    530\u001b[0m     grads_and_vars = self._compute_gradients(\n\u001b[0;32m--> 531\u001b[0;31m         loss, var_list=var_list, grad_loss=grad_loss, tape=tape)\n\u001b[0m\u001b[1;32m    532\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36m_compute_gradients\u001b[0;34m(self, loss, var_list, grad_loss, tape)\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0mvar_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/gradients\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m       \u001b[0mgrads_and_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m     self._assert_valid_dtypes([\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36m_get_gradients\u001b[0;34m(self, tape, loss, var_list, grad_loss)\u001b[0m\n\u001b[1;32m    462\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_get_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m     \u001b[0;34m\"\"\"Called in `minimize` to compute gradients from loss.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 464\u001b[0;31m     \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    465\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1085\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1086\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1087\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m   1088\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1089\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     71\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    154\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/math_grad.py\u001b[0m in \u001b[0;36m_AddGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m   1312\u001b[0m     \u001b[0mgy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1313\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1314\u001b[0;31m     \u001b[0mgy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1315\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mop_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1080\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mreduce_sum\u001b[0;34m(input_tensor, axis, keepdims, name)\u001b[0m\n\u001b[1;32m   2310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2311\u001b[0m   return reduce_sum_with_dims(input_tensor, axis, keepdims, name,\n\u001b[0;32m-> 2312\u001b[0;31m                               _ReductionDims(input_tensor, axis))\n\u001b[0m\u001b[1;32m   2313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mreduce_sum_with_dims\u001b[0;34m(input_tensor, axis, keepdims, name, dims)\u001b[0m\n\u001b[1;32m   2321\u001b[0m   return _may_reduce_to_scalar(\n\u001b[1;32m   2322\u001b[0m       \u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2323\u001b[0;31m       gen_math_ops._sum(input_tensor, dims, keepdims, name=name))\n\u001b[0m\u001b[1;32m   2324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36m_may_reduce_to_scalar\u001b[0;34m(keepdims, axis, output)\u001b[0m\n\u001b[1;32m   2160\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_may_reduce_to_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeepdims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2161\u001b[0m   \u001b[0;34m\"\"\"Set a reduction's output shape to be a scalar if we are certain.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2162\u001b[0;31m   if not _has_fully_defined_shape(output) and (not keepdims) and (\n\u001b[0m\u001b[1;32m   2163\u001b[0m       axis is None):\n\u001b[1;32m   2164\u001b[0m     \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36m_has_fully_defined_shape\u001b[0;34m(tensor)\u001b[0m\n\u001b[1;32m   2155\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_has_fully_defined_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2156\u001b[0m   \u001b[0;34m\"\"\"Returns true if tensor has a fully defined shape.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2157\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_fully_defined\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tf.keras.utils.plot_model(get_model(), show_shapes=True)"
      ],
      "metadata": {
        "papermill": {
          "duration": 6.240302,
          "end_time": "2021-11-29T18:09:55.791092",
          "exception": false,
          "start_time": "2021-11-29T18:09:49.55079",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2022-02-17T00:59:13.605398Z",
          "iopub.execute_input": "2022-02-17T00:59:13.605663Z",
          "iopub.status.idle": "2022-02-17T00:59:13.610915Z",
          "shell.execute_reply.started": "2022-02-17T00:59:13.605628Z",
          "shell.execute_reply": "2022-02-17T00:59:13.608967Z"
        },
        "trusted": true,
        "id": "xUVEr5uZgdaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EPOCHS = 100\n",
        "# seed = 1 \n",
        "# \n",
        "# tf.random.set_seed(seed)\n",
        "# estop = keras.callbacks.EarlyStopping(monitor = 'val_loss', patience = 5, verbose = 0, mode = 'min',restore_best_weights = True)\n",
        "# # scheduler = keras.optimizers.schedules.ExponentialDecay(1e-3, (0.5 * len(X_train) / BATCH_SIZE), 1e-3)\n",
        "# # lr = keras.callbacks.LearningRateScheduler(scheduler, verbose = 1)\n",
        "# mcp_save = keras.callbacks.ModelCheckpoint('model_multivariate_min_val_loss', save_best_only=True, monitor='val_loss', mode='min')\n",
        "# plateau = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=1, mode='min')\n",
        "# \n",
        "# history = model.fit(train_generator, validation_data = (val_generator), epochs = EPOCHS, callbacks = [estop, mcp_save, plateau], verbose=1)\n",
        "# model.save(\"model_multivariate\")\n",
        "\n"
      ],
      "metadata": {
        "papermill": {
          "duration": 791.992821,
          "end_time": "2021-11-29T18:23:07.875583",
          "exception": false,
          "start_time": "2021-11-29T18:09:55.882762",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2022-02-17T00:59:13.613208Z",
          "iopub.execute_input": "2022-02-17T00:59:13.613664Z",
          "iopub.status.idle": "2022-02-17T00:59:13.619716Z",
          "shell.execute_reply.started": "2022-02-17T00:59:13.613625Z",
          "shell.execute_reply": "2022-02-17T00:59:13.618913Z"
        },
        "trusted": true,
        "id": "-P7xiiNsgdax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['val_loss'], label= \"validation_loss\")\n",
        "plt.plot(history.history['loss'], label= \"train_loss\")\n",
        "plt.legend(loc=\"upper left\", bbox_to_anchor=(1, 0.5))\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T00:59:13.620719Z",
          "iopub.execute_input": "2022-02-17T00:59:13.620977Z",
          "iopub.status.idle": "2022-02-17T00:59:13.834846Z",
          "shell.execute_reply.started": "2022-02-17T00:59:13.620926Z",
          "shell.execute_reply": "2022-02-17T00:59:13.834207Z"
        },
        "trusted": true,
        "id": "o5Xag6hqgday"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['val_mae'], label= \"validation metric (mae)\")\n",
        "plt.plot(history.history['mae'], label= \"train metric (mae)\")\n",
        "plt.legend(loc=\"upper left\", bbox_to_anchor=(1, 0.5))\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T00:59:13.836128Z",
          "iopub.execute_input": "2022-02-17T00:59:13.836366Z",
          "iopub.status.idle": "2022-02-17T00:59:14.056202Z",
          "shell.execute_reply.started": "2022-02-17T00:59:13.836333Z",
          "shell.execute_reply": "2022-02-17T00:59:14.055496Z"
        },
        "trusted": true,
        "id": "nfdEIm0mgdaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history.history.keys()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T00:59:14.057348Z",
          "iopub.execute_input": "2022-02-17T00:59:14.058255Z",
          "iopub.status.idle": "2022-02-17T00:59:14.063936Z",
          "shell.execute_reply.started": "2022-02-17T00:59:14.058214Z",
          "shell.execute_reply": "2022-02-17T00:59:14.063131Z"
        },
        "trusted": true,
        "id": "IWgwFtE0gdaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['val_correlation'], label= \"validation metric W-Corr\")\n",
        "plt.plot(history.history['correlation'], label= \"train metric W-Corr\")\n",
        "plt.legend(loc=\"upper left\", bbox_to_anchor=(1, 0.5))\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T00:59:14.065698Z",
          "iopub.execute_input": "2022-02-17T00:59:14.066047Z",
          "iopub.status.idle": "2022-02-17T00:59:14.281173Z",
          "shell.execute_reply.started": "2022-02-17T00:59:14.066013Z",
          "shell.execute_reply": "2022-02-17T00:59:14.280453Z"
        },
        "trusted": true,
        "id": "eUm5j_C1gda0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create predictions on training set\n",
        "predictions = model.predict(train_generator)\n",
        "y_true = []\n",
        "for x,y in train_generator: y_true.append(y)\n",
        "y_true = np.squeeze(np.concatenate(y_true), axis=-1)\n",
        "\n",
        "\n",
        "print(predictions.shape, y_true.shape)\n",
        "assert predictions.shape == y_true.shape, f\"{predictions.shape}, {y_true.shape}\"\n",
        "\n",
        "# Evaluate predictions on validation set\n",
        "print(\"Window Size: \", WINDOW_SIZE)\n",
        "print(\"Prediction length: \", prediction_length)\n",
        "print(\"Epochs: \", EPOCHS)\n",
        "\n",
        "print('---------------------')\n",
        "print('Asset:    Corr. coef.')\n",
        "print('---------------------')\n",
        "asset_w_corr = []\n",
        "asset_corrs = []\n",
        "asset_mae = []\n",
        "y_true = np.squeeze(y_true)\n",
        "y_pred = np.squeeze(predictions)\n",
        "real_target_ind = np.argwhere(y_true!=0)\n",
        "# asset_id = list(assets_order.keys())[i]\n",
        "# asset_name = assets[assets.Asset_ID == asset_id]['Asset_Name'].item()\n",
        "mae_asset = mae(y_true, y_pred)\n",
        "asset_corr = np.corrcoef(np.nan_to_num(y_pred.flatten()), np.nan_to_num(y_true.flatten()))[0,1]\n",
        "\n",
        "print(f\"corr: {asset_corr:.4f}\")\n",
        "print(f\"mae: {mae_asset:.4f}\")\n",
        "print(\"\\n\")\n",
        "print(f\"Predictions Min: {predictions.min()}, Predictions Max: {predictions.max()}, Predictions mean:{predictions.mean()}, Predictions var:{predictions.var()}\") \n",
        "print(f\"y_true Min: {y_true.min()}, y_true Max: {y_true.max()}, y_true mean:{y_true.mean()}, y_true var:{y_true.var()}\") "
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T00:59:14.283396Z",
          "iopub.execute_input": "2022-02-17T00:59:14.284233Z",
          "iopub.status.idle": "2022-02-17T00:59:25.446139Z",
          "shell.execute_reply.started": "2022-02-17T00:59:14.284191Z",
          "shell.execute_reply": "2022-02-17T00:59:25.44524Z"
        },
        "trusted": true,
        "id": "05KQCO5-gda2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create predictions on validation set\n",
        "predictions = model.predict(val_generator)\n",
        "y_true = []\n",
        "for x,y in val_generator: y_true.append(y)\n",
        "y_true = np.squeeze(np.concatenate(y_true), axis=-1)\n",
        "\n",
        "\n",
        "print(predictions.shape, y_true.shape)\n",
        "assert predictions.shape == y_true.shape, f\"{predictions.shape}, {y_true.shape}\"\n",
        "\n",
        "# Evaluate predictions on validation set\n",
        "print(\"Window Size: \", WINDOW_SIZE)\n",
        "print(\"Prediction length: \", prediction_length)\n",
        "print(\"Epochs: \", EPOCHS)\n",
        "\n",
        "print('---------------------')\n",
        "print('Asset:    Corr. coef.')\n",
        "print('---------------------')\n",
        "asset_w_corr = []\n",
        "asset_corrs = []\n",
        "asset_mae = []\n",
        "y_true = np.squeeze(y_true)\n",
        "y_pred = np.squeeze(predictions)\n",
        "real_target_ind = np.argwhere(y_true!=0)\n",
        "# asset_id = list(assets_order.keys())[i]\n",
        "# asset_name = assets[assets.Asset_ID == asset_id]['Asset_Name'].item()\n",
        "mae_asset = mae(y_true, y_pred)\n",
        "asset_corr = np.corrcoef(np.nan_to_num(y_pred.flatten()), np.nan_to_num(y_true.flatten()))[0,1]\n",
        "\n",
        "print(f\"corr: {asset_corr:.4f}\")\n",
        "print(f\"mae: {mae_asset:.4f}\")\n",
        "print(\"\\n\")\n",
        "print(f\"Predictions Min: {predictions.min()}, Predictions Max: {predictions.max()}, Predictions mean:{predictions.mean()}, Predictions var:{predictions.var()}\") \n",
        "print(f\"y_true Min: {y_true.min()}, y_true Max: {y_true.max()}, y_true mean:{y_true.mean()}, y_true var:{y_true.var()}\") "
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T00:59:25.448301Z",
          "iopub.execute_input": "2022-02-17T00:59:25.448786Z",
          "iopub.status.idle": "2022-02-17T00:59:26.72903Z",
          "shell.execute_reply.started": "2022-02-17T00:59:25.448742Z",
          "shell.execute_reply": "2022-02-17T00:59:26.727478Z"
        },
        "trusted": true,
        "id": "VOXCGhkQgda3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create predictions on validation set\n",
        "predictions = model.predict(test_generator)\n",
        "y_true = []\n",
        "for x,y in test_generator: y_true.append(y)\n",
        "y_true = np.squeeze(np.concatenate(y_true), axis=-1)\n",
        "\n",
        "\n",
        "print(predictions.shape, y_true.shape)\n",
        "assert predictions.shape == y_true.shape, f\"{predictions.shape}, {y_true.shape}\"\n",
        "\n",
        "# Evaluate predictions on validation set\n",
        "print(\"Window Size: \", WINDOW_SIZE)\n",
        "print(\"Prediction length: \", prediction_length)\n",
        "print(\"Epochs: \", EPOCHS)\n",
        "\n",
        "asset_w_corr = []\n",
        "asset_corrs = []\n",
        "asset_mae = []\n",
        "y_true = np.squeeze(y_true)\n",
        "predictions = np.squeeze(predictions)\n",
        "real_target_ind = np.argwhere(y_true!=0)\n",
        "# asset_id = list(assets_order.keys())[i]\n",
        "# asset_name = assets[assets.Asset_ID == asset_id]['Asset_Name'].item()\n",
        "mae_asset = mae(y_true, predictions)\n",
        "asset_corr = np.corrcoef(np.nan_to_num(y_pred.flatten()), np.nan_to_num(y_true.flatten()))[0,1]\n",
        "\n",
        "print('---------------------')\n",
        "print('Asset:    Corr. coef.')\n",
        "print('---------------------')\n",
        "print(f\"corr: {asset_corr:.4f}\")\n",
        "print(f\"mae: {mae_asset:.4f}\")\n",
        "print(\"\\n\")\n",
        "print(f\"Predictions Min: {predictions.min()}, Predictions Max: {predictions.max()}, Predictions mean:{predictions.mean()}, Predictions var:{predictions.var()}\") \n",
        "print(f\"y_true Min: {y_true.min()}, y_true Max: {y_true.max()}, y_true mean:{y_true.mean()}, y_true var:{y_true.var()}\") \n",
        "\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-02-17T00:59:26.731599Z",
          "iopub.execute_input": "2022-02-17T00:59:26.731937Z",
          "iopub.status.idle": "2022-02-17T00:59:27.980154Z",
          "shell.execute_reply.started": "2022-02-17T00:59:26.731852Z",
          "shell.execute_reply": "2022-02-17T00:59:27.979435Z"
        },
        "trusted": true,
        "id": "9VGActtpgda6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "oxHwOrV8gda7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Eq5SN190gda8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "MnRNd5Digda8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "jx43e9xXgda9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "IHhEifvKgda9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "qld_i0lRgda_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "EJkCv_SOgda_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}